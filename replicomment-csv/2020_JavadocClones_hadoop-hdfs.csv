Class;Method1;Method2;Type;Param1;Param2;Cloned text;Legit?
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher;void cancelDelegationToken(URLConnectionFactory factory, URI nnAddr, Token tok);long renewDelegationToken(URLConnectionFactory factory, URI nnAddr, Token tok);@param;URI nnAddr;URI nnAddr;the NameNode's address;true
org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer;void initServerAndWait(String fsimage);void initServer(String fsimage);@param;String fsimage;String fsimage;the fsimage to load.;true
org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageVisitor;void visitEnclosingElement(ImageElement element);void visitEnclosingElement(ImageElement element, ImageElement key, String value);@param;ImageElement element;ImageElement element;Element being visited;true
org.apache.hadoop.hdfs.tools.offlineImageViewer.TextWriterImageVisitor; TextWriterImageVisitor(String filename); TextWriterImageVisitor(String filename, boolean printToScreen);@param;String filename;String filename;Name of file to write output to;true
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader;String getFileStatus(String path);String listStatus(String path);@throws;;;if failed to serialize fileStatus to JSON.;true
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader;String getFileStatus(String path);String getAclStatus(String path);@param;String path;String path;a path specifies a file;true
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader;String getFileStatus(String path);String getAclStatus(String path);@throws;;;if failed to serialize fileStatus to JSON.;true
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader;String listStatus(String path);String getAclStatus(String path);@throws;;;if failed to serialize fileStatus to JSON.;true
org.apache.hadoop.hdfs.tools.DFSAdmin; DFSAdmin(); DFSAdmin(Configuration conf);Whole;;;Construct a DFSAdmin object.    ;false
org.apache.hadoop.hdfs.tools.DFSAdmin;void report(String[] argv, int i);void setSafeMode(String[] argv, int idx);@throws;;;if the filesystem does not exist.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void report(String[] argv, int i);int run(String[] argv);@throws;;;if the filesystem does not exist.;false
org.apache.hadoop.hdfs.tools.DFSAdmin;void report(String[] argv, int i);void main(String[] argv);@throws;;;if the filesystem does not exist.;false
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);void allowSnapshot(String[] argv);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);void disallowSnapshot(String[] argv);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);int setBalancerBandwidth(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);int setBalancerBandwidth(String[] argv, int idx);@param;int idx;int idx;The index of the command that is being processed.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);int fetchImage(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);int fetchImage(String[] argv, int idx);@param;int idx;int idx;The index of the command that is being processed.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);int metaSave(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);int metaSave(String[] argv, int idx);@param;int idx;int idx;The index of the command that is being processed.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);int run(String[] argv);@throws;;;if the filesystem does not exist.;false
org.apache.hadoop.hdfs.tools.DFSAdmin;void setSafeMode(String[] argv, int idx);void main(String[] argv);@throws;;;if the filesystem does not exist.;false
org.apache.hadoop.hdfs.tools.DFSAdmin;void allowSnapshot(String[] argv);void disallowSnapshot(String[] argv);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void allowSnapshot(String[] argv);int setBalancerBandwidth(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void allowSnapshot(String[] argv);int fetchImage(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void allowSnapshot(String[] argv);int metaSave(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void disallowSnapshot(String[] argv);int setBalancerBandwidth(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void disallowSnapshot(String[] argv);int fetchImage(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;void disallowSnapshot(String[] argv);int metaSave(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;int setBalancerBandwidth(String[] argv, int idx);int fetchImage(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;int setBalancerBandwidth(String[] argv, int idx);int fetchImage(String[] argv, int idx);@param;int idx;int idx;The index of the command that is being processed.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;int setBalancerBandwidth(String[] argv, int idx);int metaSave(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;int setBalancerBandwidth(String[] argv, int idx);int metaSave(String[] argv, int idx);@param;int idx;int idx;The index of the command that is being processed.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;int fetchImage(String[] argv, int idx);int metaSave(String[] argv, int idx);@param;String[] argv;String[] argv;List of of command line parameters.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;int fetchImage(String[] argv, int idx);int metaSave(String[] argv, int idx);@param;int idx;int idx;The index of the command that is being processed.;true
org.apache.hadoop.hdfs.tools.DFSAdmin;int refreshServiceAcl();int refreshUserToGroupsMappings();@return;;;exitcode 0 on success, non-zero on failure;false
org.apache.hadoop.hdfs.tools.DFSAdmin;int refreshServiceAcl();int refreshSuperUserGroupsConfiguration();@return;;;exitcode 0 on success, non-zero on failure;false
org.apache.hadoop.hdfs.tools.DFSAdmin;int refreshUserToGroupsMappings();int refreshSuperUserGroupsConfiguration();@return;;;exitcode 0 on success, non-zero on failure;false
org.apache.hadoop.hdfs.tools.DFSAdmin;int run(String[] argv);void main(String[] argv);@throws;;;if the filesystem does not exist.;true
org.apache.hadoop.hdfs.DFSUtil;byte[][] bytes2byteArray(byte[] bytes, byte separator);byte[][] bytes2byteArray(byte[] bytes, int len, byte separator);@param;byte separator;byte separator;the delimiting byte;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Collection getNameNodeIds(Configuration conf, String nsId);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Map getHaNnRpcAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Map getBackupNodeAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Map getSecondaryNameNodeAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Map getNNServiceRpcAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Collection getInternalNsRpcUris(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceIds(Configuration conf);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);Map getHaNnRpcAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);Map getBackupNodeAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);Map getSecondaryNameNodeAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);Map getNNServiceRpcAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);Map getNNServiceRpcAddressesForCluster(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);Collection getInternalNsRpcUris(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameNodeIds(Configuration conf, String nsId);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getRpcAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue);String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getRpcAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue);String getNamenodeServiceAddr(Configuration conf, String nsId, String nnId);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getRpcAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue);float getInvalidateWorkPctPerIteration(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getRpcAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue);int getReplWorkMultiplier(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getRpcAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue);String getSpnegoKeytabKey(Configuration conf, String defaultKey);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getRpcAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue);KeyProvider createKeyProvider(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getRpcAddressesForNameserviceId(Configuration conf, String nsId, String defaultValue);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Map getHaNnWebHdfsAddresses(Configuration conf, String scheme);@return;;;list of InetSocketAddresses;false
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Map getBackupNodeAddresses(Configuration conf);@return;;;list of InetSocketAddresses;false
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Map getBackupNodeAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Map getSecondaryNameNodeAddresses(Configuration conf);@return;;;list of InetSocketAddresses;false
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Map getSecondaryNameNodeAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Map getNNServiceRpcAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Collection getInternalNsRpcUris(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnRpcAddresses(Configuration conf);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnWebHdfsAddresses(Configuration conf, String scheme);Map getBackupNodeAddresses(Configuration conf);@return;;;list of InetSocketAddresses;false
org.apache.hadoop.hdfs.DFSUtil;Map getHaNnWebHdfsAddresses(Configuration conf, String scheme);Map getSecondaryNameNodeAddresses(Configuration conf);@return;;;list of InetSocketAddresses;false
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Map getSecondaryNameNodeAddresses(Configuration conf);@return;;;list of InetSocketAddresses;false
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Map getSecondaryNameNodeAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Map getSecondaryNameNodeAddresses(Configuration conf);@throws;;;on error;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Map getNNServiceRpcAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Map getNNServiceRpcAddresses(Configuration conf);@throws;;;on error;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@throws;;;on error;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Collection getInternalNsRpcUris(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);@throws;;;on error;false
org.apache.hadoop.hdfs.DFSUtil;Map getBackupNodeAddresses(Configuration conf);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);Map getNNServiceRpcAddresses(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);Map getNNServiceRpcAddresses(Configuration conf);@throws;;;on error;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@throws;;;on error;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);Collection getInternalNsRpcUris(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);@throws;;;on error;false
org.apache.hadoop.hdfs.DFSUtil;Map getSecondaryNameNodeAddresses(Configuration conf);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@return;;;list of InetSocketAddress;false
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);Map getNNServiceRpcAddressesForCluster(Configuration conf);@throws;;;on error;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);Collection getInternalNsRpcUris(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);@throws;;;on error;false
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddresses(Configuration conf);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddressesForCluster(Configuration conf);Collection getInternalNsRpcUris(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddressesForCluster(Configuration conf);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddressesForCluster(Configuration conf);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddressesForCluster(Configuration conf);String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);@throws;;;on error;false
org.apache.hadoop.hdfs.DFSUtil;Map getNNServiceRpcAddressesForCluster(Configuration conf);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getInternalNsRpcUris(Configuration conf);Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getInternalNsRpcUris(Configuration conf);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getInternalNsRpcUris(Configuration conf);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;Collection getNameServiceUris(Configuration conf, Collection nameServices, String keys);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;URI getInfoServer(InetSocketAddress namenodeAddr, Configuration conf, String scheme);void addPBProtocol(Configuration conf, Class protocol, BlockingService service, RPC.Server server);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.DFSUtil;String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);String getNamenodeServiceAddr(Configuration conf, String nsId, String nnId);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);float getInvalidateWorkPctPerIteration(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);int getReplWorkMultiplier(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);String getSpnegoKeytabKey(Configuration conf, String defaultKey);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);KeyProvider createKeyProvider(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String[] getSuffixIDs(Configuration conf, String addressKey, String knownNsId, String knownNNId, AddressMatcher matcher);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String getNamenodeServiceAddr(Configuration conf, String nsId, String nnId);float getInvalidateWorkPctPerIteration(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String getNamenodeServiceAddr(Configuration conf, String nsId, String nnId);int getReplWorkMultiplier(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String getNamenodeServiceAddr(Configuration conf, String nsId, String nnId);String getSpnegoKeytabKey(Configuration conf, String defaultKey);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String getNamenodeServiceAddr(Configuration conf, String nsId, String nnId);KeyProvider createKeyProvider(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String getNamenodeServiceAddr(Configuration conf, String nsId, String nnId);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;float getInvalidateWorkPctPerIteration(Configuration conf);int getReplWorkMultiplier(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;float getInvalidateWorkPctPerIteration(Configuration conf);String getSpnegoKeytabKey(Configuration conf, String defaultKey);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;float getInvalidateWorkPctPerIteration(Configuration conf);KeyProvider createKeyProvider(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;float getInvalidateWorkPctPerIteration(Configuration conf);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;int getReplWorkMultiplier(Configuration conf);String getSpnegoKeytabKey(Configuration conf, String defaultKey);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;int getReplWorkMultiplier(Configuration conf);KeyProvider createKeyProvider(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;int getReplWorkMultiplier(Configuration conf);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String getSpnegoKeytabKey(Configuration conf, String defaultKey);KeyProvider createKeyProvider(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;String getSpnegoKeytabKey(Configuration conf, String defaultKey);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;KeyProvider createKeyProvider(Configuration conf);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.DFSUtil;KeyProvider createKeyProvider(Configuration conf);KeyProviderCryptoExtension createKeyProviderCryptoExtension(Configuration conf);@throws;;;if the KeyProvider is improperly specified in the Configuration;true
org.apache.hadoop.hdfs.net.Peer;void setReadTimeout(int timeoutMs);void setWriteTimeout(int timeoutMs);@param;int timeoutMs;int timeoutMs;The timeout in milliseconds.;true
org.apache.hadoop.hdfs.DFSInotifyEventInputStream;EventBatch poll(long time, TimeUnit tu);EventBatch take();@throws;;;see DFSInotifyEventInputStream#poll();true
org.apache.hadoop.hdfs.DFSInotifyEventInputStream;EventBatch poll(long time, TimeUnit tu);EventBatch take();@throws;;;see DFSInotifyEventInputStream#poll();false
org.apache.hadoop.hdfs.DFSInotifyEventInputStream;EventBatch poll(long time, TimeUnit tu);EventBatch take();@throws;;;see DFSInotifyEventInputStream#poll();false
org.apache.hadoop.hdfs.DFSInotifyEventInputStream;EventBatch poll(long time, TimeUnit tu);EventBatch take();@throws;;;see DFSInotifyEventInputStream#poll();true
org.apache.hadoop.hdfs.DFSInotifyEventInputStream;EventBatch poll(long time, TimeUnit tu);EventBatch take();@throws;;;if the calling thread is interrupted;true
org.apache.hadoop.hdfs.util.LightWeightHashSet;boolean add(T element);boolean addElem(T element);@return;;;true if the element was not present in the table, false otherwise;false
org.apache.hadoop.hdfs.util.LightWeightHashSet;boolean remove(Object key);LinkedElement removeElem(T key);@return;;;If such element exists, return true. Otherwise, return false.;false
org.apache.hadoop.hdfs.util.LightWeightLinkedSet;T pollFirst();List pollN(int n);@return;;;first element;false
org.apache.hadoop.hdfs.util.DataTransferThrottler; DataTransferThrottler(long bandwidthPerSec); DataTransferThrottler(long period, long bandwidthPerSec);@param;long bandwidthPerSec;long bandwidthPerSec;bandwidth allowed in bytes per second.;true
org.apache.hadoop.hdfs.util.DataTransferThrottler;void throttle(long numOfBytes);void throttle(long numOfBytes, Canceler canceler);@param;long numOfBytes;long numOfBytes;number of bytes sent/received since last time throttle was called;true
org.apache.hadoop.hdfs.util.Diff;UndoInfo delete(E element);UndoInfo modify(E oldElement, E newElement);@return;;;the undo information.;false
org.apache.hadoop.hdfs.DistributedFileSystem;boolean mkdir(Path f, FsPermission permission);boolean mkdirs(Path f, FsPermission permission);@param;Path f;Path f;The path to create;true
org.apache.hadoop.hdfs.DistributedFileSystem;boolean mkdir(Path f, FsPermission permission);boolean mkdirs(Path f, FsPermission permission);@param;FsPermission permission;FsPermission permission;The permission. See FsPermission#applyUMask for details about how this is used to calculate the effective permission.;true
org.apache.hadoop.hdfs.DistributedFileSystem;long getRawCapacity();long getRawUsed();Whole;;;replication. @deprecated Use {@link org.apache.hadoop.fs.FileSystem#getStatus()} instead    ;false
org.apache.hadoop.hdfs.DistributedFileSystem;FileStatus getFileStatus(Path f);boolean isFileClosed(Path src);@throws;;;if the file does not exist.;true
org.apache.hadoop.hdfs.DistributedFileSystem;long addCacheDirective(CacheDirectiveInfo info, EnumSet flags);void modifyCacheDirective(CacheDirectiveInfo info, EnumSet flags);@param;EnumSet<CacheFlag> flags;EnumSet<CacheFlag> flags;CacheFlags to use for this operation.;true
org.apache.hadoop.hdfs.DistributedFileSystem;void addCachePool(CachePoolInfo info);void modifyCachePool(CachePoolInfo info);@throws;;;If the request could not be completed.;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);Free text;;;Creates the namenode proxy with the passed protocol. This will handle creation of either HA- or non-HA-enabled proxy objects, depending upon if the provided URI is a configured logical URI. ;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);@return;;;an object containing both the proxy and the associated delegation token service it corresponds to;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);@param;Configuration conf;Configuration conf;the configuration containing the required IPC properties, client failover configurations, etc.;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);@param;URI nameNodeUri;URI nameNodeUri;the URI pointing either to a specific NameNode or to a logical nameservice.;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);@throws;;;if there is an error creating the proxy;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@param;Configuration conf;Configuration config;the configuration containing the required IPC properties, client failover configurations, etc.;false
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@param;URI nameNodeUri;URI nameNodeUri;the URI pointing either to a specific NameNode or to a logical nameservice.;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@throws;;;if there is an error creating the proxy;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);@return;;;an object containing both the proxy and the associated delegation token service it corresponds to;false
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@return;;;an object containing both the proxy and the associated delegation token service it corresponds to;false
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@param;Configuration conf;Configuration config;the configuration containing the required IPC properties, client failover configurations, etc.;false
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@param;URI nameNodeUri;URI nameNodeUri;the URI pointing either to a specific NameNode or to a logical nameservice.;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@param;AtomicBoolean fallbackToSimpleAuth;AtomicBoolean fallbackToSimpleAuth;set to true or false during calls to indicate if a secure client falls back to simple auth;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);@throws;;;if there is an error creating the proxy;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);@return;;;an object containing both the proxy and the associated delegation token service it corresponds to;false
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@return;;;an object containing both the proxy and the associated delegation token service it corresponds to;false
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxy(Configuration conf, URI nameNodeUri, Class xface, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createProxyWithLossyRetryHandler(Configuration config, URI nameNodeUri, Class xface, int numResponseToDrop, AtomicBoolean fallbackToSimpleAuth);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);Free text;;;Creates an explicitly non-HA-enabled proxy object. Most of the time you don't want to use this, and should instead use {@link NameNodeProxies#createProxy}. ;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@return;;;an object containing both the proxy and the associated delegation token service it corresponds to;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;Configuration conf;Configuration conf;the configuration object;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;InetSocketAddress nnAddr;InetSocketAddress nnAddr;address of the remote NN to connect to;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> xface;Class<T> xface;the IPC interface which should be created;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;UserGroupInformation ugi;UserGroupInformation ugi;the user who is making the calls on the proxy object;true
org.apache.hadoop.hdfs.NameNodeProxies;ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries);ProxyAndInfo createNonHAProxy(Configuration conf, InetSocketAddress nnAddr, Class xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth);@param;boolean withRetries;boolean withRetries;certain interfaces have a non-standard retry policy;true
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager; BlockTokenSecretManager(long keyUpdateInterval, long tokenLifetime, String blockPoolId, String encryptionAlgorithm); BlockTokenSecretManager(long keyUpdateInterval, long tokenLifetime, int nnIndex, String blockPoolId, String encryptionAlgorithm);@param;long keyUpdateInterval;long keyUpdateInterval;how often a new key will be generated;true
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager; BlockTokenSecretManager(long keyUpdateInterval, long tokenLifetime, String blockPoolId, String encryptionAlgorithm); BlockTokenSecretManager(long keyUpdateInterval, long tokenLifetime, int nnIndex, String blockPoolId, String encryptionAlgorithm);@param;long tokenLifetime;long tokenLifetime;how long an individual token is valid;true
org.apache.hadoop.hdfs.web.resources.OverwriteParam; OverwriteParam(Boolean value); OverwriteParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.ModificationTimeParam; ModificationTimeParam(Long value); ModificationTimeParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.LengthParam; LengthParam(Long value); LengthParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.RecursiveParam; RecursiveParam(Boolean value); RecursiveParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.ReplicationParam; ReplicationParam(Short value); ReplicationParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.BlockSizeParam; BlockSizeParam(Long value); BlockSizeParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.PermissionParam; PermissionParam(FsPermission value); PermissionParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.BufferSizeParam; BufferSizeParam(Integer value); BufferSizeParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.AccessTimeParam; AccessTimeParam(Long value); AccessTimeParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam; RenameOptionSetParam(Options.Rename options); RenameOptionSetParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.CreateParentParam; CreateParentParam(Boolean value); CreateParentParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.FsActionParam; FsActionParam(String str); FsActionParam(FsAction value);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.resources.OffsetParam; OffsetParam(Long value); OffsetParam(String str);Free text;;;Constructor. ;true
org.apache.hadoop.hdfs.web.URLConnectionFactory;URLConnection openConnection(URL url);URLConnection openConnection(URL url, boolean isSpnego);Free text;;;Opens a url with read and connect timeouts ;true
org.apache.hadoop.hdfs.web.URLConnectionFactory;URLConnection openConnection(URL url);URLConnection openConnection(URL url, boolean isSpnego);@return;;;URLConnection;true
org.apache.hadoop.hdfs.DFSClient;HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos, FileSystem.Statistics statistics);HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos, FileSystem.Statistics statistics, long startPos);Whole;;;Wraps the stream in a CryptoOutputStream if the underlying file is encrypted.    ;false
org.apache.hadoop.hdfs.DFSClient;OutputStream create(String src, boolean overwrite, short replication, long blockSize, Progressable progress, int buffersize);DFSOutputStream create(String src, FsPermission permission, EnumSet flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt);@return;;;output stream;true
org.apache.hadoop.hdfs.DFSClient;OutputStream create(String src, boolean overwrite, short replication, long blockSize, Progressable progress, int buffersize);DFSOutputStream create(String src, FsPermission permission, EnumSet flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt);@param;String src;String src;File name;true
org.apache.hadoop.hdfs.DFSClient;OutputStream create(String src, boolean overwrite, short replication, long blockSize, Progressable progress, int buffersize);DFSOutputStream create(String src, FsPermission permission, EnumSet flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt);@param;long blockSize;long blockSize;maximum block size;true
org.apache.hadoop.hdfs.DFSClient;OutputStream create(String src, boolean overwrite, short replication, long blockSize, Progressable progress, int buffersize);DFSOutputStream create(String src, FsPermission permission, EnumSet flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt);@param;Progressable progress;Progressable progress;interface for reporting client progress;true
org.apache.hadoop.hdfs.DFSClient;DFSOutputStream create(String src, FsPermission permission, EnumSet flag, boolean createParent, short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt);boolean mkdirs(String src, FsPermission permission, boolean createParent);@param;boolean createParent;boolean createParent;create missing parent directory if true;true
org.apache.hadoop.hdfs.DFSClient;HdfsDataOutputStream append(String src, int buffersize, Progressable progress, FileSystem.Statistics statistics);boolean setReplication(String src, short replication);@param;String src;String src;file name;true
org.apache.hadoop.hdfs.DFSClient;void setPermission(String src, FsPermission permission);void setOwner(String src, String username, String groupname);@param;String src;String src;path name.;true
org.apache.hadoop.hdfs.DFSClient;boolean primitiveMkdir(String src, FsPermission absPermission);boolean primitiveMkdir(String src, FsPermission absPermission, boolean createParent);Whole;;;Same {{@link #mkdirs(String, FsPermission, boolean)} except that the permissions has already been masked against umask.    ;false
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered; ReplicaWaitingToBeRecovered(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir); ReplicaWaitingToBeRecovered(Block block, FsVolumeSpi vol, File dir);Free text;;;Constructor ;true
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered; ReplicaWaitingToBeRecovered(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir); ReplicaWaitingToBeRecovered(Block block, FsVolumeSpi vol, File dir);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered; ReplicaWaitingToBeRecovered(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir); ReplicaWaitingToBeRecovered(Block block, FsVolumeSpi vol, File dir);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaInPipeline(Block block, FsVolumeSpi vol, File dir, Thread writer);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaInPipeline(Block block, FsVolumeSpi vol, File dir, Thread writer);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;long blockId;long blockId;block id;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;long genStamp;long genStamp;replica generation stamp;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;long bytesToReserve;long bytesToReserve;disk space to reserve for this replica, based on the estimated maximum block length.;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);Free text;;;Constructor ;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline; ReplicaInPipeline(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaInPipeline(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;Thread writer;Thread writer;a thread that is writing to this replica;true
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry;boolean processBlockMunlockRequest(ExtendedBlockId blockId);void processBlockInvalidation(ExtendedBlockId blockId);@param;ExtendedBlockId blockId;ExtendedBlockId blockId;The block ID.;true
org.apache.hadoop.hdfs.server.datanode.FinalizedReplica; FinalizedReplica(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir); FinalizedReplica(Block block, FsVolumeSpi vol, File dir);Free text;;;Constructor ;true
org.apache.hadoop.hdfs.server.datanode.FinalizedReplica; FinalizedReplica(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir); FinalizedReplica(Block block, FsVolumeSpi vol, File dir);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.FinalizedReplica; FinalizedReplica(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir); FinalizedReplica(Block block, FsVolumeSpi vol, File dir);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage;List loadBpStorageDirectories(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);@param;DataNode datanode;DataNode datanode;Datanode to which this storage belongs to;true
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage;List loadBpStorageDirectories(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);@param;NamespaceInfo nsInfo;NamespaceInfo nsInfo;namespace information;true
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage;List loadBpStorageDirectories(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);@param;Collection<File> dataDirs;Collection<File> dataDirs;storage directories of block pool;true
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage;List loadBpStorageDirectories(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);@param;StartupOption startOpt;StartupOption startOpt;startup option;true
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage;List loadBpStorageDirectories(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);@throws;;;on error;true
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage;List loadBpStorageDirectories(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void doUpgrade(DataNode datanode, StorageDirectory bpSd, NamespaceInfo nsInfo);@throws;;;on error;true
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage;void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void doUpgrade(DataNode datanode, StorageDirectory bpSd, NamespaceInfo nsInfo);@throws;;;on error;true
org.apache.hadoop.hdfs.server.datanode.DataNode;void runDatanodeDaemon();DataNode createDataNode(String[] args, Configuration conf);Whole;;; If this thread is specifically interrupted, it will stop waiting.    ;false
org.apache.hadoop.hdfs.server.datanode.DataNode;void runDatanodeDaemon();DataNode createDataNode(String[] args, Configuration conf, SecureResources resources);Whole;;; If this thread is specifically interrupted, it will stop waiting.    ;false
org.apache.hadoop.hdfs.server.datanode.DataNode;DataNode createDataNode(String[] args, Configuration conf);DataNode createDataNode(String[] args, Configuration conf, SecureResources resources);Whole;;; If this thread is specifically interrupted, it will stop waiting.    ;false
org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader;DataChecksum readDataChecksum(File metaFile);DataChecksum readDataChecksum(DataInputStream metaIn, Object name);@return;;;the data checksum obtained from the header.;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInfo; ReplicaInfo(Block block, FsVolumeSpi vol, File dir); ReplicaInfo(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir);Free text;;;Constructor ;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInfo; ReplicaInfo(Block block, FsVolumeSpi vol, File dir); ReplicaInfo(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaInfo; ReplicaInfo(Block block, FsVolumeSpi vol, File dir); ReplicaInfo(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.DataStorage;List addStorageLocations(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);@param;StartupOption startOpt;StartupOption startOpt;startup option;true
org.apache.hadoop.hdfs.server.datanode.DataStorage;void recoverTransitionRead(DataNode datanode, NamespaceInfo nsInfo, Collection dataDirs, StartupOption startOpt);void doUpgrade(DataNode datanode, StorageDirectory sd, NamespaceInfo nsInfo);@throws;;;on error;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList;FsVolumeImpl getNextVolume(StorageType storageType, long blockSize);FsVolumeImpl getNextTransientVolume(long blockSize);Free text;;;Get next volume. ;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList;FsVolumeImpl getNextVolume(StorageType storageType, long blockSize);FsVolumeImpl getNextTransientVolume(long blockSize);@return;;;next volume to store the block in.;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList;FsVolumeImpl getNextVolume(StorageType storageType, long blockSize);FsVolumeImpl getNextTransientVolume(long blockSize);@param;long blockSize;long blockSize;free space needed on the volume;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl;long getDfsUsed();long getBlockPoolUsed(String bpid);Whole;;;Return the total space used by dfs datanode    ;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl;List getFinalizedBlocks(String bpid);List getFinalizedBlocksOnPersistentStorage(String bpid);Whole;;;Get the list of finalized blocks from in-memory blockmap for a block pool.    ;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);ReplicaInfo get(String bpid, long blockId);@return;;;the replica's meta information;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);ReplicaInfo get(String bpid, long blockId);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);ReplicaInfo add(String bpid, ReplicaInfo replicaInfo);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);ReplicaInfo remove(String bpid, Block block);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);ReplicaInfo remove(String bpid, Block block);@param;Block block;Block block;block with its id as the key;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);ReplicaInfo remove(String bpid, long blockId);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);int size(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, Block block);Collection replicas(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, long blockId);ReplicaInfo add(String bpid, ReplicaInfo replicaInfo);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, long blockId);ReplicaInfo remove(String bpid, Block block);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, long blockId);ReplicaInfo remove(String bpid, long blockId);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, long blockId);int size(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo get(String bpid, long blockId);Collection replicas(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo add(String bpid, ReplicaInfo replicaInfo);ReplicaInfo remove(String bpid, Block block);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo add(String bpid, ReplicaInfo replicaInfo);ReplicaInfo remove(String bpid, long blockId);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo add(String bpid, ReplicaInfo replicaInfo);int size(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo add(String bpid, ReplicaInfo replicaInfo);Collection replicas(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo remove(String bpid, Block block);ReplicaInfo remove(String bpid, long blockId);@return;;;the removed replica's meta information;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo remove(String bpid, Block block);ReplicaInfo remove(String bpid, long blockId);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo remove(String bpid, Block block);int size(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo remove(String bpid, Block block);Collection replicas(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo remove(String bpid, long blockId);int size(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;ReplicaInfo remove(String bpid, long blockId);Collection replicas(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap;int size(String bpid);Collection replicas(String bpid);@param;String bpid;String bpid;block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;List getFinalizedBlocks(String bpid);List getFinalizedBlocksOnPersistentStorage(String bpid);Whole;;;  @return a list of finalized blocks for the given block pool. ;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, long blkoff, long ckoff);@return;;;an input stream to read the contents of the specified block, starting at the offset;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;InputStream getBlockInputStream(ExtendedBlock b, long seekOffset);void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams outs, int checksumSize);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);@return;;;the meta info of the replica which is being written to;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);@throws;;;if an error occurs;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);@return;;;the meta info of the replica which is being written to;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);@throws;;;if an error occurs;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@return;;;the meta info of the replica which is being written to;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createTemporary(StorageType storageType, ExtendedBlock b);void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams outs, int checksumSize);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);@return;;;the meta info of the replica which is being written to;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);@throws;;;if an error occurs;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@return;;;the meta info of the replica which is being written to;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface createRbw(StorageType storageType, ExtendedBlock b, boolean allowLazyPersist);void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams outs, int checksumSize);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long newGS;long newGS;the new generation stamp for the replica;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@return;;;the meta info of the replica which is being written to;false
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long newGS;long newGS;the new generation stamp for the replica;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long newGS;long newGS;the new generation stamp for the replica;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverRbw(ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd);void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams outs, int checksumSize);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long newGS;long newGS;the new generation stamp for the replica;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long expectedBlockLen;long expectedBlockLen;the number of bytes the replica is expected to have;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long newGS;long newGS;the new generation stamp for the replica;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long expectedBlockLen;long expectedBlockLen;the number of bytes the replica is expected to have;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface append(ExtendedBlock b, long newGS, long expectedBlockLen);void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams outs, int checksumSize);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long newGS;long newGS;the new generation stamp for the replica;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);@param;long expectedBlockLen;long expectedBlockLen;the number of bytes the replica is expected to have;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;ReplicaInPipelineInterface recoverAppend(ExtendedBlock b, long newGS, long expectedBlockLen);void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams outs, int checksumSize);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen);void adjustCrcChannelPosition(ExtendedBlock b, ReplicaOutputStreams outs, int checksumSize);@param;ExtendedBlock b;ExtendedBlock b;block;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;Map getBlockReports(String bpid);List getCacheReport(String bpid);@param;String bpid;String bpid;Block Pool Id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;void invalidate(String bpid, Block[] invalidBlks);void addBlockPool(String bpid, Configuration conf);@param;String bpid;String bpid;Block pool Id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;void cache(String bpid, long[] blockIds);void uncache(String bpid, long[] blockIds);@param;String bpid;String bpid;Block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;void cache(String bpid, long[] blockIds);boolean isCached(String bpid, long blockId);@param;String bpid;String bpid;Block pool id;true
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;void uncache(String bpid, long[] blockIds);boolean isCached(String bpid, long blockId);@param;String bpid;String bpid;Block pool id;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaBeingWritten(Block block, FsVolumeSpi vol, File dir, Thread writer);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaBeingWritten(Block block, FsVolumeSpi vol, File dir, Thread writer);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;long blockId;long blockId;block id;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;long genStamp;long genStamp;replica generation stamp;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(long blockId, long genStamp, FsVolumeSpi vol, File dir, long bytesToReserve); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;long bytesToReserve;long bytesToReserve;disk space to reserve for this replica, based on the estimated maximum block length.;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);Free text;;;Constructor ;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;FsVolumeSpi vol;FsVolumeSpi vol;volume where replica is located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;File dir;File dir;directory path where block and meta files are located;true
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten; ReplicaBeingWritten(Block block, FsVolumeSpi vol, File dir, Thread writer); ReplicaBeingWritten(long blockId, long len, long genStamp, FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve);@param;Thread writer;Thread writer;a thread that is writing to this replica;true
org.apache.hadoop.hdfs.server.namenode.TransferFsImage;void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid);void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid, Canceler canceler);@param;URL fsName;URL fsName;the http address for the remote NN;true
org.apache.hadoop.hdfs.server.namenode.TransferFsImage;void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid);void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid, Canceler canceler);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.server.namenode.TransferFsImage;void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid);void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid, Canceler canceler);@param;NNStorage storage;NNStorage storage;the storage directory to transfer the image from;true
org.apache.hadoop.hdfs.server.namenode.TransferFsImage;void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid);void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid, Canceler canceler);@param;NameNodeFile nnf;NameNodeFile nnf;the NameNodeFile type of the image;true
org.apache.hadoop.hdfs.server.namenode.TransferFsImage;void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid);void uploadImageFromStorage(URL fsName, Configuration conf, NNStorage storage, NameNodeFile nnf, long txid, Canceler canceler);@param;long txid;long txid;the transaction ID of the image to be uploaded;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem; FSNamesystem(Configuration conf, FSImage fsImage, boolean ignoreRetryCache);List getSharedEditsDirs(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;LocatedBlocks getBlockLocations(String clientMachine, String src, long offset, long length);LocatedBlocks getBlockLocations(String src, long offset, long length, boolean doAccessTime, boolean needBlockToken, boolean checkSafeMode);Free text;;;Get block locations within the specified range. @see ClientProtocol#getBlockLocations(String, long, long) ;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void concat(String target, String[] srcs);String closeFileCommitBlocks(INodeFile pendingFile, BlockInfo storedBlock);@throws;;;on error;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void concat(String target, String[] srcs);Token getDelegationToken(Text renewer);@throws;;;on error;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void concat(String target, String[] srcs);void cancelDelegationToken(Token token);@throws;;;on error;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;LocatedBlock prepareFileForWrite(String src, INodesInPath iip, String leaseHolder, String clientMachine, boolean writeToEditLog, boolean logRetryCache);BlockInfo saveAllocatedBlock(String src, INodesInPath inodes, Block newBlock, DatanodeStorageInfo[] targets);@param;String src;String src;path to the file;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void removeBlocks(BlocksMapUpdateInfo blocks);void removeBlocksAndUpdateSafemodeTotal(BlocksMapUpdateInfo blocks);@param;BlocksMapUpdateInfo blocks;BlocksMapUpdateInfo blocks;An instance of BlocksMapUpdateInfo which contains a list of blocks that need to be removed from blocksMap;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;HdfsFileStatus getFileInfo(String srcArg, boolean resolveLink);ContentSummary getContentSummary(String srcArg);@return;;;object containing information regarding the file or null if file not found;false
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;HdfsFileStatus getFileInfo(String srcArg, boolean resolveLink);ContentSummary getContentSummary(String srcArg);@param;String srcArg;String srcArg;The string representation of the path to the file;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;HdfsFileStatus getFileInfo(String srcArg, boolean resolveLink);ContentSummary getContentSummary(String srcArg);@throws;;;if access is denied;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;HdfsFileStatus getFileInfo(String srcArg, boolean resolveLink);ContentSummary getContentSummary(String srcArg);@throws;;;if a symlink is encountered.;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;HdfsFileStatus getFileInfo(String srcArg, boolean resolveLink);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;if access is denied;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;ContentSummary getContentSummary(String srcArg);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;if access is denied;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;String closeFileCommitBlocks(INodeFile pendingFile, BlockInfo storedBlock);Token getDelegationToken(Text renewer);@throws;;;on error;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;String closeFileCommitBlocks(INodeFile pendingFile, BlockInfo storedBlock);void cancelDelegationToken(Token token);@throws;;;on error;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void saveNamespace();boolean restoreFailedStorage(String arg);@throws;;;if superuser privilege is violated.;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void setGenerationStampV2(long stamp);long getGenerationStampV2();Whole;;;Gets the current generation stamp for this filesystem    ;false
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName);void updatePipeline(String clientName, ExtendedBlock oldBlock, ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs);@throws;;;if any error occurs;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;Token getDelegationToken(Text renewer);void cancelDelegationToken(Token token);@throws;;;on error;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void createEncryptionZone(String src, String keyName);EncryptionZone getEZForPath(String srcArg);@throws;;;if the caller is not the superuser.;true
org.apache.hadoop.hdfs.server.namenode.FSNamesystem;void createEncryptionZone(String src, String keyName);EncryptionZone getEZForPath(String srcArg);@throws;;;if the path can't be resolved.;true
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotFSImageFormat;void loadSnapshotList(INodeDirectory snapshottableParent, int numSnapshots, DataInput in, FSImageFormat.Loader loader);void loadDirectoryDiffList(INodeDirectory dir, DataInput in, FSImageFormat.Loader loader);@param;INodeDirectory snapshottableParent;INodeDirectory dir;The snapshottable directory for loading.;false
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotFSImageFormat;void loadSnapshotList(INodeDirectory snapshottableParent, int numSnapshots, DataInput in, FSImageFormat.Loader loader);void loadDirectoryDiffList(INodeDirectory dir, DataInput in, FSImageFormat.Loader loader);@param;FSImageFormat.Loader loader;FSImageFormat.Loader loader;The loader;true
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager;INodeDirectory getSnapshottableRoot(String path);String createSnapshot(String path, String snapshotName);@param;String path;String path;The directory path where the snapshot will be taken.;true
org.apache.hadoop.hdfs.server.namenode.FSImageSerialization;void writeINodeFile(INodeFile file, DataOutput out, boolean writeUnderConstruction);void writeINodeDirectory(INodeDirectory node, DataOutput out);@param; nullParamName;INodeDirectory node;The node to write;false
org.apache.hadoop.hdfs.server.namenode.FSImageSerialization;void writeINodeFile(INodeFile file, DataOutput out, boolean writeUnderConstruction);void writeINodeDirectoryAttributes(INodeDirectoryAttributes a, DataOutput out);@param; nullParamName;INodeDirectoryAttributes a;The node to write;false
org.apache.hadoop.hdfs.server.namenode.FSImageSerialization;void writeINodeDirectory(INodeDirectory node, DataOutput out);void writeINodeDirectoryAttributes(INodeDirectoryAttributes a, DataOutput out);Whole;;;Serialize a {@link INodeDirectory}  @param The node to write@param The DataOutput where the fields are written  ;false
org.apache.hadoop.hdfs.server.namenode.INode;String getUserName(int snapshotId);String getGroupName(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getUserName(int snapshotId);FsPermission getFsPermission(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getUserName(int snapshotId);XAttrFeature getXAttrFeature(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getUserName(int snapshotId);long getModificationTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getUserName(int snapshotId);long getAccessTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getGroupName(int snapshotId);FsPermission getFsPermission(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getGroupName(int snapshotId);XAttrFeature getXAttrFeature(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getGroupName(int snapshotId);long getModificationTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;String getGroupName(int snapshotId);long getAccessTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;FsPermission getFsPermission(int snapshotId);XAttrFeature getXAttrFeature(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;FsPermission getFsPermission(int snapshotId);long getModificationTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;FsPermission getFsPermission(int snapshotId);long getAccessTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;XAttrFeature getXAttrFeature(int snapshotId);long getModificationTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;XAttrFeature getXAttrFeature(int snapshotId);long getAccessTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;Quota.Counts cleanSubtree(int snapshotId, int priorSnapshotId, BlocksMapUpdateInfo collectedBlocks, List removedINodes, boolean countDiffChange);void destroyAndCollectBlocks(BlocksMapUpdateInfo collectedBlocks, List removedINodes);@param;List<INode> removedINodes;List<INode> removedINodes;INodes collected from the descents for further cleaning up of inodeMap;true
org.apache.hadoop.hdfs.server.namenode.INode;void addSpaceConsumed(long nsDelta, long dsDelta, boolean verify);void addSpaceConsumed2Parent(long nsDelta, long dsDelta, boolean verify);Whole;;;Check and add namespace/diskspace consumed to itself and the ancestors.    @throws if quote is violated.;false
org.apache.hadoop.hdfs.server.namenode.INode;long getModificationTime(int snapshotId);long getAccessTime(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the given snapshot, otherwise, get the result from the current inode.;true
org.apache.hadoop.hdfs.server.namenode.INode;void setAccessTime(long accessTime);INode setAccessTime(long accessTime, int latestSnapshotId);Whole;;;Set last access time of inode.    ;false
org.apache.hadoop.hdfs.server.namenode.XAttrStorage;List readINodeXAttrs(INode inode, int snapshotId);List readINodeXAttrs(INode inode);@return;;;List<XAttr> XAttr list.;true
org.apache.hadoop.hdfs.server.namenode.NNStorage;Collection getImageDirectories();Collection getEditsDirectories();@throws;;;in case of URI processing error;true
org.apache.hadoop.hdfs.server.namenode.NNStorage;void writeTransactionIdFile(StorageDirectory sd, long txid);void setPropertiesFromFields(Properties props, StorageDirectory sd);@param;StorageDirectory sd;StorageDirectory sd;storage directory;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);boolean unprotectedRenameTo(String src, String dst, long timestamp, Options.Rename options);@param;String src;String src;source path;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);boolean unprotectedRenameTo(String src, String dst, long timestamp, Options.Rename options);@param;String dst;String dst;destination path;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);boolean unprotectedRenameTo(String src, String dst, long timestamp, BlocksMapUpdateInfo collectedBlocks, Options.Rename options);@param;String src;String src;source path;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);boolean unprotectedRenameTo(String src, String dst, long timestamp, BlocksMapUpdateInfo collectedBlocks, Options.Rename options);@param;String dst;String dst;destination path;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);void unprotectedDelete(String src, long mtime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);INode getINode4Write(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);boolean isValidToCreate(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);boolean isDirMutable(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);INodeDirectory setQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp);INodesInPath getINodesInPath4Write(String src, boolean resolveLink);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp, Options.Rename options);boolean unprotectedRenameTo(String src, String dst, long timestamp, BlocksMapUpdateInfo collectedBlocks, Options.Rename options);@param;String src;String src;source path;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp, Options.Rename options);boolean unprotectedRenameTo(String src, String dst, long timestamp, BlocksMapUpdateInfo collectedBlocks, Options.Rename options);@param;String dst;String dst;destination path;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp, Options.Rename options);boolean unprotectedRenameTo(String src, String dst, long timestamp, BlocksMapUpdateInfo collectedBlocks, Options.Rename options);@param;long timestamp;long timestamp;modification time;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean unprotectedRenameTo(String src, String dst, long timestamp, Options.Rename options);boolean unprotectedRenameTo(String src, String dst, long timestamp, BlocksMapUpdateInfo collectedBlocks, Options.Rename options);@param;Options.Rename options;Options.Rename options;Rename options;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void concat(String target, String[] srcs, long timestamp);void unprotectedConcat(String target, String[] srcs, long timestamp);Free text;;;Concat all the blocks from srcs to trg and delete the srcs files ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void unprotectedDelete(String src, long mtime);long unprotectedDelete(INodesInPath iip, BlocksMapUpdateInfo collectedBlocks, List removedINodes, long mtime);@param;long mtime;long mtime;the time the inode is removed;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void unprotectedDelete(String src, long mtime);INode getINode4Write(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void unprotectedDelete(String src, long mtime);boolean isValidToCreate(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void unprotectedDelete(String src, long mtime);boolean isDirMutable(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void unprotectedDelete(String src, long mtime);INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void unprotectedDelete(String src, long mtime);INodeDirectory setQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void unprotectedDelete(String src, long mtime);INodesInPath getINodesInPath4Write(String src, boolean resolveLink);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode(String src);INodesInPath getLastINodeInPath(String src);Whole;;;Get {@link INode} associated with the file / directory.    ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode(String src);INodesInPath getINodesInPath4Write(String src);Whole;;;Get {@link INode} associated with the file / directory.    ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode(String src);INode getINode4Write(String src);Free text;;;Get {@link INode} associated with the file / directory. ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INodesInPath getLastINodeInPath(String src);INodesInPath getINodesInPath4Write(String src);Whole;;;Get {@link INode} associated with the file / directory.    ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INodesInPath getLastINodeInPath(String src);INode getINode4Write(String src);Free text;;;Get {@link INode} associated with the file / directory. ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INodesInPath getINodesInPath4Write(String src);INode getINode4Write(String src);Free text;;;Get {@link INode} associated with the file / directory. ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode4Write(String src);boolean isValidToCreate(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode4Write(String src);boolean isDirMutable(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode4Write(String src);INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode4Write(String src);INodeDirectory setQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INode getINode4Write(String src);INodesInPath getINodesInPath4Write(String src, boolean resolveLink);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isValidToCreate(String src);boolean isDirMutable(String src);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isValidToCreate(String src);INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isValidToCreate(String src);INodeDirectory setQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isValidToCreate(String src);INodesInPath getINodesInPath4Write(String src, boolean resolveLink);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isDir(String src);boolean isDirMutable(String src);Free text;;;Check whether the path specifies a directory ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isDirMutable(String src);INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isDirMutable(String src);INodeDirectory setQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;boolean isDirMutable(String src);INodesInPath getINodesInPath4Write(String src, boolean resolveLink);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;void addToInodeMap(INode inode);void removeFromInodeMap(List inodes);Whole;;;This method is always called with writeLock of FSDirectory held.    ;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);INodeDirectory setQuota(String src, long nsQuota, long dsQuota);@return;;;INodeDirectory if any of the quotas have changed. null otherwise.;false
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);INodeDirectory setQuota(String src, long nsQuota, long dsQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota);INodesInPath getINodesInPath4Write(String src, boolean resolveLink);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.FSDirectory;INodeDirectory setQuota(String src, long nsQuota, long dsQuota);INodesInPath getINodesInPath4Write(String src, boolean resolveLink);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream;FSEditLogOp readOp();FSEditLogOp nextOp();@return;;;an operation from the stream or null if at end of stream;false
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream;FSEditLogOp readOp();FSEditLogOp nextOp();@throws;;;if there is an error reading from the stream;true
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream;FSEditLogOp readOp();FSEditLogOp nextValidOp();@return;;;an operation from the stream or null if at end of stream;false
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream;FSEditLogOp nextOp();FSEditLogOp nextValidOp();@return;;;an operation from the stream or null if at end of stream;false
org.apache.hadoop.hdfs.server.namenode.FSImage; FSImage(Configuration conf); FSImage(Configuration conf, Collection imageDirs, List editsDirs);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List filterDefaultAclEntries(List existingAcl);@return;;;List<AclEntry> new ACL;false
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List filterDefaultAclEntries(List existingAcl);@param;List<AclEntry> existingAcl;List<AclEntry> existingAcl;List<AclEntry> existing ACL;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List filterDefaultAclEntries(List existingAcl);@throws;;;if validation fails;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List mergeAclEntries(List existingAcl, List inAclSpec);@return;;;List<AclEntry> new ACL;false
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List mergeAclEntries(List existingAcl, List inAclSpec);@param;List<AclEntry> existingAcl;List<AclEntry> existingAcl;List<AclEntry> existing ACL;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List mergeAclEntries(List existingAcl, List inAclSpec);@throws;;;if validation fails;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List replaceAclEntries(List existingAcl, List inAclSpec);@return;;;List<AclEntry> new ACL;false
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List replaceAclEntries(List existingAcl, List inAclSpec);@param;List<AclEntry> existingAcl;List<AclEntry> existingAcl;List<AclEntry> existing ACL;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterAclEntriesByAclSpec(List existingAcl, List inAclSpec);List replaceAclEntries(List existingAcl, List inAclSpec);@throws;;;if validation fails;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterDefaultAclEntries(List existingAcl);List mergeAclEntries(List existingAcl, List inAclSpec);@return;;;List<AclEntry> new ACL;false
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterDefaultAclEntries(List existingAcl);List mergeAclEntries(List existingAcl, List inAclSpec);@param;List<AclEntry> existingAcl;List<AclEntry> existingAcl;List<AclEntry> existing ACL;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterDefaultAclEntries(List existingAcl);List mergeAclEntries(List existingAcl, List inAclSpec);@throws;;;if validation fails;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterDefaultAclEntries(List existingAcl);List replaceAclEntries(List existingAcl, List inAclSpec);@return;;;List<AclEntry> new ACL;false
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterDefaultAclEntries(List existingAcl);List replaceAclEntries(List existingAcl, List inAclSpec);@param;List<AclEntry> existingAcl;List<AclEntry> existingAcl;List<AclEntry> existing ACL;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List filterDefaultAclEntries(List existingAcl);List replaceAclEntries(List existingAcl, List inAclSpec);@throws;;;if validation fails;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List mergeAclEntries(List existingAcl, List inAclSpec);List replaceAclEntries(List existingAcl, List inAclSpec);@return;;;List<AclEntry> new ACL;false
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List mergeAclEntries(List existingAcl, List inAclSpec);List replaceAclEntries(List existingAcl, List inAclSpec);@param;List<AclEntry> existingAcl;List<AclEntry> existingAcl;List<AclEntry> existing ACL;true
org.apache.hadoop.hdfs.server.namenode.AclTransformation;List mergeAclEntries(List existingAcl, List inAclSpec);List replaceAclEntries(List existingAcl, List inAclSpec);@throws;;;if validation fails;true
org.apache.hadoop.hdfs.server.namenode.NameNode;String getServiceRpcServerBindHost(Configuration conf);String getRpcServerBindHost(Configuration conf);Whole;;; If the bind host is not configured returns null.    ;false
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void setStateInternal(HAContext context, HAState s);void setState(HAContext context, HAState s);@param;HAState s;HAState s;new state;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void setStateInternal(HAContext context, HAState s);void setState(HAContext context, HAState s);@throws;;;on failure to transition to new state.;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToEnterState(HAContext context);void enterState(HAContext context);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToEnterState(HAContext context);void prepareToExitState(HAContext context);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToEnterState(HAContext context);void prepareToExitState(HAContext context);@throws;;;on precondition failure;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToEnterState(HAContext context);void exitState(HAContext context);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToEnterState(HAContext context);void setState(HAContext context, HAState s);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToEnterState(HAContext context);void checkOperation(HAContext context, OperationCategory op);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void enterState(HAContext context);void prepareToExitState(HAContext context);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void enterState(HAContext context);void exitState(HAContext context);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void enterState(HAContext context);void exitState(HAContext context);@throws;;;on failure to enter the state.;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void enterState(HAContext context);void setState(HAContext context, HAState s);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void enterState(HAContext context);void checkOperation(HAContext context, OperationCategory op);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToExitState(HAContext context);void exitState(HAContext context);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToExitState(HAContext context);void setState(HAContext context, HAState s);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void prepareToExitState(HAContext context);void checkOperation(HAContext context, OperationCategory op);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void exitState(HAContext context);void setState(HAContext context, HAState s);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void exitState(HAContext context);void checkOperation(HAContext context, OperationCategory op);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.ha.HAState;void setState(HAContext context, HAState s);void checkOperation(HAContext context, OperationCategory op);@param;HAContext context;HAContext context;HA context;true
org.apache.hadoop.hdfs.server.namenode.FSEditLog;void logEdit(FSEditLogOp op);void logEdit(int length, byte[] data);Whole;;;Write an operation to the edit log. Do not sync to persistent store yet.    ;false
org.apache.hadoop.hdfs.server.namenode.FSEditLog;JournalSet getJournalSet();void setMetricsForTests(NameNodeMetrics metrics);Whole;;;Used only by tests.    ;false
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressMetrics; StartupProgressMetrics(StartupProgress startupProgress);void register(StartupProgress prog);@param;StartupProgress startupProgress;StartupProgress prog;StartupProgress to link;false
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void beginPhase(Phase phase);void beginStep(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to begin;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void endPhase(Phase phase);void endStep(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to end;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;Status getStatus(Phase phase);Counter getCounter(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setCount(Phase phase, Step step, long count);void setFile(Phase phase, String file);@param;Phase phase;Phase phase;Phase to set;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setCount(Phase phase, Step step, long count);void setSize(Phase phase, long size);@param;Phase phase;Phase phase;Phase to set;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setCount(Phase phase, Step step, long count);void setSize(Phase phase, long size);@param;long count;long size;long to set;false
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setCount(Phase phase, Step step, long count);void setTotal(Phase phase, Step step, long total);@param;Phase phase;Phase phase;Phase to set;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setCount(Phase phase, Step step, long count);void setTotal(Phase phase, Step step, long total);@param;Step step;Step step;Step to set;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setCount(Phase phase, Step step, long count);void setTotal(Phase phase, Step step, long total);@param;long count;long total;long to set;false
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setFile(Phase phase, String file);void setSize(Phase phase, long size);@param;Phase phase;Phase phase;Phase to set;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setFile(Phase phase, String file);void setTotal(Phase phase, Step step, long total);@param;Phase phase;Phase phase;Phase to set;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setSize(Phase phase, long size);void setTotal(Phase phase, Step step, long total);@param;Phase phase;Phase phase;Phase to set;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;void setSize(Phase phase, long size);void setTotal(Phase phase, Step step, long total);@param;long size;long total;long to set;false
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type); Step(String file);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type); Step(String file, long size);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type); Step(StepType type, String file);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type); Step(StepType type, String file);@param;StepType type;StepType type;StepType type of step;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type); Step(StepType type, String file, long size);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type); Step(StepType type, String file, long size);@param;StepType type;StepType type;StepType type of step;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file); Step(String file, long size);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file); Step(String file, long size);@param;String file;String file;String file;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file); Step(StepType type, String file);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file); Step(StepType type, String file);@param;String file;String file;String file;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file); Step(StepType type, String file, long size);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file); Step(StepType type, String file, long size);@param;String file;String file;String file;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file, long size); Step(StepType type, String file);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file, long size); Step(StepType type, String file);@param;String file;String file;String file;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file, long size); Step(StepType type, String file, long size);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file, long size); Step(StepType type, String file, long size);@param;String file;String file;String file;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(String file, long size); Step(StepType type, String file, long size);@param;long size;long size;long size in bytes;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type, String file); Step(StepType type, String file, long size);Free text;;;Creates a new Step. ;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type, String file); Step(StepType type, String file, long size);@param;StepType type;StepType type;StepType type of step;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step; Step(StepType type, String file); Step(StepType type, String file, long size);@param;String file;String file;String file;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);long getCount(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);long getElapsedTime(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);long getElapsedTime(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);String getFile(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);float getPercentComplete(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);float getPercentComplete(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);Iterable getSteps(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);long getElapsedTime(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);long getElapsedTime(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);long getElapsedTime(Phase phase, Step step);@param;Step step;Step step;Step to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);String getFile(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);float getPercentComplete(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);float getPercentComplete(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);float getPercentComplete(Phase phase, Step step);@param;Step step;Step step;Step to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);Iterable getSteps(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getCount(Phase phase, Step step);long getTotal(Phase phase, Step step);@param;Step step;Step step;Step to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime();long getElapsedTime(Phase phase);@return;;;long elapsed time;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime();long getElapsedTime(Phase phase, Step step);@return;;;long elapsed time;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);long getElapsedTime(Phase phase, Step step);@return;;;long elapsed time;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);long getElapsedTime(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);String getFile(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);float getPercentComplete(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);float getPercentComplete(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);Iterable getSteps(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);String getFile(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);float getPercentComplete(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);float getPercentComplete(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);float getPercentComplete(Phase phase, Step step);@param;Step step;Step step;Step to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);Iterable getSteps(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getElapsedTime(Phase phase, Step step);long getTotal(Phase phase, Step step);@param;Step step;Step step;Step to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;String getFile(Phase phase);float getPercentComplete(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;String getFile(Phase phase);float getPercentComplete(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;String getFile(Phase phase);Iterable getSteps(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;String getFile(Phase phase);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;String getFile(Phase phase);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;String getFile(Phase phase);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;String getFile(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete();float getPercentComplete(Phase phase);@return;;;float percent complete;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete();float getPercentComplete(Phase phase, Step step);@return;;;float percent complete;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase);float getPercentComplete(Phase phase, Step step);@return;;;float percent complete;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase);float getPercentComplete(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase);Iterable getSteps(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase, Step step);Iterable getSteps(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase, Step step);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase, Step step);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase, Step step);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase, Step step);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;float getPercentComplete(Phase phase, Step step);long getTotal(Phase phase, Step step);@param;Step step;Step step;Step to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;Iterable getSteps(Phase phase);long getSize(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;Iterable getSteps(Phase phase);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;Iterable getSteps(Phase phase);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;Iterable getSteps(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getSize(Phase phase);Status getStatus(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getSize(Phase phase);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getSize(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;Status getStatus(Phase phase);long getTotal(Phase phase);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;Status getStatus(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView;long getTotal(Phase phase);long getTotal(Phase phase, Step step);@param;Phase phase;Phase phase;Phase to get;true
org.apache.hadoop.hdfs.server.namenode.AclStorage;List readINodeAcl(INode inode, int snapshotId);List readINodeLogicalAcl(INode inode);@param;INode inode;INode inode;INode to read;true
org.apache.hadoop.hdfs.server.namenode.AclStorage;void removeINodeAcl(INode inode, int snapshotId);void updateINodeAcl(INode inode, List newAcl, int snapshotId);@param;INode inode;INode inode;INode to update;true
org.apache.hadoop.hdfs.server.namenode.AclStorage;void removeINodeAcl(INode inode, int snapshotId);void updateINodeAcl(INode inode, List newAcl, int snapshotId);@param;int snapshotId;int snapshotId;int latest snapshot ID of inode;true
org.apache.hadoop.hdfs.server.namenode.AclStorage;void removeINodeAcl(INode inode, int snapshotId);void updateINodeAcl(INode inode, List newAcl, int snapshotId);@throws;;;if quota limit is exceeded;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;boolean canRollBack(StorageDirectory sd, StorageInfo storage, StorageInfo prevStorage, int targetLayoutVersion);void doFinalize(StorageDirectory sd);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;boolean canRollBack(StorageDirectory sd, StorageInfo storage, StorageInfo prevStorage, int targetLayoutVersion);void doPreUpgrade(Configuration conf, StorageDirectory sd);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;boolean canRollBack(StorageDirectory sd, StorageInfo storage, StorageInfo prevStorage, int targetLayoutVersion);void doUpgrade(StorageDirectory sd, Storage storage);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;boolean canRollBack(StorageDirectory sd, StorageInfo storage, StorageInfo prevStorage, int targetLayoutVersion);void doRollBack(StorageDirectory sd);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;void doFinalize(StorageDirectory sd);void doPreUpgrade(Configuration conf, StorageDirectory sd);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;void doFinalize(StorageDirectory sd);void doUpgrade(StorageDirectory sd, Storage storage);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;void doFinalize(StorageDirectory sd);void doRollBack(StorageDirectory sd);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;void doPreUpgrade(Configuration conf, StorageDirectory sd);void doUpgrade(StorageDirectory sd, Storage storage);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;void doPreUpgrade(Configuration conf, StorageDirectory sd);void doRollBack(StorageDirectory sd);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil;void doUpgrade(StorageDirectory sd, Storage storage);void doRollBack(StorageDirectory sd);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream; EditLogFileInputStream(File name); EditLogFileInputStream(File name, long firstTxId, long lastTxId, boolean isInProgress);@param;File name;File name;filename to open;true
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager;void addEncryptionZone(Long inodeId, CipherSuite suite, CryptoProtocolVersion version, String keyName);void unprotectedAddEncryptionZone(Long inodeId, CipherSuite suite, CryptoProtocolVersion version, String keyName);@param;Long inodeId;Long inodeId;of the encryption zone;true
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager;void addEncryptionZone(Long inodeId, CipherSuite suite, CryptoProtocolVersion version, String keyName);void unprotectedAddEncryptionZone(Long inodeId, CipherSuite suite, CryptoProtocolVersion version, String keyName);@param;String keyName;String keyName;encryption zone key name;true
org.apache.hadoop.hdfs.server.namenode.INodeDirectory;INode getChild(byte[] name, int snapshotId);ReadOnlyList getChildrenList(int snapshotId);@param;int snapshotId;int snapshotId;if it is not Snapshot#CURRENT_STATE_ID, get the result from the corresponding snapshot, otherwise, get the result from the current directory.;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void journal(JournalInfo journalInfo, long epoch, long firstTxnId, int numTxns, byte[] records);void startLogSegment(JournalInfo journalInfo, long epoch, long txid);@param;JournalInfo journalInfo;JournalInfo journalInfo;journal information;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void journal(JournalInfo journalInfo, long epoch, long firstTxnId, int numTxns, byte[] records);void startLogSegment(JournalInfo journalInfo, long epoch, long txid);@param;long epoch;long epoch;marks beginning a new journal writer;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void journal(JournalInfo journalInfo, long epoch, long firstTxnId, int numTxns, byte[] records);void startLogSegment(JournalInfo journalInfo, long epoch, long txid);@throws;;;if the resource has been fenced;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void journal(JournalInfo journalInfo, long epoch, long firstTxnId, int numTxns, byte[] records);FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo);@param;JournalInfo journalInfo;JournalInfo journalInfo;journal information;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void journal(JournalInfo journalInfo, long epoch, long firstTxnId, int numTxns, byte[] records);FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo);@param;long epoch;long epoch;marks beginning a new journal writer;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void journal(JournalInfo journalInfo, long epoch, long firstTxnId, int numTxns, byte[] records);FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo);@throws;;;if the resource has been fenced;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void startLogSegment(JournalInfo journalInfo, long epoch, long txid);FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo);@param;JournalInfo journalInfo;JournalInfo journalInfo;journal information;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void startLogSegment(JournalInfo journalInfo, long epoch, long txid);FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo);@param;long epoch;long epoch;marks beginning a new journal writer;true
org.apache.hadoop.hdfs.server.protocol.JournalProtocol;void startLogSegment(JournalInfo journalInfo, long epoch, long txid);FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo);@throws;;;if the resource has been fenced;true
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol;NamenodeCommand startCheckpoint(NamenodeRegistration registration);void endCheckpoint(NamenodeRegistration registration, CheckpointSignature sig);@param;NamenodeRegistration registration;NamenodeRegistration registration;the requesting node;true
org.apache.hadoop.hdfs.server.protocol.BlockCommand; BlockCommand(int action, String poolId, Block[] blocks); BlockCommand(int action, String poolId, Block[] blocks, DatanodeInfo[][] targets, StorageType[][] targetStorageTypes, String[][] targetStorageIDs);Whole;;;Create BlockCommand for the given action  @param blocks related to the action  ;false
org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol;DatanodeRegistration registerDatanode(DatanodeRegistration registration);HeartbeatResponse sendHeartbeat(DatanodeRegistration registration, StorageReport[] reports, long dnCacheCapacity, long dnCacheUsed, int xmitsInProgress, int xceiverCount, int failedVolumes);@param;DatanodeRegistration registration;DatanodeRegistration registration;datanode registration information;true
org.apache.hadoop.hdfs.server.common.Storage;Iterator dirIterator(boolean includeShared);Iterator dirIterator(StorageDirType dirType, boolean includeShared);@return;;;an iterator over the configured storage dirs.;true
org.apache.hadoop.hdfs.server.common.Util;URI stringAsURI(String s);URI fileAsURI(File f);@return;;;the resulting URI;false
org.apache.hadoop.hdfs.server.common.JspHelper;String getDelegationTokenUrlParam(String tokenString);String getUrlParam(String name, String val, String paramSeparator);@return;;;url parameter;false
org.apache.hadoop.hdfs.server.common.JspHelper;String getDelegationTokenUrlParam(String tokenString);String getUrlParam(String name, String val, boolean firstParam);@return;;;url parameter;false
org.apache.hadoop.hdfs.server.common.JspHelper;String getDelegationTokenUrlParam(String tokenString);String getUrlParam(String name, String val);@return;;;url parameter;false
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, String paramSeparator);String getUrlParam(String name, String val, boolean firstParam);@return;;;url parameter;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, String paramSeparator);String getUrlParam(String name, String val, boolean firstParam);@param;String name;String name;parameter name;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, String paramSeparator);String getUrlParam(String name, String val, boolean firstParam);@param;String val;String val;parameter value;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, String paramSeparator);String getUrlParam(String name, String val);@return;;;url parameter;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, String paramSeparator);String getUrlParam(String name, String val);@param;String name;String name;parameter name;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, String paramSeparator);String getUrlParam(String name, String val);@param;String val;String val;parameter value;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, boolean firstParam);String getUrlParam(String name, String val);@return;;;url parameter;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, boolean firstParam);String getUrlParam(String name, String val);@param;String name;String name;parameter name;true
org.apache.hadoop.hdfs.server.common.JspHelper;String getUrlParam(String name, String val, boolean firstParam);String getUrlParam(String name, String val);@param;String val;String val;parameter value;true
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager;DatanodeDescriptor getDatanodeByHost(String host);DatanodeDescriptor getDatanodeByXferAddr(String host, int xferPort);Whole;;;  @return the datanode descriptor for the host. ;false
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor; DatanodeDescriptor(DatanodeID nodeID); DatanodeDescriptor(DatanodeID nodeID, String networkLocation);Free text;;;DatanodeDescriptor constructor ;true
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor; DatanodeDescriptor(DatanodeID nodeID); DatanodeDescriptor(DatanodeID nodeID, String networkLocation);@param;DatanodeID nodeID;DatanodeID nodeID;id of the data node;true
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;boolean removeBlock(BlockInfo b);boolean removeBlock(String storageID, BlockInfo b);Whole;;;Remove block from the list of blocks belonging to the data-node. Remove data-node from the block.    ;false
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;boolean findDatanode(DatanodeDescriptor dn);int findStorageInfo(DatanodeStorageInfo storageInfo);@return;;;index or -1 if not found.;false
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;DatanodeStorageInfo findStorageInfo(DatanodeDescriptor dn);int findStorageInfo(DatanodeStorageInfo storageInfo);Free text;;;Find specified DatanodeStorageInfo. ;true
org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks;boolean add(Block block, int curReplicas, int decomissionedReplicas, int expectedReplicas);void update(Block block, int curReplicas, int decommissionedReplicas, int curExpectedReplicas, int curReplicasDelta, int expectedReplicasDelta);@param;int curReplicas;int curReplicas;current number of replicas of the block;true
org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks;boolean add(Block block, int curReplicas, int decomissionedReplicas, int expectedReplicas);void update(Block block, int curReplicas, int decommissionedReplicas, int curExpectedReplicas, int curReplicasDelta, int expectedReplicasDelta);@param;int decomissionedReplicas;int decommissionedReplicas;the number of decommissioned replicas;false
org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks;boolean add(Block block, int curReplicas, int decomissionedReplicas, int expectedReplicas);void update(Block block, int curReplicas, int decommissionedReplicas, int curExpectedReplicas, int curReplicasDelta, int expectedReplicasDelta);@param;int expectedReplicas;int curExpectedReplicas;expected number of replicas of the block;false
org.apache.hadoop.hdfs.protocol.HdfsFileStatus;String getFullName(String parent);Path getFullPath(Path parent);@param;String parent;Path parent;the parent path;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void readBlock(ExtendedBlock blk, Token blockToken, String clientName, long blockOffset, long length, boolean sendChecksum, CachingStrategy cachingStrategy);void writeBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes, DatanodeInfo source, BlockConstructionStage stage, int pipelineSize, long minBytesRcvd, long maxBytesRcvd, long latestGenerationStamp, DataChecksum requestedChecksum, CachingStrategy cachingStrategy, boolean allowLazyPersist);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void readBlock(ExtendedBlock blk, Token blockToken, String clientName, long blockOffset, long length, boolean sendChecksum, CachingStrategy cachingStrategy);void writeBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes, DatanodeInfo source, BlockConstructionStage stage, int pipelineSize, long minBytesRcvd, long maxBytesRcvd, long latestGenerationStamp, DataChecksum requestedChecksum, CachingStrategy cachingStrategy, boolean allowLazyPersist);@param;String clientName;String clientName;client's name.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void readBlock(ExtendedBlock blk, Token blockToken, String clientName, long blockOffset, long length, boolean sendChecksum, CachingStrategy cachingStrategy);void transferBlock(ExtendedBlock blk, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void readBlock(ExtendedBlock blk, Token blockToken, String clientName, long blockOffset, long length, boolean sendChecksum, CachingStrategy cachingStrategy);void transferBlock(ExtendedBlock blk, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes);@param;String clientName;String clientName;client's name.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void readBlock(ExtendedBlock blk, Token blockToken, String clientName, long blockOffset, long length, boolean sendChecksum, CachingStrategy cachingStrategy);void replaceBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String delHint, DatanodeInfo source);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void readBlock(ExtendedBlock blk, Token blockToken, String clientName, long blockOffset, long length, boolean sendChecksum, CachingStrategy cachingStrategy);void copyBlock(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void readBlock(ExtendedBlock blk, Token blockToken, String clientName, long blockOffset, long length, boolean sendChecksum, CachingStrategy cachingStrategy);void blockChecksum(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void writeBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes, DatanodeInfo source, BlockConstructionStage stage, int pipelineSize, long minBytesRcvd, long maxBytesRcvd, long latestGenerationStamp, DataChecksum requestedChecksum, CachingStrategy cachingStrategy, boolean allowLazyPersist);void transferBlock(ExtendedBlock blk, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void writeBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes, DatanodeInfo source, BlockConstructionStage stage, int pipelineSize, long minBytesRcvd, long maxBytesRcvd, long latestGenerationStamp, DataChecksum requestedChecksum, CachingStrategy cachingStrategy, boolean allowLazyPersist);void transferBlock(ExtendedBlock blk, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes);@param;String clientName;String clientName;client's name.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void writeBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes, DatanodeInfo source, BlockConstructionStage stage, int pipelineSize, long minBytesRcvd, long maxBytesRcvd, long latestGenerationStamp, DataChecksum requestedChecksum, CachingStrategy cachingStrategy, boolean allowLazyPersist);void replaceBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String delHint, DatanodeInfo source);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void writeBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes, DatanodeInfo source, BlockConstructionStage stage, int pipelineSize, long minBytesRcvd, long maxBytesRcvd, long latestGenerationStamp, DataChecksum requestedChecksum, CachingStrategy cachingStrategy, boolean allowLazyPersist);void copyBlock(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void writeBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes, DatanodeInfo source, BlockConstructionStage stage, int pipelineSize, long minBytesRcvd, long maxBytesRcvd, long latestGenerationStamp, DataChecksum requestedChecksum, CachingStrategy cachingStrategy, boolean allowLazyPersist);void blockChecksum(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void transferBlock(ExtendedBlock blk, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes);void replaceBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String delHint, DatanodeInfo source);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void transferBlock(ExtendedBlock blk, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes);void copyBlock(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void transferBlock(ExtendedBlock blk, Token blockToken, String clientName, DatanodeInfo[] targets, StorageType[] targetStorageTypes);void blockChecksum(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void replaceBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String delHint, DatanodeInfo source);void copyBlock(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void replaceBlock(ExtendedBlock blk, StorageType storageType, Token blockToken, String delHint, DatanodeInfo source);void blockChecksum(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;void copyBlock(ExtendedBlock blk, Token blockToken);void blockChecksum(ExtendedBlock blk, Token blockToken);@param;Token<BlockTokenIdentifier> blockToken;Token<BlockTokenIdentifier> blockToken;security token for accessing the block.;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant;SaslParticipant createServerSaslParticipant(Map saslProps, CallbackHandler callbackHandler);SaslParticipant createClientSaslParticipant(String userName, Map saslProps, CallbackHandler callbackHandler);@param;Map<String,String> saslProps;Map<String,String> saslProps;properties of SASL negotiation;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant;SaslParticipant createServerSaslParticipant(Map saslProps, CallbackHandler callbackHandler);SaslParticipant createClientSaslParticipant(String userName, Map saslProps, CallbackHandler callbackHandler);@param;CallbackHandler callbackHandler;CallbackHandler callbackHandler;for handling all SASL callbacks;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant;SaslParticipant createServerSaslParticipant(Map saslProps, CallbackHandler callbackHandler);SaslParticipant createClientSaslParticipant(String userName, Map saslProps, CallbackHandler callbackHandler);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant;byte[] wrap(byte[] bytes, int off, int len);byte[] unwrap(byte[] bytes, int off, int len);@param;int off;int off;The starting position at the array;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient; SaslDataTransferClient(Configuration conf, SaslPropertiesResolver saslPropsResolver, TrustedChannelResolver trustedChannelResolver); SaslDataTransferClient(Configuration conf, SaslPropertiesResolver saslPropsResolver, TrustedChannelResolver trustedChannelResolver, AtomicBoolean fallbackToSimpleAuth);@param;Configuration conf;Configuration conf;the configuration;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient; SaslDataTransferClient(Configuration conf, SaslPropertiesResolver saslPropsResolver, TrustedChannelResolver trustedChannelResolver); SaslDataTransferClient(Configuration conf, SaslPropertiesResolver saslPropsResolver, TrustedChannelResolver trustedChannelResolver, AtomicBoolean fallbackToSimpleAuth);@param;SaslPropertiesResolver saslPropsResolver;SaslPropertiesResolver saslPropsResolver;for determining properties of SASL negotiation;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient; SaslDataTransferClient(Configuration conf, SaslPropertiesResolver saslPropsResolver, TrustedChannelResolver trustedChannelResolver); SaslDataTransferClient(Configuration conf, SaslPropertiesResolver saslPropsResolver, TrustedChannelResolver trustedChannelResolver, AtomicBoolean fallbackToSimpleAuth);@param;TrustedChannelResolver trustedChannelResolver;TrustedChannelResolver trustedChannelResolver;for identifying trusted connections that do not require SASL negotiation;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@return;;;new pair of streams, wrapped after SASL negotiation;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;DataEncryptionKeyFactory encryptionKeyFactory;DataEncryptionKeyFactory encryptionKeyFactory;for creation of an encryption key;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;Token<BlockTokenIdentifier> accessToken;Token<BlockTokenIdentifier> accessToken;connection block access token;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;DatanodeID datanodeId;DatanodeID datanodeId;ID of destination DataNode;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@return;;;new pair of streams, wrapped after SASL negotiation;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;Socket socket;Socket socket;connection socket;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;OutputStream underlyingOut;OutputStream underlyingOut;connection output stream;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;InputStream underlyingIn;InputStream underlyingIn;connection input stream;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;DataEncryptionKeyFactory encryptionKeyFactory;DataEncryptionKeyFactory encryptionKeyFactory;for creation of an encryption key;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;Token<BlockTokenIdentifier> accessToken;Token<BlockTokenIdentifier> accessToken;connection block access token;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;DatanodeID datanodeId;DatanodeID datanodeId;ID of destination DataNode;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;IOStreamPair newSocketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@return;;;new pair of streams, wrapped after SASL negotiation;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;DataEncryptionKeyFactory encryptionKeyFactory;DataEncryptionKeyFactory encryptionKeyFactory;for creation of an encryption key;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;Token<BlockTokenIdentifier> accessToken;Token<BlockTokenIdentifier> accessToken;connection block access token;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@param;DatanodeID datanodeId;DatanodeID datanodeId;ID of destination DataNode;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;Peer peerSend(Peer peer, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);IOStreamPair socketSend(Socket socket, OutputStream underlyingOut, InputStream underlyingIn, DataEncryptionKeyFactory encryptionKeyFactory, Token accessToken, DatanodeID datanodeId);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);boolean requestedQopContainsPrivacy(Map saslProps);@param;Map<String,String> saslProps;Map<String,String> saslProps;properties of SASL negotiation;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);byte[] readSaslMessage(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);void sendGenericSaslErrorMessage(OutputStream out, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);void sendSaslMessage(OutputStream out, byte[] payload);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void checkSaslComplete(SaslParticipant sasl, Map saslProps);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);@param;InputStream in;InputStream in;stream to read;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);void sendGenericSaslErrorMessage(OutputStream out, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);void sendSaslMessage(OutputStream out, byte[] payload);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@param;InputStream in;InputStream in;stream to read;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessage(InputStream in);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);void sendGenericSaslErrorMessage(OutputStream out, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);void sendSaslMessage(OutputStream out, byte[] payload);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@param;InputStream in;InputStream in;stream to read;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;byte[] readSaslMessageAndNegotiationCipherOptions(InputStream in, List cipherOptions);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;CipherOption negotiateCipherOption(Configuration conf, List options);IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);@param;Configuration conf;Configuration conf;the configuration;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);@param;CipherOption option;CipherOption cipherOption;negotiated cipher option;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendGenericSaslErrorMessage(OutputStream out, String message);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendGenericSaslErrorMessage(OutputStream out, String message);@param;byte[] payload;String message;to send;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendGenericSaslErrorMessage(OutputStream out, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessage(OutputStream out, byte[] payload);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessage(OutputStream out, byte[] payload);@param;byte[] payload;byte[] payload;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessage(OutputStream out, byte[] payload);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@param;byte[] payload;byte[] payload;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);CipherOption wrap(CipherOption option, SaslParticipant sasl);@param;CipherOption option;CipherOption option;negotiated cipher option;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@param;CipherOption option;CipherOption option;negotiated cipher option;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;byte[] payload;byte[] payload;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;byte[] payload;String message;to send;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiatedCipherOption(OutputStream out, byte[] payload, CipherOption option);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);void sendGenericSaslErrorMessage(OutputStream out, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);void sendSaslMessage(OutputStream out, byte[] payload);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);CipherOption wrap(CipherOption option, SaslParticipant sasl);@param;CipherOption cipherOption;CipherOption option;negotiated cipher option;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@param;CipherOption cipherOption;CipherOption option;negotiated cipher option;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;IOStreamPair createStreamPair(Configuration conf, CipherOption cipherOption, OutputStream out, InputStream in, boolean isServer);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessage(OutputStream out, byte[] payload);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessage(OutputStream out, byte[] payload);@param;String message;byte[] payload;to send;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessage(OutputStream out, byte[] payload);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@param;String message;byte[] payload;to send;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;String message;byte[] payload;to send;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;String message;String message;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendGenericSaslErrorMessage(OutputStream out, String message);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@param;byte[] payload;byte[] payload;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);Free text;;;Sends a SASL negotiation message. ;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;byte[] payload;byte[] payload;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;byte[] payload;String message;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessage(OutputStream out, byte[] payload);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;OutputStream out;OutputStream out;stream to receive message;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;byte[] payload;byte[] payload;to send;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@param;byte[] payload;String message;to send;false
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;void sendSaslMessageAndNegotiationCipherOptions(OutputStream out, byte[] payload, List options);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);CipherOption wrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;SaslResponseWithNegotiatedCipherOption readSaslMessageAndNegotiatedCipherOption(InputStream in);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;CipherOption wrap(CipherOption option, SaslParticipant sasl);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@param;CipherOption option;CipherOption option;negotiated cipher option;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;CipherOption wrap(CipherOption option, SaslParticipant sasl);CipherOption unwrap(CipherOption option, SaslParticipant sasl);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;CipherOption wrap(CipherOption option, SaslParticipant sasl);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil;CipherOption unwrap(CipherOption option, SaslParticipant sasl);void sendSaslMessage(OutputStream out, DataTransferEncryptorStatus status, byte[] payload, String message);@throws;;;for any error;true
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck; PipelineAck(long seqno, Status[] replies); PipelineAck(long seqno, Status[] replies, long downstreamAckTimeNanos);@param;long seqno;long seqno;sequence number;true
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck; PipelineAck(long seqno, Status[] replies); PipelineAck(long seqno, Status[] replies, long downstreamAckTimeNanos);@param;Status[] replies;Status[] replies;an array of replies;true
org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver;boolean isTrusted();boolean isTrusted(InetAddress peerAddress);@return;;;true if the channel is trusted and false otherwise.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);LocatedBlock append(String src, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean setReplication(String src, short replication);@param;String src;String src;file name;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean setReplication(String src, short replication);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean setReplication(String src, short replication);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setStoragePolicy(String src, String policyName);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setPermission(String src, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setPermission(String src, FsPermission permission);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setPermission(String src, FsPermission permission);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setOwner(String src, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setOwner(String src, String username, String groupname);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setOwner(String src, String username, String groupname);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlocks getBlockLocations(String src, long offset, long length);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock append(String src, String clientName);@param;String src;String src;path of the file being created.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock append(String src, String clientName);@param;String clientName;String clientName;name of the current client.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock append(String src, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock append(String src, String clientName);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean setReplication(String src, short replication);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean setReplication(String src, short replication);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setStoragePolicy(String src, String policyName);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setPermission(String src, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setPermission(String src, FsPermission permission);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setPermission(String src, FsPermission permission);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setOwner(String src, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setOwner(String src, String username, String groupname);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setOwner(String src, String username, String groupname);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean rename(String src, String dst);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void concat(String trg, String[] srcs);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void rename2(String src, String dst, Options.Rename options);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean delete(String src, boolean recursive);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean mkdirs(String src, FsPermission masked, boolean createParent);@param;boolean createParent;boolean createParent;create missing parent directory if true;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If parent of src does not exist and createParent is false;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean setReplication(String src, short replication);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean setReplication(String src, short replication);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setPermission(String src, FsPermission permission);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setPermission(String src, FsPermission permission);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setPermission(String src, FsPermission permission);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setOwner(String src, String username, String groupname);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setOwner(String src, String username, String groupname);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setOwner(String src, String username, String groupname);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean rename(String src, String dst);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void concat(String trg, String[] srcs);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void rename2(String src, String dst, Options.Rename options);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean delete(String src, boolean recursive);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock append(String src, String clientName);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setStoragePolicy(String src, String policyName);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setStoragePolicy(String src, String policyName);@throws;;;if src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setPermission(String src, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setPermission(String src, FsPermission permission);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setPermission(String src, FsPermission permission);@throws;;;not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setPermission(String src, FsPermission permission);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setPermission(String src, FsPermission permission);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setOwner(String src, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setOwner(String src, String username, String groupname);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setOwner(String src, String username, String groupname);@throws;;;not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setOwner(String src, String username, String groupname);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setOwner(String src, String username, String groupname);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean rename(String src, String dst);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void concat(String trg, String[] srcs);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void rename2(String src, String dst, Options.Rename options);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean delete(String src, boolean recursive);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);HdfsFileStatus getFileLinkInfo(String src);@throws;;;if src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean setReplication(String src, short replication);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);void setPermission(String src, FsPermission permission);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);void setOwner(String src, String username, String groupname);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);boolean delete(String src, boolean recursive);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setStoragePolicy(String src, String policyName);HdfsFileStatus getFileLinkInfo(String src);@throws;;;if src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setOwner(String src, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setOwner(String src, String username, String groupname);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setOwner(String src, String username, String groupname);@throws;;;not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setOwner(String src, String username, String groupname);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setOwner(String src, String username, String groupname);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setOwner(String src, String username, String groupname);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean rename(String src, String dst);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void concat(String trg, String[] srcs);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void rename2(String src, String dst, Options.Rename options);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean delete(String src, boolean recursive);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setPermission(String src, FsPermission permission);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean rename(String src, String dst);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void concat(String trg, String[] srcs);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void rename2(String src, String dst, Options.Rename options);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean delete(String src, boolean recursive);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setOwner(String src, String username, String groupname);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);HdfsFileStatus getFileInfo(String src);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean isFileClosed(String src);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void setTimes(String src, long mtime, long atime);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void abandonBlock(ExtendedBlock b, long fileId, String src, String holder);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@param;String src;String src;the file being created;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@param;String clientName;String clientName;the name of the client that adds the block;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@param;long fileId;long fileId;the id uniquely identifying a file;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean delete(String src, boolean recursive);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean delete(String src, boolean recursive);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId, String[] favoredNodes);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean complete(String src, String clientName, ExtendedBlock last, long fileId);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean delete(String src, boolean recursive);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean delete(String src, boolean recursive);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName);@param;String clientName;String clientName;the name of the client;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock getAdditionalDatanode(String src, long fileId, ExtendedBlock blk, DatanodeInfo[] existings, String[] existingStorageIDs, DatanodeInfo[] excludes, int numAdditionalNodes, String clientName);void updatePipeline(String clientName, ExtendedBlock oldBlock, ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs);@param;String clientName;String clientName;the name of the client;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);void rename2(String src, String dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);void rename2(String src, String dst, Options.Rename options);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean delete(String src, boolean recursive);@throws;;;If file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean delete(String src, boolean recursive);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean delete(String src, boolean recursive);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean complete(String src, String clientName, ExtendedBlock last, long fileId);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);void concat(String trg, String[] srcs);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);void rename2(String src, String dst, Options.Rename options);@param;String src;String src;existing file or directory name.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);void rename2(String src, String dst, Options.Rename options);@param;String dst;String dst;new name.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);void rename2(String src, String dst, Options.Rename options);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean rename(String src, String dst);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void concat(String trg, String[] srcs);void rename2(String src, String dst, Options.Rename options);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void concat(String trg, String[] srcs);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void concat(String trg, String[] srcs);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void concat(String trg, String[] srcs);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void concat(String trg, String[] srcs);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void concat(String trg, String[] srcs);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);boolean delete(String src, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);boolean delete(String src, boolean recursive);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);boolean delete(String src, boolean recursive);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void rename2(String src, String dst, Options.Rename options);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;create not allowed in safemode;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);boolean mkdirs(String src, FsPermission masked, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean delete(String src, boolean recursive);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean mkdirs(String src, FsPermission masked, boolean createParent);DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);@throws;;;If src contains a symlink;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean mkdirs(String src, FsPermission masked, boolean createParent);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean mkdirs(String src, FsPermission masked, boolean createParent);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean mkdirs(String src, FsPermission masked, boolean createParent);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);SnapshottableDirectoryStatus[] getSnapshottableDirListing();@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void renewLease(String clientName);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);HdfsFileStatus getFileInfo(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);HdfsFileStatus getFileInfo(String src);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);boolean isFileClosed(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);boolean isFileClosed(String src);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);HdfsFileStatus getFileLinkInfo(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);ContentSummary getContentSummary(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void setTimes(String src, long mtime, long atime);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();void renewLease(String clientName);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;SnapshottableDirectoryStatus[] getSnapshottableDirListing();void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);HdfsFileStatus getFileInfo(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);HdfsFileStatus getFileInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);boolean isFileClosed(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);HdfsFileStatus getFileLinkInfo(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);ContentSummary getContentSummary(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void renewLease(String clientName);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;long getPreferredBlockSize(String filename);HdfsFileStatus getFileInfo(String src);@throws;;;if the path contains a symlink.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;long getPreferredBlockSize(String filename);boolean isFileClosed(String src);@throws;;;if the path contains a symlink.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;long getPreferredBlockSize(String filename);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if the path contains a symlink.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void saveNamespace();boolean restoreFailedStorage(String arg);@throws;;;if the superuser privilege is violated.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);boolean isFileClosed(String src);@param;String src;String src;The string representation of the path to the file;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);boolean isFileClosed(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);boolean isFileClosed(String src);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);boolean isFileClosed(String src);@throws;;;if the path contains a symlink.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);boolean isFileClosed(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);HdfsFileStatus getFileLinkInfo(String src);@return;;;object containing information regarding the file or null if file not found;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);HdfsFileStatus getFileLinkInfo(String src);@param;String src;String src;The string representation of the path to the file;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);HdfsFileStatus getFileLinkInfo(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);ContentSummary getContentSummary(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if the path contains a symlink.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void setTimes(String src, long mtime, long atime);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileInfo(String src);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);HdfsFileStatus getFileLinkInfo(String src);@param;String src;String src;The string representation of the path to the file;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);HdfsFileStatus getFileLinkInfo(String src);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);HdfsFileStatus getFileLinkInfo(String src);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);ContentSummary getContentSummary(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;if the path contains a symlink.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void setTimes(String src, long mtime, long atime);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;boolean isFileClosed(String src);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);ContentSummary getContentSummary(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);ContentSummary getContentSummary(String path);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;HdfsFileStatus getFileLinkInfo(String src);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;file path is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void setQuota(String path, long namespaceQuota, long diskspaceQuota);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void fsync(String src, long inodeId, String client, long lastBlockLength);@param;String path;String src;The string representation of the path;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void setTimes(String src, long mtime, long atime);@param;String path;String src;The string representation of the path;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;ContentSummary getContentSummary(String path);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void fsync(String src, long inodeId, String client, long lastBlockLength);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void setTimes(String src, long mtime, long atime);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setQuota(String path, long namespaceQuota, long diskspaceQuota);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);void setTimes(String src, long mtime, long atime);@param;String src;String src;The string representation of the path;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);void setTimes(String src, long mtime, long atime);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);void setTimes(String src, long mtime, long atime);@throws;;;file src is not found;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);void setTimes(String src, long mtime, long atime);@throws;;;if src contains a symlink.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);void setTimes(String src, long mtime, long atime);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void fsync(String src, long inodeId, String client, long lastBlockLength);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setTimes(String src, long mtime, long atime);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setTimes(String src, long mtime, long atime);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;if path is in RO snapshot;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setTimes(String src, long mtime, long atime);void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setTimes(String src, long mtime, long atime);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void createSymlink(String target, String link, FsPermission dirPerm, boolean createParent);String getLinkTarget(String path);@throws;;;permission denied;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName);void updatePipeline(String clientName, ExtendedBlock oldBlock, ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs);@param;String clientName;String clientName;the name of the client;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName);void updatePipeline(String clientName, ExtendedBlock oldBlock, ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs);@throws;;;if any error occurs;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void allowSnapshot(String snapshotRoot);void disallowSnapshot(String snapshotRoot);@throws;;;on error;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void allowSnapshot(String snapshotRoot);SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot, String fromSnapshot, String toSnapshot);@throws;;;on error;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void disallowSnapshot(String snapshotRoot);SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot, String fromSnapshot, String toSnapshot);@throws;;;on error;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;long addCacheDirective(CacheDirectiveInfo directive, EnumSet flags);void modifyCacheDirective(CacheDirectiveInfo directive, EnumSet flags);@param;EnumSet<CacheFlag> flags;EnumSet<CacheFlag> flags;CacheFlags to use for this operation.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void addCachePool(CachePoolInfo info);void modifyCachePool(CachePoolInfo req);@throws;;;If the request could not be completed.;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setXAttr(String src, XAttr xAttr, EnumSet flag);List getXAttrs(String src, List xAttrs);@param;String src;String src;file or directory;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setXAttr(String src, XAttr xAttr, EnumSet flag);List listXAttrs(String src);@param;String src;String src;file or directory;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;void setXAttr(String src, XAttr xAttr, EnumSet flag);void removeXAttr(String src, XAttr xAttr);@param;String src;String src;file or directory;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;List getXAttrs(String src, List xAttrs);List listXAttrs(String src);@return;;;List<XAttr> XAttr list;false
org.apache.hadoop.hdfs.protocol.ClientProtocol;List getXAttrs(String src, List xAttrs);List listXAttrs(String src);@param;String src;String src;file or directory;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;List getXAttrs(String src, List xAttrs);void removeXAttr(String src, XAttr xAttr);@param;String src;String src;file or directory;true
org.apache.hadoop.hdfs.protocol.ClientProtocol;List listXAttrs(String src);void removeXAttr(String src, XAttr xAttr);@param;String src;String src;file or directory;true
org.apache.hadoop.hdfs.protocol.DatanodeID;String getInfoAddr();String getInfoSecureAddr();Whole;;;  @return IP:infoPort string ;false
org.apache.hadoop.hdfs.protocol.DatanodeID;String getXferAddr(boolean useHostname);String getIpcAddr(boolean useHostname);@param;boolean useHostname;boolean useHostname;true to use the DN hostname, use the IP otherwise;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication);List chooseStorageTypes(short replication, Iterable chosen);@return;;;a list of StorageTypes for storing the replicas of a block.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication);List chooseStorageTypes(short replication, Iterable chosen, EnumSet unavailables, boolean isNewBlock);@return;;;a list of StorageTypes for storing the replicas of a block.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication, Iterable chosen);List chooseStorageTypes(short replication, Iterable chosen, EnumSet unavailables, boolean isNewBlock);@return;;;a list of StorageTypes for storing the replicas of a block.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication, Iterable chosen);List chooseStorageTypes(short replication, Iterable chosen, EnumSet unavailables, boolean isNewBlock);@param;short replication;short replication;the replication number.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication, Iterable chosen);List chooseStorageTypes(short replication, Iterable chosen, EnumSet unavailables, boolean isNewBlock);@param;Iterable<StorageType> chosen;Iterable<StorageType> chosen;the storage types of the chosen replicas.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication, Iterable chosen);List chooseExcess(short replication, Iterable chosen);@param;short replication;short replication;the replication number.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication, Iterable chosen);List chooseExcess(short replication, Iterable chosen);@param;Iterable<StorageType> chosen;Iterable<StorageType> chosen;the storage types of the chosen replicas.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication, Iterable chosen, EnumSet unavailables, boolean isNewBlock);List chooseExcess(short replication, Iterable chosen);@param;short replication;short replication;the replication number.;true
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;List chooseStorageTypes(short replication, Iterable chosen, EnumSet unavailables, boolean isNewBlock);List chooseExcess(short replication, Iterable chosen);@param;Iterable<StorageType> chosen;Iterable<StorageType> chosen;the storage types of the chosen replicas.;true
org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol;void refreshNamenodes();BlockLocalPathInfo getBlockLocalPathInfo(ExtendedBlock block, Token token);@throws;;;on error;true
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel;ExecutorService createSingleThreadExecutor();ExecutorService createParallelExecutor();Whole;;;Separated out for easy overriding in tests.    ;false
org.apache.hadoop.hdfs.client.HdfsAdmin;void setQuota(Path src, long quota);void clearQuota(Path src);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void setQuota(Path src, long quota);void setSpaceQuota(Path src, long spaceQuota);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void setQuota(Path src, long quota);void clearSpaceQuota(Path src);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void clearQuota(Path src);void setSpaceQuota(Path src, long spaceQuota);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void clearQuota(Path src);void clearSpaceQuota(Path src);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void setSpaceQuota(Path src, long spaceQuota);void clearSpaceQuota(Path src);@throws;;;in the event of error;true
org.apache.hadoop.hdfs.client.HdfsAdmin;long addCacheDirective(CacheDirectiveInfo info, EnumSet flags);void modifyCacheDirective(CacheDirectiveInfo info, EnumSet flags);@param;EnumSet<CacheFlag> flags;EnumSet<CacheFlag> flags;CacheFlags to use for this operation.;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void addCachePool(CachePoolInfo info);void modifyCachePool(CachePoolInfo info);@throws;;;If the request could not be completed.;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void createEncryptionZone(Path path, String keyName);EncryptionZone getEncryptionZoneForPath(Path path);@throws;;;if there was a general IO exception;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void createEncryptionZone(Path path, String keyName);EncryptionZone getEncryptionZoneForPath(Path path);@throws;;;if the caller does not have access to path;true
org.apache.hadoop.hdfs.client.HdfsAdmin;void createEncryptionZone(Path path, String keyName);EncryptionZone getEncryptionZoneForPath(Path path);@throws;;;if the path does not exist;true
org.apache.hadoop.hdfs.HAUtil;boolean isHAEnabled(Configuration conf, String nsId);boolean usesSharedEditsDir(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;boolean isHAEnabled(Configuration conf, String nsId);String getNameNodeId(Configuration conf, String nsId);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;boolean isHAEnabled(Configuration conf, String nsId);boolean isClientFailoverConfigured(Configuration conf, URI nameNodeUri);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;boolean isHAEnabled(Configuration conf, String nsId);boolean useLogicalUri(Configuration conf, URI nameNodeUri);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;boolean usesSharedEditsDir(Configuration conf);String getNameNodeId(Configuration conf, String nsId);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;boolean usesSharedEditsDir(Configuration conf);boolean isClientFailoverConfigured(Configuration conf, URI nameNodeUri);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;boolean usesSharedEditsDir(Configuration conf);boolean useLogicalUri(Configuration conf, URI nameNodeUri);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;String getNameNodeId(Configuration conf, String nsId);boolean isClientFailoverConfigured(Configuration conf, URI nameNodeUri);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;String getNameNodeId(Configuration conf, String nsId);boolean useLogicalUri(Configuration conf, URI nameNodeUri);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;String getNameNodeIdOfOtherNode(Configuration conf, String nsId);Configuration getConfForOtherNode(Configuration myConf);@param;Configuration conf;Configuration myConf;the configuration of this node;false
org.apache.hadoop.hdfs.HAUtil;boolean isClientFailoverConfigured(Configuration conf, URI nameNodeUri);boolean useLogicalUri(Configuration conf, URI nameNodeUri);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.hdfs.HAUtil;boolean isClientFailoverConfigured(Configuration conf, URI nameNodeUri);boolean useLogicalUri(Configuration conf, URI nameNodeUri);@param;URI nameNodeUri;URI nameNodeUri;The URI of namenode;true
org.apache.hadoop.hdfs.HAUtil;List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId);List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId, Class xface);Free text;;;Get an RPC proxy for each NN in an HA nameservice. Used when a given RPC call should be made on every NN in an HA nameservice, not just the active. ;true
org.apache.hadoop.hdfs.HAUtil;List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId);List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId, Class xface);@return;;;a list of RPC proxies for each NN in the nameservice.;true
org.apache.hadoop.hdfs.HAUtil;List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId);List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId, Class xface);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.hdfs.HAUtil;List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId);List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId, Class xface);@param;String nsId;String nsId;the nameservice to get all of the proxies for.;true
org.apache.hadoop.hdfs.HAUtil;List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId);List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId, Class xface);@throws;;;in the event of error.;true
org.apache.hadoop.hdfs.HAUtil;List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId);boolean isAtLeastOneActive(List namenodes);@throws;;;in the event of error.;true
org.apache.hadoop.hdfs.HAUtil;List getProxiesForAllNameNodesInNameservice(Configuration conf, String nsId, Class xface);boolean isAtLeastOneActive(List namenodes);@throws;;;in the event of error.;true
