Class;Method1;Method2;Type;Param1;Param2;Cloned text;Legit?
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, int tagValue);@param;String tagName;String tagName;name of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, int tagValue);@param;String tagValue;int tagValue;new value of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, int tagValue);@throws;;;if the tagName conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, long tagValue);@param;String tagName;String tagName;name of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, long tagValue);@param;String tagValue;long tagValue;new value of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, long tagValue);@throws;;;if the tagName conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, short tagValue);@param;String tagName;String tagName;name of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, short tagValue);@param;String tagValue;short tagValue;new value of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, short tagValue);@throws;;;if the tagName conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, byte tagValue);@param;String tagName;String tagName;name of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, byte tagValue);@param;String tagValue;byte tagValue;new value of the tag;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, String tagValue);void setTag(String tagName, byte tagValue);@throws;;;if the tagName conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, int tagValue);void setTag(String tagName, long tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, int tagValue);void setTag(String tagName, short tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, int tagValue);void setTag(String tagName, byte tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, long tagValue);void setTag(String tagName, short tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, long tagValue);void setTag(String tagName, byte tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setTag(String tagName, short tagValue);void setTag(String tagName, byte tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void setMetric(String metricName, long metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void setMetric(String metricName, short metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void setMetric(String metricName, byte metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void setMetric(String metricName, short metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void setMetric(String metricName, byte metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void setMetric(String metricName, byte metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.MetricsRecord;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, long metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, short metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, byte metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, long metricValue);void incrMetric(String metricName, short metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, long metricValue);void incrMetric(String metricName, byte metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, long metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, short metricValue);void incrMetric(String metricName, byte metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, short metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.MetricsRecord;void incrMetric(String metricName, byte metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.ContextFactory;Object getAttribute(String attributeName);void setAttribute(String attributeName, Object value);@param;String attributeName;String attributeName;the attribute name;true
org.apache.hadoop.metrics.ContextFactory;Object getAttribute(String attributeName);void removeAttribute(String attributeName);@param;String attributeName;String attributeName;the attribute name;true
org.apache.hadoop.metrics.ContextFactory;void setAttribute(String attributeName, Object value);void removeAttribute(String attributeName);@param;String attributeName;String attributeName;the attribute name;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingRate; MetricsTimeVaryingRate(String nam, MetricsRegistry registry, String description); MetricsTimeVaryingRate(String nam, MetricsRegistry registry);Free text;;;Constructor - create a new metric ;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingRate; MetricsTimeVaryingRate(String nam, MetricsRegistry registry, String description); MetricsTimeVaryingRate(String nam, MetricsRegistry registry);@param;String nam;String nam;the name of the metrics to be used to publish the metric;true
org.apache.hadoop.metrics.util.MetricsLongValue; MetricsLongValue(String nam, MetricsRegistry registry, String description); MetricsLongValue(String nam, MetricsRegistry registry);Free text;;;Constructor - create a new metric ;true
org.apache.hadoop.metrics.util.MetricsLongValue; MetricsLongValue(String nam, MetricsRegistry registry, String description); MetricsLongValue(String nam, MetricsRegistry registry);@param;String nam;String nam;the name of the metrics to be used to publish the metric;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingInt; MetricsTimeVaryingInt(String nam, MetricsRegistry registry, String description); MetricsTimeVaryingInt(String nam, MetricsRegistry registry);Free text;;;Constructor - create a new metric ;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingInt; MetricsTimeVaryingInt(String nam, MetricsRegistry registry, String description); MetricsTimeVaryingInt(String nam, MetricsRegistry registry);@param;String nam;String nam;the name of the metrics to be used to publish the metric;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingInt;int getPreviousIntervalValue();int getCurrentIntervalValue();@return;;;prev interval value;false
org.apache.hadoop.metrics.util.MetricsIntValue; MetricsIntValue(String nam, MetricsRegistry registry, String description); MetricsIntValue(String nam, MetricsRegistry registry);Free text;;;Constructor - create a new metric ;true
org.apache.hadoop.metrics.util.MetricsIntValue; MetricsIntValue(String nam, MetricsRegistry registry, String description); MetricsIntValue(String nam, MetricsRegistry registry);@param;String nam;String nam;the name of the metrics to be used to publish the metric;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingLong; MetricsTimeVaryingLong(String nam, MetricsRegistry registry, String description); MetricsTimeVaryingLong(String nam, MetricsRegistry registry);Free text;;;Constructor - create a new metric ;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingLong; MetricsTimeVaryingLong(String nam, MetricsRegistry registry, String description); MetricsTimeVaryingLong(String nam, MetricsRegistry registry);@param;String nam;String nam;the name of the metrics to be used to publish the metric;true
org.apache.hadoop.metrics.util.MetricsTimeVaryingLong;long getPreviousIntervalValue();long getCurrentIntervalValue();@return;;;prev interval value;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, String tagValue);void setTag(String tagName, int tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, String tagValue);void setTag(String tagName, long tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, String tagValue);void setTag(String tagName, short tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, String tagValue);void setTag(String tagName, byte tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, int tagValue);void setTag(String tagName, long tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, int tagValue);void setTag(String tagName, short tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, int tagValue);void setTag(String tagName, byte tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, long tagValue);void setTag(String tagName, short tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, long tagValue);void setTag(String tagName, byte tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setTag(String tagName, short tagValue);void setTag(String tagName, byte tagValue);Whole;;;Sets the named tag to the specified value.  @param name of the tag@param new value of the tag  @throws if the tagName conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void setMetric(String metricName, long metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void setMetric(String metricName, short metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void setMetric(String metricName, byte metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, int metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void setMetric(String metricName, short metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void setMetric(String metricName, byte metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, long metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void setMetric(String metricName, byte metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, short metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void setMetric(String metricName, float metricValue);Whole;;;Sets the named metric to the specified value.  @param name of the metric@param new value of the metric  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, byte metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, int metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, int metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, long metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, long metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, short metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, short metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, byte metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, byte metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, float metricValue);@param;String metricName;String metricName;name of the metric;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void setMetric(String metricName, float metricValue);void incrMetric(String metricName, float metricValue);@throws;;;if the metricName or the type of the metricValue conflicts with the configuration;true
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, long metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, short metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, byte metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, int metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, long metricValue);void incrMetric(String metricName, short metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, long metricValue);void incrMetric(String metricName, byte metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, long metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, short metricValue);void incrMetric(String metricName, byte metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, short metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.MetricsRecordImpl;void incrMetric(String metricName, byte metricValue);void incrMetric(String metricName, float metricValue);Whole;;;Increments the named metric by the specified value.  @param name of the metric@param incremental value  @throws if the metricName or the type of the metricValue conflicts with the configuration;false
org.apache.hadoop.metrics.spi.AbstractMetricsContext;MetricsRecord createRecord(String recordName);MetricsRecord newRecord(String recordName);@param;String recordName;String recordName;the name of the record;true
org.apache.hadoop.metrics2.impl.SinkQueue;void consume(Consumer consumer);void consumeAll(Consumer consumer);@param;Consumer<T> consumer;Consumer<T> consumer;the consumer callback object;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(String name);boolean accepts(MetricsTag tag);@param;String name;MetricsTag tag;to filter on;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(String name);boolean accepts(Iterable tags);@param;String name;Iterable<MetricsTag> tags;to filter on;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(String name);boolean accepts(MetricsRecord record);@return;;;true to accept, false otherwise.;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(String name);boolean accepts(MetricsRecord record);@param;String name;MetricsRecord record;to filter on;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(MetricsTag tag);boolean accepts(Iterable tags);@return;;;true to accept, false otherwise;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(MetricsTag tag);boolean accepts(Iterable tags);@param;MetricsTag tag;Iterable<MetricsTag> tags;to filter on;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(MetricsTag tag);boolean accepts(MetricsRecord record);@param;MetricsTag tag;MetricsRecord record;to filter on;true
org.apache.hadoop.metrics2.MetricsFilter;boolean accepts(Iterable tags);boolean accepts(MetricsRecord record);@param;Iterable<MetricsTag> tags;MetricsRecord record;to filter on;true
org.apache.hadoop.metrics2.MetricsException; MetricsException(String message); MetricsException(String message, Throwable cause);@param;String message;String message;for the exception;true
org.apache.hadoop.metrics2.MetricsException; MetricsException(String message, Throwable cause); MetricsException(Throwable cause);@param;Throwable cause;Throwable cause;of the exception;true
org.apache.hadoop.metrics2.util.SampleStat;SampleStat add(double x);SampleStat add(long nSamples, double x);@return;;;self;true
org.apache.hadoop.metrics2.util.Contracts;T checkArg(T arg, boolean expression, Object msg);int checkArg(int arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;T checkArg(T arg, boolean expression, Object msg);long checkArg(long arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;T checkArg(T arg, boolean expression, Object msg);float checkArg(float arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;T checkArg(T arg, boolean expression, Object msg);double checkArg(double arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;int checkArg(int arg, boolean expression, Object msg);long checkArg(long arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;int checkArg(int arg, boolean expression, Object msg);float checkArg(float arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;int checkArg(int arg, boolean expression, Object msg);double checkArg(double arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;long checkArg(long arg, boolean expression, Object msg);float checkArg(float arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;long checkArg(long arg, boolean expression, Object msg);double checkArg(double arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.Contracts;float checkArg(float arg, boolean expression, Object msg);double checkArg(double arg, boolean expression, Object msg);Whole;;;Check an argument for false conditions  @param the argument to check@param the boolean expression for the condition@param the error message if expression is false @return the argument for convenience ;false
org.apache.hadoop.metrics2.util.MetricsCache;Record update(MetricsRecord mr, boolean includingTags);Record update(MetricsRecord mr);@return;;;the updated cache record;true
org.apache.hadoop.metrics2.util.MetricsCache;Record update(MetricsRecord mr, boolean includingTags);Record update(MetricsRecord mr);@param;MetricsRecord mr;MetricsRecord mr;the update record;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void gauge(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void gauge(MetricsInfo info, long value);@param;int value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void gauge(MetricsInfo info, float value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void gauge(MetricsInfo info, float value);@param;int value;float value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void gauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void gauge(MetricsInfo info, double value);@param;int value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void counter(MetricsInfo info, int value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void counter(MetricsInfo info, int value);@param;int value;int value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void counter(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, int value);void counter(MetricsInfo info, long value);@param;int value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void gauge(MetricsInfo info, float value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void gauge(MetricsInfo info, float value);@param;long value;float value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void gauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void gauge(MetricsInfo info, double value);@param;long value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void counter(MetricsInfo info, int value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void counter(MetricsInfo info, int value);@param;long value;int value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void counter(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, long value);void counter(MetricsInfo info, long value);@param;long value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, float value);void gauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, float value);void gauge(MetricsInfo info, double value);@param;float value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, float value);void counter(MetricsInfo info, int value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, float value);void counter(MetricsInfo info, int value);@param;float value;int value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, float value);void counter(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, float value);void counter(MetricsInfo info, long value);@param;float value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, double value);void counter(MetricsInfo info, int value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, double value);void counter(MetricsInfo info, int value);@param;double value;int value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, double value);void counter(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void gauge(MetricsInfo info, double value);void counter(MetricsInfo info, long value);@param;double value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsVisitor;void counter(MetricsInfo info, int value);void counter(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;the metric info;true
org.apache.hadoop.metrics2.MetricsVisitor;void counter(MetricsInfo info, int value);void counter(MetricsInfo info, long value);@param;int value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsCollector;MetricsRecordBuilder addRecord(String name);MetricsRecordBuilder addRecord(MetricsInfo info);Whole;;;Add a metrics record  @param of the record @return a metrics record builder for the record ;false
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended); MutableStat(String name, String description, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended); MutableStat(String name, String description, String sampleName, String valueName);@param;String name;String description;of the metric;true
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended); MutableStat(String name, String description, String sampleName, String valueName);@param;String description;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended); MutableStat(String name, String description, String sampleName, String valueName);@param;String description;String description;of the metric;true
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended); MutableStat(String name, String description, String sampleName, String valueName);@param;String sampleName;String sampleName;of the metric (e.g. "Ops");true
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended); MutableStat(String name, String description, String sampleName, String valueName);@param;String valueName;String valueName;of the metric (e.g. "Time", "Latency");true
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended);void add(long value);@param;String name;long value;of the metric;false
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName, boolean extended);void add(long value);@param;String description;long value;of the metric;false
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName);void add(long value);@param;String name;long value;of the metric;false
org.apache.hadoop.metrics2.lib.MutableStat; MutableStat(String name, String description, String sampleName, String valueName);void add(long value);@param;String description;long value;of the metric;false
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);Free text;;;Get a metrics tag ;true
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);@return;;;an interned metrics tag;true
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);@param;MetricsInfo info;String name;of the tag;true
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);@param;MetricsInfo info;String description;of the tag;true
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);@param;MetricsInfo info;String value;of the tag;true
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);@param;String value;String name;of the tag;true
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);@param;String value;String description;of the tag;true
org.apache.hadoop.metrics2.lib.Interns;MetricsTag tag(MetricsInfo info, String value);MetricsTag tag(String name, String description, String value);@param;String value;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableCounterInt newCounter(String name, String desc, int iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableCounterLong newCounter(String name, String desc, long iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableMetric get(String name);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsTag getTag(String name);MetricsRegistry tag(String name, String description, String value);@param;String name;String name;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsTag getTag(String name);MetricsRegistry tag(String name, String description, String value);@param;String name;String description;of the tag;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsTag getTag(String name);MetricsRegistry tag(String name, String description, String value);@param;String name;String value;of the tag;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsTag getTag(String name);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String name;String name;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsTag getTag(String name);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String name;String description;of the tag;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsTag getTag(String name);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String name;String value;of the tag;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsTag getTag(String name);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;String name;String value;of the tag;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterInt newCounter(MetricsInfo info, int iVal);Free text;;;Create a mutable integer counter ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterInt newCounter(MetricsInfo info, int iVal);@return;;;a new counter object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterInt newCounter(MetricsInfo info, int iVal);@param;int iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterLong newCounter(String name, String desc, long iVal);@return;;;a new counter object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterLong newCounter(String name, String desc, long iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterLong newCounter(String name, String desc, long iVal);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterLong newCounter(String name, String desc, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);@return;;;a new counter object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;int iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@param;int iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(String name, String desc, int iVal);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableCounterLong newCounter(String name, String desc, long iVal);@return;;;a new counter object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableCounterLong newCounter(String name, String desc, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);@return;;;a new counter object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;int iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@param;int iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterInt newCounter(MetricsInfo info, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);Free text;;;Create a mutable long integer counter ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);@return;;;a new counter object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableCounterLong newCounter(MetricsInfo info, long iVal);@param;long iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;long iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@param;long iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;long iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;long iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(String name, String desc, long iVal);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(MetricsInfo info, long iVal);MutableGaugeInt newGauge(String name, String desc, int iVal);@param;long iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(MetricsInfo info, long iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(MetricsInfo info, long iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@param;long iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(MetricsInfo info, long iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;long iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(MetricsInfo info, long iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableCounterLong newCounter(MetricsInfo info, long iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;long iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);Free text;;;Create a mutable integer gauge ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@return;;;a new gauge object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeInt newGauge(MetricsInfo info, int iVal);@param;int iVal;int iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@return;;;a new gauge object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@return;;;a new gauge object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(String name, String desc, int iVal);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(MetricsInfo info, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@return;;;a new gauge object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(MetricsInfo info, int iVal);MutableGaugeLong newGauge(String name, String desc, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(MetricsInfo info, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@return;;;a new gauge object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(MetricsInfo info, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeInt newGauge(MetricsInfo info, int iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;int iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);Free text;;;Create a mutable long integer gauge ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@return;;;a new gauge object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableGaugeLong newGauge(MetricsInfo info, long iVal);@param;long iVal;long iVal;initial value;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableGaugeLong newGauge(String name, String desc, long iVal);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String sampleName;String sampleName;of the metric (e.g., "Ops");true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);@param;String valueName;String valueName;of the metric (e.g., "Time" or "Latency");true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String sampleName;String sampleName;of the metric (e.g., "Ops");true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String valueName;String valueName;of the metric (e.g., "Time" or "Latency");true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableQuantiles newQuantiles(String name, String desc, String sampleName, String valueName, int interval);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableStat newStat(String name, String desc, String sampleName, String valueName);Free text;;;Create a mutable metric with stats ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String desc;String desc;metric description;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String sampleName;String sampleName;of the metric (e.g., "Ops");true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableStat newStat(String name, String desc, String sampleName, String valueName);@param;String valueName;String valueName;of the metric (e.g., "Time" or "Latency");true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName, boolean extended);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName);MutableRate newRate(String name);@return;;;a new mutable metric object;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName);MutableRate newRate(String name);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableStat newStat(String name, String desc, String sampleName, String valueName);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name);MutableRate newRate(String name, String description);Free text;;;Create a mutable rate metric ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name);MutableRate newRate(String name, String description);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name);MutableRate newRate(String name, String description);@param;String name;String description;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name, String description);MutableRate newRate(String name, String desc, boolean extended);@return;;;a new mutable rate metric object;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name, String description);MutableRate newRate(String name, String desc, boolean extended);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name, String description);MutableRate newRate(String name, String desc, boolean extended);@param;String description;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name, String description);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name, String description);void add(String name, long value);@param;String description;String name;of the metric;false
org.apache.hadoop.metrics2.lib.MetricsRegistry;MutableRate newRate(String name, String desc, boolean extended);void add(String name, long value);@param;String name;String name;of the metric;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);Free text;;;Add a tag to the metrics ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@return;;;the registry (for keep adding tags);true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String name;String name;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String name;String description;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String name;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String description;String name;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String description;String description;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String description;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String value;String name;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String value;String description;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(String name, String description, String value, boolean override);@param;String value;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(MetricsInfo info, String value, boolean override);Free text;;;Add a tag to the metrics ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;String name;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;String description;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;String value;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value, boolean override);MetricsRegistry tag(MetricsInfo info, String value, boolean override);Free text;;;Add a tag to the metrics ;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value, boolean override);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;String name;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value, boolean override);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;String description;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value, boolean override);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;String value;String value;of the tag;true
org.apache.hadoop.metrics2.lib.MetricsRegistry;MetricsRegistry tag(String name, String description, String value, boolean override);MetricsRegistry tag(MetricsInfo info, String value, boolean override);@param;boolean override;boolean override;existing tag if true;true
org.apache.hadoop.metrics2.lib.MutableMetricsFactory;MutableMetric newForField(Field field, Metric annotation);MutableMetric newForMethod(Object source, Method method, Metric annotation);@return;;;a new metric object or null;false
org.apache.hadoop.metrics2.lib.MutableMetric;void snapshot(MetricsRecordBuilder builder, boolean all);void snapshot(MetricsRecordBuilder builder);@param;MetricsRecordBuilder builder;MetricsRecordBuilder builder;the metrics record builder;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder add(MetricsTag tag);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder add(AbstractMetric metric);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder setContext(String value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder addCounter(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder addCounter(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder tag(MetricsInfo info, String value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder add(AbstractMetric metric);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder setContext(String value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder addCounter(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder addCounter(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(MetricsTag tag);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(AbstractMetric metric);MetricsRecordBuilder setContext(String value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(AbstractMetric metric);MetricsRecordBuilder addCounter(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(AbstractMetric metric);MetricsRecordBuilder addCounter(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(AbstractMetric metric);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(AbstractMetric metric);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(AbstractMetric metric);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder add(AbstractMetric metric);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder setContext(String value);MetricsRecordBuilder addCounter(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder setContext(String value);MetricsRecordBuilder addCounter(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder setContext(String value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder setContext(String value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder setContext(String value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder setContext(String value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addCounter(MetricsInfo info, long value);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addCounter(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addCounter(MetricsInfo info, long value);@param;int value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@param;int value;int value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@param;int value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;int value;float value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;int value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, int value);@param;long value;int value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@param;long value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;long value;float value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;false
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addCounter(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;long value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, long value);@param;int value;long value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;int value;float value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, int value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;int value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, float value);@param;long value;float value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, long value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;long value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, float value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@return;;;self;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, float value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;MetricsInfo info;MetricsInfo info;metadata of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsRecordBuilder addGauge(MetricsInfo info, float value);MetricsRecordBuilder addGauge(MetricsInfo info, double value);@param;float value;double value;of the metric;true
org.apache.hadoop.metrics2.MetricsRecordBuilder;MetricsCollector parent();MetricsCollector endRecord();@return;;;the parent metrics collector object;false
org.apache.hadoop.metrics2.MetricsSystem;T register(String name, String desc, T source);T register(T source);@return;;;the source object;true
org.apache.hadoop.metrics2.MetricsSystem;T register(String name, String desc, T source);T register(T source);@param;T source;T source;object to register;true
org.apache.hadoop.tracing.TraceAdminProtocol;SpanReceiverInfo[] listSpanReceivers();long addSpanReceiver(SpanReceiverInfo desc);@throws;;;On error.;true
org.apache.hadoop.tracing.TraceAdminProtocol;SpanReceiverInfo[] listSpanReceivers();void removeSpanReceiver(long spanReceiverId);@throws;;;On error.;true
org.apache.hadoop.tracing.TraceAdminProtocol;long addSpanReceiver(SpanReceiverInfo desc);void removeSpanReceiver(long spanReceiverId);@throws;;;On error.;true
org.apache.hadoop.crypto.key.kms.ValueQueue;E getNext(String keyName);List getAtMost(String keyName, int num);@param;String keyName;String keyName;String key name;true
org.apache.hadoop.crypto.key.KeyShell;int run(String[] args);void main(String[] args);@param;String[] args;String[] args;Command line arguments.;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion getCurrentKey(String name);KeyVersion createKey(String name, byte[] material, Options options);@param;String name;String name;the base name of the key;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion getCurrentKey(String name);KeyVersion createKey(String name, Options options);@param;String name;String name;the base name of the key;true
org.apache.hadoop.crypto.key.KeyProvider;Metadata getMetadata(String name);KeyVersion rollNewVersion(String name, byte[] material);@param;String name;String name;the basename of the key;true
org.apache.hadoop.crypto.key.KeyProvider;Metadata getMetadata(String name);KeyVersion rollNewVersion(String name);@param;String name;String name;the basename of the key;true
org.apache.hadoop.crypto.key.KeyProvider;Metadata getMetadata(String name);String buildVersionName(String name, int version);@param;String name;String name;the basename of the key;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion createKey(String name, byte[] material, Options options);KeyVersion createKey(String name, Options options);@return;;;the version name of the first version of the key.;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion createKey(String name, byte[] material, Options options);KeyVersion createKey(String name, Options options);@param;String name;String name;the base name of the key;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion createKey(String name, byte[] material, Options options);KeyVersion createKey(String name, Options options);@param;Options options;Options options;the options for the new key.;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion rollNewVersion(String name, byte[] material);KeyVersion rollNewVersion(String name);@return;;;the name of the new version of the key;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion rollNewVersion(String name, byte[] material);KeyVersion rollNewVersion(String name);@param;String name;String name;the basename of the key;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion rollNewVersion(String name, byte[] material);String buildVersionName(String name, int version);@param;String name;String name;the basename of the key;true
org.apache.hadoop.crypto.key.KeyProvider;KeyVersion rollNewVersion(String name);String buildVersionName(String name, int version);@param;String name;String name;the basename of the key;true
org.apache.hadoop.crypto.CryptoCodec;CryptoCodec getInstance(Configuration conf, CipherSuite cipherSuite);CryptoCodec getInstance(Configuration conf);@param;Configuration conf;Configuration conf;the configuration;true
org.apache.hadoop.crypto.OpensslCipher;int update(ByteBuffer input, ByteBuffer output);int doFinal(ByteBuffer output);@return;;;int number of bytes stored in output;false
org.apache.hadoop.crypto.OpensslCipher;int update(ByteBuffer input, ByteBuffer output);int doFinal(ByteBuffer output);@param;ByteBuffer output;ByteBuffer output;the output ByteBuffer;true
org.apache.hadoop.tools.GetGroupsBase; GetGroupsBase(Configuration conf, PrintStream out);InetSocketAddress getProtocolAddress(Configuration conf);@param;Configuration conf;Configuration conf;The configuration to use.;true
org.apache.hadoop.net.SocketInputStream; SocketInputStream(Socket socket, long timeout); SocketInputStream(Socket socket);@param;Socket socket;Socket socket;should have a channel associated with it.;true
org.apache.hadoop.net.NetworkTopology;Node getNodeForNetworkLocation(Node node);Node getNode(String loc);@return;;;a reference to the node, null if the node is not in the tree;false
org.apache.hadoop.net.NetworkTopology;int getWeight(Node reader, Node node);void sortByDistance(Node reader, Node[] nodes, int activeLen);@param;Node reader;Node reader;Node where data will be read;true
org.apache.hadoop.net.NetUtils;SocketFactory getSocketFactory(Configuration conf, Class clazz);SocketFactory getDefaultSocketFactory(Configuration conf);@param;Configuration conf;Configuration conf;the configuration;true
org.apache.hadoop.net.NetUtils;InetSocketAddress getConnectAddress(Server server);InetSocketAddress getConnectAddress(InetSocketAddress addr);@return;;;socket address that a client can use to connect to the server.;true
org.apache.hadoop.net.NetUtils;OutputStream getOutputStream(Socket socket);OutputStream getOutputStream(Socket socket, long timeout);@return;;;OutputStream for writing to the socket.;true
org.apache.hadoop.net.NetUtils;void connect(Socket socket, SocketAddress address, int timeout);void connect(Socket socket, SocketAddress endpoint, SocketAddress localAddr, int timeout);@param;SocketAddress address;SocketAddress endpoint;the remote address;true
org.apache.hadoop.net.NetUtils;void connect(Socket socket, SocketAddress address, int timeout);void connect(Socket socket, SocketAddress endpoint, SocketAddress localAddr, int timeout);@param;int timeout;int timeout;timeout in milliseconds;true
org.apache.hadoop.net.unix.DomainSocket;DomainSocket bindAndListen(String path);DomainSocket connect(String path);@return;;;The new DomainSocket.;false
org.apache.hadoop.net.DNS;String[] getIPs(String strInterface, boolean returnSubinterfaces);String getDefaultIP(String strInterface);@throws;;;If the given interface is invalid;true
org.apache.hadoop.net.DNS;String[] getHosts(String strInterface, String nameserver);String getDefaultHost(String strInterface, String nameserver);@param;String nameserver;String nameserver;The DNS host name;true
org.apache.hadoop.net.DNS;String[] getHosts(String strInterface);String getDefaultHost(String strInterface, String nameserver);@param;String strInterface;String strInterface;The name of the network interface to query (e.g. eth0);true
org.apache.hadoop.net.DNS;String[] getHosts(String strInterface);String getDefaultHost(String strInterface, String nameserver);@throws;;;If one is encountered while querying the default interface;true
org.apache.hadoop.net.DNS;String[] getHosts(String strInterface);String getDefaultHost(String strInterface);@throws;;;If one is encountered while querying the default interface;true
org.apache.hadoop.net.DNS;String getDefaultHost(String strInterface, String nameserver);String getDefaultHost(String strInterface);@throws;;;If one is encountered while querying the default interface;true
org.apache.hadoop.net.NodeBase; NodeBase(String name, String location); NodeBase(String name, String location, Node parent, int level);@param;String name;String name;this node's name (can be null, must not contain PATH_SEPARATOR);true
org.apache.hadoop.net.NodeBase; NodeBase(String name, String location); NodeBase(String name, String location, Node parent, int level);@param;String location;String location;this node's location;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeBool(boolean b, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeBool(boolean b, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeInt(int i, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeInt(int i, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeLong(long l, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeLong(long l, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeFloat(float f, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeFloat(float f, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeDouble(double d, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeDouble(double d, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeString(String s, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeString(String s, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeBuffer(Buffer buf, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void writeBuffer(Buffer buf, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeByte(byte b, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeInt(int i, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeInt(int i, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeLong(long l, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeLong(long l, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeFloat(float f, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeFloat(float f, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeDouble(double d, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeDouble(double d, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeString(String s, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeString(String s, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeBuffer(Buffer buf, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void writeBuffer(Buffer buf, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBool(boolean b, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeLong(long l, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeLong(long l, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeFloat(float f, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeFloat(float f, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeDouble(double d, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeDouble(double d, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeString(String s, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeString(String s, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeBuffer(Buffer buf, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void writeBuffer(Buffer buf, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeInt(int i, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeFloat(float f, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeFloat(float f, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeDouble(double d, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeDouble(double d, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeString(String s, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeString(String s, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeBuffer(Buffer buf, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void writeBuffer(Buffer buf, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeLong(long l, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void writeDouble(double d, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void writeDouble(double d, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void writeString(String s, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void writeString(String s, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void writeBuffer(Buffer buf, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void writeBuffer(Buffer buf, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeFloat(float f, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void writeString(String s, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void writeString(String s, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void writeBuffer(Buffer buf, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void writeBuffer(Buffer buf, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeDouble(double d, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void writeBuffer(Buffer buf, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void writeBuffer(Buffer buf, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeString(String s, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void startRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void startRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void writeBuffer(Buffer buf, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void endRecord(Record r, String tag);@param;Record r;Record r;Record to be serialized;true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void endRecord(Record r, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void endRecord(Record r, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startRecord(Record r, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void startVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void startVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void endRecord(Record r, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startVector(ArrayList v, String tag);void endVector(ArrayList v, String tag);@param;ArrayList v;ArrayList v;Vector to be serialized;true
org.apache.hadoop.record.RecordOutput;void startVector(ArrayList v, String tag);void endVector(ArrayList v, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startVector(ArrayList v, String tag);void endVector(ArrayList v, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startVector(ArrayList v, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startVector(ArrayList v, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startVector(ArrayList v, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startVector(ArrayList v, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void endVector(ArrayList v, String tag);void startMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void endVector(ArrayList v, String tag);void startMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void endVector(ArrayList v, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void endVector(ArrayList v, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.RecordOutput;void startMap(TreeMap m, String tag);void endMap(TreeMap m, String tag);@param;TreeMap m;TreeMap m;Map to be serialized;true
org.apache.hadoop.record.RecordOutput;void startMap(TreeMap m, String tag);void endMap(TreeMap m, String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordOutput;void startMap(TreeMap m, String tag);void endMap(TreeMap m, String tag);@throws;;;Indicates error in serialization;true
org.apache.hadoop.record.Utils;String toXMLString(String s);String fromXMLString(String s);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String toXMLString(String s);String toCSVString(String s);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String toXMLString(String s);String toXMLBuffer(Buffer s);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String toXMLString(String s);String toCSVBuffer(Buffer buf);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String fromXMLString(String s);String toCSVString(String s);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String fromXMLString(String s);String toXMLBuffer(Buffer s);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String fromXMLString(String s);String toCSVBuffer(Buffer buf);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String toCSVString(String s);String toXMLBuffer(Buffer s);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String toCSVString(String s);String toCSVBuffer(Buffer buf);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;String fromCSVString(String s);Buffer fromXMLBuffer(String s);Whole;;; @param  @return  @throws ;false
org.apache.hadoop.record.Utils;String toXMLBuffer(Buffer s);String toCSVBuffer(Buffer buf);Whole;;; @param  @return  ;false
org.apache.hadoop.record.Utils;long readVLong(byte[] bytes, int start);long readVLong(DataInput in);@return;;;deserialized long;true
org.apache.hadoop.record.Utils;int readVInt(byte[] bytes, int start);int readVInt(DataInput in);@return;;;deserialized integer;true
org.apache.hadoop.record.Utils;long readVLong(DataInput in);int readVInt(DataInput in);@param;DataInput in;DataInput in;input stream;true
org.apache.hadoop.record.Utils;void writeVLong(DataOutput stream, long i);void writeVInt(DataOutput stream, int i);@param;DataOutput stream;DataOutput stream;Binary output stream;true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);boolean readBool(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;byte readByte(String tag);boolean readBool(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);int readInt(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;byte readByte(String tag);int readInt(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);long readLong(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;byte readByte(String tag);long readLong(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);float readFloat(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;byte readByte(String tag);float readFloat(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);double readDouble(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;byte readByte(String tag);double readDouble(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);String readString(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;byte readByte(String tag);String readString(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);Buffer readBuffer(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;byte readByte(String tag);Buffer readBuffer(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;byte readByte(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);int readInt(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);int readInt(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);long readLong(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);long readLong(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);float readFloat(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);float readFloat(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);double readDouble(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);double readDouble(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);String readString(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);String readString(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);Buffer readBuffer(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);Buffer readBuffer(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;boolean readBool(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);long readLong(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;int readInt(String tag);long readLong(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);float readFloat(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;int readInt(String tag);float readFloat(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);double readDouble(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;int readInt(String tag);double readDouble(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);String readString(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;int readInt(String tag);String readString(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);Buffer readBuffer(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;int readInt(String tag);Buffer readBuffer(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;int readInt(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);float readFloat(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;long readLong(String tag);float readFloat(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);double readDouble(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;long readLong(String tag);double readDouble(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);String readString(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;long readLong(String tag);String readString(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);Buffer readBuffer(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;long readLong(String tag);Buffer readBuffer(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;long readLong(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);double readDouble(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;float readFloat(String tag);double readDouble(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);String readString(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;float readFloat(String tag);String readString(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);Buffer readBuffer(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;float readFloat(String tag);Buffer readBuffer(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;float readFloat(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);String readString(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;double readDouble(String tag);String readString(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);Buffer readBuffer(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;double readDouble(String tag);Buffer readBuffer(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;double readDouble(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;String readString(String tag);Buffer readBuffer(String tag);@return;;;value read from serialized record.;false
org.apache.hadoop.record.RecordInput;String readString(String tag);Buffer readBuffer(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;String readString(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;String readString(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;String readString(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;String readString(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;String readString(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;String readString(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Buffer readBuffer(String tag);void startRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Buffer readBuffer(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Buffer readBuffer(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Buffer readBuffer(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Buffer readBuffer(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Buffer readBuffer(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void startRecord(String tag);void endRecord(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void startRecord(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void startRecord(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void startRecord(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void startRecord(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void endRecord(String tag);Index startVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void endRecord(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void endRecord(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void endRecord(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Index startVector(String tag);void endVector(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Index startVector(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Index startVector(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void endVector(String tag);Index startMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;void endVector(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.RecordInput;Index startMap(String tag);void endMap(String tag);@param;String tag;String tag;Used by tagged serialization formats (such as XML);true
org.apache.hadoop.record.Buffer; Buffer(byte[] bytes, int offset, int length);void copy(byte[] bytes, int offset, int length);@param;int offset;int offset;offset into byte array;true
org.apache.hadoop.record.Buffer; Buffer(byte[] bytes, int offset, int length);void copy(byte[] bytes, int offset, int length);@param;int length;int length;length of data;true
org.apache.hadoop.record.Buffer; Buffer(byte[] bytes, int offset, int length);void append(byte[] bytes, int offset, int length);@param;int offset;int offset;offset into byte array;true
org.apache.hadoop.record.Buffer; Buffer(byte[] bytes, int offset, int length);void append(byte[] bytes, int offset, int length);@param;int length;int length;length of data;true
org.apache.hadoop.record.Buffer;void copy(byte[] bytes, int offset, int length);void append(byte[] bytes, int offset, int length);@param;int offset;int offset;offset into byte array;true
org.apache.hadoop.record.Buffer;void copy(byte[] bytes, int offset, int length);void append(byte[] bytes, int offset, int length);@param;int length;int length;length of data;true
org.apache.hadoop.record.Buffer;void append(byte[] bytes, int offset, int length);void append(byte[] bytes);@param;byte[] bytes;byte[] bytes;byte array to be appended;true
org.apache.hadoop.record.Record;void serialize(RecordOutput rout, String tag);void serialize(RecordOutput rout);@param;RecordOutput rout;RecordOutput rout;Record output destination;true
org.apache.hadoop.record.Record;void deserialize(RecordInput rin, String tag);void deserialize(RecordInput rin);@param;RecordInput rin;RecordInput rin;Record input source;true
org.apache.hadoop.util.StringUtils;String uriToString(URI[] uris);Path[] stringToPath(String[] str);Whole;;; @param   ;false
org.apache.hadoop.util.StringUtils;String formatTimeDiff(long finishTime, long startTime);String getFormattedTimeWithDiff(DateFormat dateFormat, long finishTime, long startTime);@param;long startTime;long startTime;start time;true
org.apache.hadoop.util.StringUtils;Collection getStringCollection(String str);Collection getStringCollection(String str, String delim);Free text;;;Returns a collection of strings. ;true
org.apache.hadoop.util.StringUtils;Collection getTrimmedStringCollection(String str);String[] getTrimmedStrings(String str);@param;String str;String str;a comma separated <String> with values;true
org.apache.hadoop.util.StringUtils;String[] split(String str);String[] split(String str, char escapeChar, char separator);@return;;;an array of strings;true
org.apache.hadoop.util.StringUtils;String[] split(String str);String[] split(String str, char escapeChar, char separator);@param;String str;String str;a string that may have escaped separator;true
org.apache.hadoop.util.StringUtils;String[] split(String str);String[] split(String str, char separator);@return;;;an array of strings;true
org.apache.hadoop.util.StringUtils;String[] split(String str, char escapeChar, char separator);String[] split(String str, char separator);@return;;;an array of strings;true
org.apache.hadoop.util.StringUtils;String[] split(String str, char escapeChar, char separator);String[] split(String str, char separator);@param;char separator;char separator;a separator char;true
org.apache.hadoop.util.StringUtils;String escapeString(String str);String escapeString(String str, char escapeChar, char charToEscape);@return;;;an escaped string;true
org.apache.hadoop.util.StringUtils;String escapeString(String str);String unEscapeString(String str);@param;String str;String str;a string;true
org.apache.hadoop.util.StringUtils;String escapeString(String str, char escapeChar, char charToEscape);String unEscapeString(String str, char escapeChar, char charToEscape);@param;String str;String str;string;true
org.apache.hadoop.util.StringUtils;String escapeString(String str, char escapeChar, char charToEscape);String unEscapeString(String str, char escapeChar, char charToEscape);@param;char escapeChar;char escapeChar;escape char;true
org.apache.hadoop.util.StringUtils;String unEscapeString(String str);String unEscapeString(String str, char escapeChar, char charToEscape);@return;;;an unescaped string;true
org.apache.hadoop.util.StringUtils;String join(CharSequence separator, Iterable strings);String join(CharSequence separator, String[] strings);Free text;;;Concatenates strings, using a separator. ;true
org.apache.hadoop.util.StringUtils;String popOptionWithArgument(String name, List args);boolean popOption(String name, List args);@param;String name;String name;Name of the option to remove. Example: -foo.;true
org.apache.hadoop.util.StringUtils;String popOptionWithArgument(String name, List args);boolean popOption(String name, List args);@param;List<String> args;List<String> args;List of arguments.;true
org.apache.hadoop.util.StringUtils;String popOptionWithArgument(String name, List args);String popFirstNonOption(List args);@param;List<String> args;List<String> args;List of arguments.;true
org.apache.hadoop.util.StringUtils;boolean popOption(String name, List args);String popFirstNonOption(List args);@param;List<String> args;List<String> args;List of arguments.;true
org.apache.hadoop.util.GenericsUtil;T[] toArray(Class c, List list);T[] toArray(List list);Free text;;;Converts the given <code>List&lt,T&gt,</code> to a an array of <code>T[]</code>. ;true
org.apache.hadoop.util.GenericsUtil;T[] toArray(Class c, List list);T[] toArray(List list);@param;List<T> list;List<T> list;the list to convert;true
org.apache.hadoop.util.NativeCodeLoader;boolean getLoadNativeLibraries(Configuration conf);void setLoadNativeLibraries(Configuration conf, boolean loadNativeLibraries);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.util.ServletUtil;String percentageGraph(int perc, int width);String percentageGraph(float perc, int width);Whole;;;Generate the percentage graph and returns HTML representation string of the same.  @param The percentage value for which graph is to be generated@param The width of the display table @return HTML String representation of the percentage graph @throws ;false
org.apache.hadoop.util.ServletUtil;String getDecodedPath(HttpServletRequest request, String servletName);String getRawPath(HttpServletRequest request, String servletName);@param;HttpServletRequest request;HttpServletRequest request;Http request to parse;true
org.apache.hadoop.util.ServletUtil;String getDecodedPath(HttpServletRequest request, String servletName);String getRawPath(HttpServletRequest request, String servletName);@param;String servletName;String servletName;the name of servlet that precedes the path;true
org.apache.hadoop.util.hash.Hash;int parseHashType(String name);int getHashType(Configuration conf);@return;;;one of the predefined constants;false
org.apache.hadoop.util.hash.Hash;int hash(byte[] bytes);int hash(byte[] bytes, int initval);@return;;;hash value;true
org.apache.hadoop.util.hash.Hash;int hash(byte[] bytes);int hash(byte[] bytes, int initval);@param;byte[] bytes;byte[] bytes;input bytes;true
org.apache.hadoop.util.hash.Hash;int hash(byte[] bytes);int hash(byte[] bytes, int length, int initval);@return;;;hash value;true
org.apache.hadoop.util.hash.Hash;int hash(byte[] bytes);int hash(byte[] bytes, int length, int initval);@param;byte[] bytes;byte[] bytes;input bytes;true
org.apache.hadoop.util.hash.Hash;int hash(byte[] bytes, int initval);int hash(byte[] bytes, int length, int initval);@return;;;hash value;true
org.apache.hadoop.util.hash.Hash;int hash(byte[] bytes, int initval);int hash(byte[] bytes, int length, int initval);@param;byte[] bytes;byte[] bytes;input bytes;true
org.apache.hadoop.util.hash.Hash;int hash(byte[] bytes, int initval);int hash(byte[] bytes, int length, int initval);@param;int initval;int initval;seed value;true
org.apache.hadoop.util.NativeCrc32;void verifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data, String fileName, long basePos);void nativeVerifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, int sumsOffset, ByteBuffer data, int dataOffset, int dataLength, String fileName, long basePos);@param;int bytesPerSum;int bytesPerSum;the chunk size (eg 512 bytes);true
org.apache.hadoop.util.NativeCrc32;void verifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data, String fileName, long basePos);void nativeVerifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, int sumsOffset, ByteBuffer data, int dataOffset, int dataLength, String fileName, long basePos);@param;ByteBuffer sums;ByteBuffer sums;the DirectByteBuffer pointing at the beginning of the stored checksums;true
org.apache.hadoop.util.NativeCrc32;void verifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data, String fileName, long basePos);void nativeVerifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, int sumsOffset, ByteBuffer data, int dataOffset, int dataLength, String fileName, long basePos);@param;ByteBuffer data;ByteBuffer data;the DirectByteBuffer pointing at the beginning of the data to check;true
org.apache.hadoop.util.NativeCrc32;void verifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data, String fileName, long basePos);void nativeVerifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, int sumsOffset, ByteBuffer data, int dataOffset, int dataLength, String fileName, long basePos);@param;long basePos;long basePos;the position in the file where the data buffer starts;true
org.apache.hadoop.util.NativeCrc32;void verifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data, String fileName, long basePos);void nativeVerifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, int sumsOffset, ByteBuffer data, int dataOffset, int dataLength, String fileName, long basePos);@param;String fileName;String fileName;the name of the file being verified;true
org.apache.hadoop.util.NativeCrc32;void verifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data, String fileName, long basePos);void nativeVerifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, int sumsOffset, ByteBuffer data, int dataOffset, int dataLength, String fileName, long basePos);@throws;;;if there is an invalid checksum;true
org.apache.hadoop.util.ShutdownThreadsHelper;boolean shutdownThread(Thread thread);boolean shutdownThread(Thread thread, long timeoutInMilliSeconds);@return;;;true if the thread is successfully interrupted, false otherwise;true
org.apache.hadoop.util.ShutdownThreadsHelper;boolean shutdownThread(Thread thread);boolean shutdownThread(Thread thread, long timeoutInMilliSeconds);@param;Thread thread;Thread thread;Thread to be shutdown;true
org.apache.hadoop.util.ShutdownThreadsHelper;boolean shutdownExecutorService(ExecutorService service);boolean shutdownExecutorService(ExecutorService service, long timeoutInMs);@return;;;true if the service is terminated, false otherwise;true
org.apache.hadoop.util.ShutdownThreadsHelper;boolean shutdownExecutorService(ExecutorService service);boolean shutdownExecutorService(ExecutorService service, long timeoutInMs);@param;ExecutorService service;ExecutorService service;ExecutorService to be shutdown;true
org.apache.hadoop.util.ExitUtil;void terminate(int status, String msg);void halt(int status, String msg);@param;int status;int status;exit code;true
org.apache.hadoop.util.ExitUtil;void terminate(int status, String msg);void terminate(int status, Throwable t);@throws;;;if System.exit is disabled for test purposes;true
org.apache.hadoop.util.ExitUtil;void terminate(int status, String msg);void terminate(int status);@throws;;;if System.exit is disabled for test purposes;true
org.apache.hadoop.util.ExitUtil;void halt(int status, String msg);void halt(int status, Throwable t);Free text;;;Forcibly terminates the currently running Java virtual machine. ;true
org.apache.hadoop.util.ExitUtil;void halt(int status, String msg);void halt(int status);Free text;;;Forcibly terminates the currently running Java virtual machine. ;true
org.apache.hadoop.util.ExitUtil;void terminate(int status, Throwable t);void terminate(int status);@throws;;;if System.exit is disabled for test purposes;true
org.apache.hadoop.util.ExitUtil;void halt(int status, Throwable t);void halt(int status);Free text;;;Forcibly terminates the currently running Java virtual machine. ;true
org.apache.hadoop.util.Shell;File appendScriptExtension(File parent, String basename);String appendScriptExtension(String basename);@param;String basename;String basename;String script file basename;true
org.apache.hadoop.util.Shell;String execCommand(String cmd);String execCommand(Map env, String cmd);Free text;;;Static method to execute a shell command. Covers most of the simple cases without requiring the user to implement the <code>Shell</code> interface. ;true
org.apache.hadoop.util.Shell;String execCommand(String cmd);String execCommand(Map env, String cmd);@return;;;the output of the executed command.;true
org.apache.hadoop.util.Shell;String execCommand(String cmd);String execCommand(Map env, String cmd);@param;String cmd;String cmd;shell command to execute.;true
org.apache.hadoop.util.HttpExceptionUtils;void createServletExceptionResponse(HttpServletResponse response, int status, Throwable ex);Response createJerseyExceptionResponse(Response.Status status, Throwable ex);@param;int status;Response.Status status;the error code to set in the response;true
org.apache.hadoop.util.HttpExceptionUtils;void createServletExceptionResponse(HttpServletResponse response, int status, Throwable ex);Response createJerseyExceptionResponse(Response.Status status, Throwable ex);@param;Throwable ex;Throwable ex;the exception to serialize in the response;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findBytes(byte[] utf, int start, int end, byte[] b);@return;;;position that first byte occures otherwise -1;false
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findBytes(byte[] utf, int start, int end, byte[] b);@param;byte[] utf;byte[] utf;a byte array containing a UTF-8 encoded string;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findBytes(byte[] utf, int start, int end, byte[] b);@param;int start;int start;starting offset;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findBytes(byte[] utf, int start, int end, byte[] b);@param;int end;int end;ending position;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findNthByte(byte[] utf, int start, int length, byte b, int n);@param;byte[] utf;byte[] utf;a byte array containing a UTF-8 encoded string;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findNthByte(byte[] utf, int start, int length, byte b, int n);@param;int start;int start;starting offset;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findNthByte(byte[] utf, int start, int length, byte b, int n);@param;byte b;byte b;the byte to find;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findNthByte(byte[] utf, byte b, int n);@param;byte[] utf;byte[] utf;a byte array containing a UTF-8 encoded string;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findByte(byte[] utf, int start, int end, byte b);int findNthByte(byte[] utf, byte b, int n);@param;byte b;byte b;the byte to find;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findBytes(byte[] utf, int start, int end, byte[] b);int findNthByte(byte[] utf, int start, int length, byte b, int n);@param;byte[] utf;byte[] utf;a byte array containing a UTF-8 encoded string;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findBytes(byte[] utf, int start, int end, byte[] b);int findNthByte(byte[] utf, int start, int length, byte b, int n);@param;int start;int start;starting offset;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findBytes(byte[] utf, int start, int end, byte[] b);int findNthByte(byte[] utf, byte b, int n);@param;byte[] utf;byte[] utf;a byte array containing a UTF-8 encoded string;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findNthByte(byte[] utf, int start, int length, byte b, int n);int findNthByte(byte[] utf, byte b, int n);Free text;;;Find the nth occurrence of the given byte b in a UTF-8 encoded string ;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findNthByte(byte[] utf, int start, int length, byte b, int n);int findNthByte(byte[] utf, byte b, int n);@return;;;position that nth occurrence of the given byte if exists, otherwise -1;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findNthByte(byte[] utf, int start, int length, byte b, int n);int findNthByte(byte[] utf, byte b, int n);@param;byte[] utf;byte[] utf;a byte array containing a UTF-8 encoded string;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findNthByte(byte[] utf, int start, int length, byte b, int n);int findNthByte(byte[] utf, byte b, int n);@param;byte b;byte b;the byte to find;true
org.apache.hadoop.util.UTF8ByteArrayUtils;int findNthByte(byte[] utf, int start, int length, byte b, int n);int findNthByte(byte[] utf, byte b, int n);@param;int n;int n;the desired occurrence of the given byte;true
org.apache.hadoop.util.DiskChecker;void mkdirsWithExistsAndPermissionCheck(LocalFileSystem localFS, Path dir, FsPermission expected);void checkDir(LocalFileSystem localFS, Path dir, FsPermission expected);@param;LocalFileSystem localFS;LocalFileSystem localFS;local filesystem;true
org.apache.hadoop.util.ToolRunner;int run(Configuration conf, Tool tool, String[] args);int run(Tool tool, String[] args);@return;;;exit code of the Tool#run(String[]) method.;true
org.apache.hadoop.util.ToolRunner;int run(Configuration conf, Tool tool, String[] args);int run(Tool tool, String[] args);@param;Tool tool;Tool tool;Tool to run.;true
org.apache.hadoop.util.ToolRunner;int run(Configuration conf, Tool tool, String[] args);int run(Tool tool, String[] args);@param;String[] args;String[] args;command-line arguments to the tool.;true
org.apache.hadoop.util.StringInterner;String strongIntern(String sample);String weakIntern(String sample);@param;String sample;String sample;string instance to be interned;true
org.apache.hadoop.util.GSet;boolean contains(K key);E get(K key);@param;K key;K key;The given key.;true
org.apache.hadoop.util.GSet;boolean contains(K key);E get(K key);@throws;;;if key == null.;true
org.apache.hadoop.util.GSet;boolean contains(K key);E remove(K key);@throws;;;if key == null.;true
org.apache.hadoop.util.GSet;E get(K key);E remove(K key);@throws;;;if key == null.;true
org.apache.hadoop.util.DataChecksum;int writeValue(DataOutputStream out, boolean reset);int writeValue(byte[] buf, int offset, boolean reset);@return;;;number of bytes written. Will be equal to getChecksumSize(),;true
org.apache.hadoop.util.IntrusiveCollection;boolean add(E elem);boolean addFirst(Element elem);@param;E elem;Element elem;The new element to add.;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in); LineReader(InputStream in, int bufferSize);@param;InputStream in;InputStream in;The input stream;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in); LineReader(InputStream in, byte[] recordDelimiterBytes);@param;InputStream in;InputStream in;The input stream;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in); LineReader(InputStream in, int bufferSize, byte[] recordDelimiterBytes);@param;InputStream in;InputStream in;The input stream;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, int bufferSize); LineReader(InputStream in, byte[] recordDelimiterBytes);@param;InputStream in;InputStream in;The input stream;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, int bufferSize); LineReader(InputStream in, int bufferSize, byte[] recordDelimiterBytes);@param;InputStream in;InputStream in;The input stream;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, int bufferSize); LineReader(InputStream in, int bufferSize, byte[] recordDelimiterBytes);@param;int bufferSize;int bufferSize;Size of the read buffer;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, Configuration conf); LineReader(InputStream in, Configuration conf, byte[] recordDelimiterBytes);@param;InputStream in;InputStream in;input stream;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, Configuration conf); LineReader(InputStream in, Configuration conf, byte[] recordDelimiterBytes);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, byte[] recordDelimiterBytes); LineReader(InputStream in, int bufferSize, byte[] recordDelimiterBytes);@param;InputStream in;InputStream in;The input stream;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, byte[] recordDelimiterBytes); LineReader(InputStream in, int bufferSize, byte[] recordDelimiterBytes);@param;byte[] recordDelimiterBytes;byte[] recordDelimiterBytes;The delimiter;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, byte[] recordDelimiterBytes); LineReader(InputStream in, Configuration conf, byte[] recordDelimiterBytes);@param;byte[] recordDelimiterBytes;byte[] recordDelimiterBytes;The delimiter;true
org.apache.hadoop.util.LineReader; LineReader(InputStream in, int bufferSize, byte[] recordDelimiterBytes); LineReader(InputStream in, Configuration conf, byte[] recordDelimiterBytes);@param;byte[] recordDelimiterBytes;byte[] recordDelimiterBytes;The delimiter;true
org.apache.hadoop.util.LineReader;int readLine(Text str, int maxLineLength, int maxBytesToConsume);int readLine(Text str, int maxLineLength);@throws;;;if the underlying stream throws;true
org.apache.hadoop.util.LineReader;int readLine(Text str, int maxLineLength, int maxBytesToConsume);int readLine(Text str);@throws;;;if the underlying stream throws;true
org.apache.hadoop.util.LineReader;int readLine(Text str, int maxLineLength);int readLine(Text str);Free text;;;Read from the InputStream into the given Text. ;true
org.apache.hadoop.util.LineReader;int readLine(Text str, int maxLineLength);int readLine(Text str);@return;;;the number of bytes read including the newline;true
org.apache.hadoop.util.LineReader;int readLine(Text str, int maxLineLength);int readLine(Text str);@param;Text str;Text str;the object to store the given line;true
org.apache.hadoop.util.LineReader;int readLine(Text str, int maxLineLength);int readLine(Text str);@throws;;;if the underlying stream throws;true
org.apache.hadoop.util.CloseableReferenceCount;void reference();void unreferenceCheckClosed();@throws;;;If the status is closed.;false
org.apache.hadoop.util.CloseableReferenceCount;int setClosed();int getReferenceCount();@return;;;The current reference count.;false
org.apache.hadoop.util.GenericOptionsParser; GenericOptionsParser(Options opts, String[] args); GenericOptionsParser(String[] args);@param;String[] args;String[] args;the command line arguments;true
org.apache.hadoop.util.ReflectionUtils;void setConf(Object theObject, Configuration conf);T newInstance(Class theClass, Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.security.token.SecretManager;byte[] retrievePassword(T identifier);byte[] retriableRetrievePassword(T identifier);@return;;;the password to use;false
org.apache.hadoop.security.token.SecretManager;byte[] retrievePassword(T identifier);byte[] retriableRetrievePassword(T identifier);@param;T identifier;T identifier;the identifier to validate;true
org.apache.hadoop.security.token.SecretManager;byte[] retrievePassword(T identifier);byte[] retriableRetrievePassword(T identifier);@throws;;;the token was invalid;true
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();int incrementCurrentKeyId();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();void setCurrentKeyId(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();int getDelegationTokenSeqNum();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();int incrementDelegationTokenSeqNum();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();void setDelegationTokenSeqNum(int seqNum);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();DelegationKey getDelegationKey(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();void storeDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getCurrentKeyId();void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();void setCurrentKeyId(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();int getDelegationTokenSeqNum();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();int incrementDelegationTokenSeqNum();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();void setDelegationTokenSeqNum(int seqNum);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();DelegationKey getDelegationKey(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();void storeDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementCurrentKeyId();void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);int getDelegationTokenSeqNum();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);int incrementDelegationTokenSeqNum();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);void setDelegationTokenSeqNum(int seqNum);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);DelegationKey getDelegationKey(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);void storeDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setCurrentKeyId(int keyId);void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();int incrementDelegationTokenSeqNum();Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();void setDelegationTokenSeqNum(int seqNum);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();DelegationKey getDelegationKey(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();void storeDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int getDelegationTokenSeqNum();void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementDelegationTokenSeqNum();void setDelegationTokenSeqNum(int seqNum);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementDelegationTokenSeqNum();DelegationKey getDelegationKey(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementDelegationTokenSeqNum();void storeDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementDelegationTokenSeqNum();void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementDelegationTokenSeqNum();DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementDelegationTokenSeqNum();void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;int incrementDelegationTokenSeqNum();void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setDelegationTokenSeqNum(int seqNum);DelegationKey getDelegationKey(int keyId);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setDelegationTokenSeqNum(int seqNum);void storeDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setDelegationTokenSeqNum(int seqNum);void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setDelegationTokenSeqNum(int seqNum);DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setDelegationTokenSeqNum(int seqNum);void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void setDelegationTokenSeqNum(int seqNum);void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;DelegationKey getDelegationKey(int keyId);void storeDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;DelegationKey getDelegationKey(int keyId);void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;DelegationKey getDelegationKey(int keyId);DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;DelegationKey getDelegationKey(int keyId);void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;DelegationKey getDelegationKey(int keyId);void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void storeDelegationKey(DelegationKey key);void updateDelegationKey(DelegationKey key);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void storeDelegationKey(DelegationKey key);DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void storeDelegationKey(DelegationKey key);void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void storeDelegationKey(DelegationKey key);void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void updateDelegationKey(DelegationKey key);DelegationTokenInformation getTokenInfo(TokenIdent ident);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void updateDelegationKey(DelegationKey key);void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void updateDelegationKey(DelegationKey key);void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;DelegationTokenInformation getTokenInfo(TokenIdent ident);void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;DelegationTokenInformation getTokenInfo(TokenIdent ident);void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;void storeToken(TokenIdent ident, DelegationTokenInformation tokenInfo);void updateToken(TokenIdent ident, DelegationTokenInformation tokenInfo);Whole;;;For subclasses externalizing the storage, for example Zookeeper based implementations    ;false
org.apache.hadoop.security.token.Token; Token(T id, SecretManager mgr); Token(byte[] identifier, byte[] password, Text kind, Text service);@param;T id;byte[] identifier;the token identifier;true
org.apache.hadoop.security.UserGroupInformation;UserGroupInformation getCurrentUser();UserGroupInformation getLoginUser();@throws;;;if login fails;true
org.apache.hadoop.security.UserGroupInformation;UserGroupInformation getCurrentUser();void loginUserFromSubject(Subject subject);@throws;;;if login fails;true
org.apache.hadoop.security.UserGroupInformation;UserGroupInformation getUGIFromTicketCache(String ticketCache, String user);UserGroupInformation getUGIFromSubject(Subject subject);@throws;;;if the kerberos login fails;true
org.apache.hadoop.security.UserGroupInformation;UserGroupInformation getLoginUser();void loginUserFromSubject(Subject subject);@throws;;;if login fails;true
org.apache.hadoop.security.UserGroupInformation;void loginUserFromKeytab(String user, String path);UserGroupInformation loginUserFromKeytabAndReturnUGI(String user, String path);@param;String user;String user;the principal name to load from the keytab;true
org.apache.hadoop.security.UserGroupInformation;void loginUserFromKeytab(String user, String path);UserGroupInformation loginUserFromKeytabAndReturnUGI(String user, String path);@param;String path;String path;the path to the keytab file;true
org.apache.hadoop.security.UserGroupInformation;void loginUserFromKeytab(String user, String path);UserGroupInformation loginUserFromKeytabAndReturnUGI(String user, String path);@throws;;;if the keytab file can't be read;true
org.apache.hadoop.security.UserGroupInformation;void reloginFromKeytab();void reloginFromTicketCache();@throws;;;on a failure;true
org.apache.hadoop.security.UserGroupInformation;boolean isLoginKeytabBased();boolean isLoginTicketBased();@return;;;true or false;false
org.apache.hadoop.security.UserGroupInformation;UserGroupInformation createRemoteUser(String user);UserGroupInformation createRemoteUser(String user, AuthMethod authMethod);Whole;;;Create a user from a login name. It is intended to be used for remote users in RPC, since it won't have any credentials.  @param the full user principal name, must not be empty or null @return the UserGroupInformation for the remote user. ;false
org.apache.hadoop.security.UserGroupInformation;UserGroupInformation createUserForTesting(String user, String[] userGroups);UserGroupInformation createProxyUserForTesting(String user, UserGroupInformation realUser, String[] userGroups);@return;;;a fake user for running unit tests;false
org.apache.hadoop.security.UserGroupInformation;UserGroupInformation createUserForTesting(String user, String[] userGroups);UserGroupInformation createProxyUserForTesting(String user, UserGroupInformation realUser, String[] userGroups);@param;String[] userGroups;String[] userGroups;the names of the groups that the user belongs to;true
org.apache.hadoop.security.UserGroupInformation;boolean addToken(Token token);boolean addToken(Text alias, Token token);@return;;;true on successful add of new token;true
org.apache.hadoop.security.UserGroupInformation;boolean addToken(Token token);boolean addToken(Text alias, Token token);@param;Token<? extends TokenIdentifier> token;Token<? extends TokenIdentifier> token;Token to be added;true
org.apache.hadoop.security.UserGroupInformation;void setAuthenticationMethod(AuthenticationMethod authMethod);void setAuthenticationMethod(AuthMethod authMethod);Whole;;;Sets the authentication method in the subject  @param   ;false
org.apache.hadoop.security.UserGroupInformation;AuthenticationMethod getAuthenticationMethod();AuthenticationMethod getRealAuthenticationMethod();@return;;;AuthenticationMethod in the subject, null if not present.;false
org.apache.hadoop.security.UserGroupInformation;T doAs(PrivilegedAction action);T doAs(PrivilegedExceptionAction action);@return;;;the value from the run method;true
org.apache.hadoop.security.UserGroupInformation;T doAs(PrivilegedAction action);T doAs(PrivilegedExceptionAction action);@param;PrivilegedAction<T> action;PrivilegedExceptionAction<T> action;the method to execute;true
org.apache.hadoop.security.SecurityUtil;boolean isTGSPrincipal(KerberosPrincipal principal);boolean isOriginalTGT(KerberosTicket ticket);@return;;;true or false;false
org.apache.hadoop.security.SecurityUtil;String getServerPrincipal(String principalConfig, String hostname);String getServerPrincipal(String principalConfig, InetAddress addr);@return;;;converted Kerberos principal name;true
org.apache.hadoop.security.SecurityUtil;String getServerPrincipal(String principalConfig, String hostname);String getServerPrincipal(String principalConfig, InetAddress addr);@throws;;;if the client address cannot be determined;true
org.apache.hadoop.security.SecurityUtil;void login(Configuration conf, String keytabFileKey, String userNameKey);void login(Configuration conf, String keytabFileKey, String userNameKey, String hostname);@param;Configuration conf;Configuration conf;conf to use;true
org.apache.hadoop.security.SecurityUtil;void login(Configuration conf, String keytabFileKey, String userNameKey);void login(Configuration conf, String keytabFileKey, String userNameKey, String hostname);@param;String keytabFileKey;String keytabFileKey;the key to look for keytab file in conf;true
org.apache.hadoop.security.SecurityUtil;void login(Configuration conf, String keytabFileKey, String userNameKey);void login(Configuration conf, String keytabFileKey, String userNameKey, String hostname);@param;String userNameKey;String userNameKey;the key to look for user's Kerberos principal name in conf;true
org.apache.hadoop.security.SecurityUtil;Text buildTokenService(InetSocketAddress addr);Text buildTokenService(URI uri);Free text;;;Construct the service key for a token ;true
org.apache.hadoop.security.SecurityUtil;Text buildTokenService(InetSocketAddress addr);Text buildTokenService(URI uri);@return;;;"ip:port" or "host:port" depending on the value of hadoop.security.token.service.use_ip;true
org.apache.hadoop.security.SecurityUtil;T doAsLoginUser(PrivilegedExceptionAction action);T doAsCurrentUser(PrivilegedExceptionAction action);@return;;;the result of the action;false
org.apache.hadoop.security.SecurityUtil;T doAsLoginUser(PrivilegedExceptionAction action);T doAsCurrentUser(PrivilegedExceptionAction action);@param;PrivilegedExceptionAction<T> action;PrivilegedExceptionAction<T> action;the action to perform;true
org.apache.hadoop.security.SecurityUtil;T doAsLoginUser(PrivilegedExceptionAction action);T doAsCurrentUser(PrivilegedExceptionAction action);@throws;;;in the event of error;true
org.apache.hadoop.security.Credentials;void addToken(Text alias, Token t);byte[] getSecretKey(Text alias);@param;Text alias;Text alias;the alias for the key;true
org.apache.hadoop.security.Credentials;void addToken(Text alias, Token t);void addSecretKey(Text alias, byte[] key);@param;Text alias;Text alias;the alias for the key;true
org.apache.hadoop.security.Credentials;void addToken(Text alias, Token t);void removeSecretKey(Text alias);@param;Text alias;Text alias;the alias for the key;true
org.apache.hadoop.security.Credentials;byte[] getSecretKey(Text alias);void addSecretKey(Text alias, byte[] key);@param;Text alias;Text alias;the alias for the key;true
org.apache.hadoop.security.Credentials;byte[] getSecretKey(Text alias);void removeSecretKey(Text alias);@param;Text alias;Text alias;the alias for the key;true
org.apache.hadoop.security.Credentials;void addSecretKey(Text alias, byte[] key);void removeSecretKey(Text alias);@param;Text alias;Text alias;the alias for the key;true
org.apache.hadoop.security.Credentials;Credentials readTokenStorageFile(Path filename, Configuration conf);Credentials readTokenStorageFile(File filename, Configuration conf);Whole;;;Convenience method for reading a token storage file, and loading the Tokens therein in the passed UGI  @param @param   @throws ;false
org.apache.hadoop.security.Credentials;void addAll(Credentials other);void mergeAll(Credentials other);@param;Credentials other;Credentials other;the credentials to copy;true
org.apache.hadoop.security.Groups;Groups getUserToGroupsMappingService();Groups getUserToGroupsMappingService(Configuration conf);Free text;;;Get the groups being used to map user-to-groups. ;true
org.apache.hadoop.security.Groups;Groups getUserToGroupsMappingService();Groups getUserToGroupsMappingService(Configuration conf);@return;;;the groups being used to map user-to-groups.;true
org.apache.hadoop.security.Groups;Groups getUserToGroupsMappingService();Groups getUserToGroupsMappingServiceWithLoadedConfiguration(Configuration conf);@return;;;the groups being used to map user-to-groups.;false
org.apache.hadoop.security.Groups;Groups getUserToGroupsMappingService(Configuration conf);Groups getUserToGroupsMappingServiceWithLoadedConfiguration(Configuration conf);@return;;;the groups being used to map user-to-groups.;false
org.apache.hadoop.security.SecurityInfo;KerberosInfo getKerberosInfo(Class protocol, Configuration conf);TokenInfo getTokenInfo(Class protocol, Configuration conf);@param;Class<?> protocol;Class<?> protocol;interface class;true
org.apache.hadoop.security.SaslInputStream; SaslInputStream(InputStream inStream, SaslServer saslServer); SaslInputStream(InputStream inStream, SaslClient saslClient);@param;InputStream inStream;InputStream inStream;the InputStream to be processed;true
org.apache.hadoop.security.SaslInputStream;int read();int read(byte[] b);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read();int read(byte[] b, int off, int len);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read();long skip(long n);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read();int available();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read();void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b);int read(byte[] b, int off, int len);@param;byte[] b;byte[] b;the buffer into which the data is read.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b);int read(byte[] b, int off, int len);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b);long skip(long n);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b);int available();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b);void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b, int off, int len);long skip(long n);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b, int off, int len);int available();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int read(byte[] b, int off, int len);void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;long skip(long n);int available();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;long skip(long n);void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslInputStream;int available();void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslPropertiesResolver;Map getServerProperties(InetAddress clientAddress);Map getClientProperties(InetAddress serverAddress);@return;;;the sasl properties to be used for the connection.;false
org.apache.hadoop.security.authorize.ProxyUsers;void refreshSuperUserGroupsConfiguration(Configuration conf, String proxyUserPrefix);void refreshSuperUserGroupsConfiguration(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.security.authorize.AccessControlList;void addUser(String user);void removeUser(String user);@param;String user;String user;The user name;true
org.apache.hadoop.security.authorize.AccessControlList;void addGroup(String group);void removeGroup(String group);@param;String group;String group;The group name;true
org.apache.hadoop.security.authorize.DefaultImpersonationProvider;String getProxySuperuserUserConfKey(String userName);String getProxySuperuserGroupConfKey(String userName);@param;String userName;String userName;name of the superuser;true
org.apache.hadoop.security.authorize.DefaultImpersonationProvider;String getProxySuperuserUserConfKey(String userName);String getProxySuperuserIpConfKey(String userName);@param;String userName;String userName;name of the superuser;true
org.apache.hadoop.security.authorize.DefaultImpersonationProvider;String getProxySuperuserGroupConfKey(String userName);String getProxySuperuserIpConfKey(String userName);@param;String userName;String userName;name of the superuser;true
org.apache.hadoop.security.SaslRpcClient;InputStream getInputStream(InputStream in);OutputStream getOutputStream(OutputStream out);@return;;;InputStream that may be using SASL unwrap;false
org.apache.hadoop.security.SaslRpcClient;InputStream getInputStream(InputStream in);OutputStream getOutputStream(OutputStream out);@param;InputStream in; nullParamName;- InputStream used to make the connection;false
org.apache.hadoop.security.SaslOutputStream; SaslOutputStream(OutputStream outStream, SaslServer saslServer); SaslOutputStream(OutputStream outStream, SaslClient saslClient);@param;OutputStream outStream;OutputStream outStream;the OutputStream to be processed;true
org.apache.hadoop.security.SaslOutputStream;void write(int b);void write(byte[] b);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(int b);void write(byte[] inBuf, int off, int len);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(int b);void flush();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(int b);void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(byte[] b);void write(byte[] inBuf, int off, int len);@param;byte[] b;byte[] inBuf;the data.;true
org.apache.hadoop.security.SaslOutputStream;void write(byte[] b);void write(byte[] inBuf, int off, int len);@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(byte[] b);void flush();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(byte[] b);void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(byte[] inBuf, int off, int len);void flush();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void write(byte[] inBuf, int off, int len);void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.security.SaslOutputStream;void flush();void close();@throws;;;if an I/O error occurs.;true
org.apache.hadoop.io.DefaultStringifier;void store(Configuration conf, K item, String keyName);K load(Configuration conf, String keyName, Class itemClass);@param;String keyName;String keyName;the name of the key to use;true
org.apache.hadoop.io.DefaultStringifier;void store(Configuration conf, K item, String keyName);K load(Configuration conf, String keyName, Class itemClass);@throws;;;: forwards Exceptions from the underlying Serialization classes.;true
org.apache.hadoop.io.DefaultStringifier;void store(Configuration conf, K item, String keyName);void storeArray(Configuration conf, K[] items, String keyName);@param;String keyName;String keyName;the name of the key to use;true
org.apache.hadoop.io.DefaultStringifier;void store(Configuration conf, K item, String keyName);void storeArray(Configuration conf, K[] items, String keyName);@throws;;;: forwards Exceptions from the underlying Serialization classes.;true
org.apache.hadoop.io.DefaultStringifier;void store(Configuration conf, K item, String keyName);K[] loadArray(Configuration conf, String keyName, Class itemClass);@param;String keyName;String keyName;the name of the key to use;true
org.apache.hadoop.io.DefaultStringifier;void store(Configuration conf, K item, String keyName);K[] loadArray(Configuration conf, String keyName, Class itemClass);@throws;;;: forwards Exceptions from the underlying Serialization classes.;true
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);void storeArray(Configuration conf, K[] items, String keyName);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);void storeArray(Configuration conf, K[] items, String keyName);@param;String keyName;String keyName;the name of the key to use;true
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);void storeArray(Configuration conf, K[] items, String keyName);@throws;;;: forwards Exceptions from the underlying Serialization classes.;true
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);K[] loadArray(Configuration conf, String keyName, Class itemClass);@return;;;restored object;false
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);K[] loadArray(Configuration conf, String keyName, Class itemClass);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);K[] loadArray(Configuration conf, String keyName, Class itemClass);@param;String keyName;String keyName;the name of the key to use;true
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);K[] loadArray(Configuration conf, String keyName, Class itemClass);@param;Class<K> itemClass;Class<K> itemClass;the class of the item;true
org.apache.hadoop.io.DefaultStringifier;K load(Configuration conf, String keyName, Class itemClass);K[] loadArray(Configuration conf, String keyName, Class itemClass);@throws;;;: forwards Exceptions from the underlying Serialization classes.;true
org.apache.hadoop.io.DefaultStringifier;void storeArray(Configuration conf, K[] items, String keyName);K[] loadArray(Configuration conf, String keyName, Class itemClass);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.io.DefaultStringifier;void storeArray(Configuration conf, K[] items, String keyName);K[] loadArray(Configuration conf, String keyName, Class itemClass);@param;String keyName;String keyName;the name of the key to use;true
org.apache.hadoop.io.DefaultStringifier;void storeArray(Configuration conf, K[] items, String keyName);K[] loadArray(Configuration conf, String keyName, Class itemClass);@throws;;;: forwards Exceptions from the underlying Serialization classes.;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, int buffSize);Free text;;;Copies from one stream to another. ;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, int buffSize);@param;InputStream in;InputStream in;InputStrem to read from;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, int buffSize);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, int buffSize);@param;int buffSize;int buffSize;the size of the buffer;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, Configuration conf);@param;InputStream in;InputStream in;InputStrem to read from;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, Configuration conf);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);Free text;;;Copies from one stream to another. ;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);@param;boolean close;boolean close;whether or not close the InputStream and OutputStream at the end. The streams are closed in the finally clause.;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close);void copyBytes(InputStream in, OutputStream out, long count, boolean close);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize);void copyBytes(InputStream in, OutputStream out, Configuration conf);@param;InputStream in;InputStream in;InputStrem to read from;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize);void copyBytes(InputStream in, OutputStream out, Configuration conf);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize);void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);Free text;;;Copies from one stream to another. ;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize);void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, int buffSize);void copyBytes(InputStream in, OutputStream out, long count, boolean close);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, Configuration conf);void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, Configuration conf);void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, Configuration conf);void copyBytes(InputStream in, OutputStream out, long count, boolean close);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);void copyBytes(InputStream in, OutputStream out, long count, boolean close);@param;InputStream in;InputStream in;InputStream to read from;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);void copyBytes(InputStream in, OutputStream out, long count, boolean close);@param;OutputStream out;OutputStream out;OutputStream to write to;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close);void readFully(InputStream in, byte[] buf, int off, int len);@param;InputStream in;InputStream in;InputStream to read from;true
org.apache.hadoop.io.IOUtils;void copyBytes(InputStream in, OutputStream out, long count, boolean close);void readFully(InputStream in, byte[] buf, int off, int len);@param;InputStream in;InputStream in;InputStream to read from;true
org.apache.hadoop.io.IOUtils;void writeFully(WritableByteChannel bc, ByteBuffer buf);void writeFully(FileChannel fc, ByteBuffer buf, long offset);@param;ByteBuffer buf;ByteBuffer buf;The input buffer;true
org.apache.hadoop.io.IOUtils;void writeFully(WritableByteChannel bc, ByteBuffer buf);void writeFully(FileChannel fc, ByteBuffer buf, long offset);@throws;;;On I/O error;true
org.apache.hadoop.io.compress.BlockCompressorStream; BlockCompressorStream(OutputStream out, Compressor compressor, int bufferSize, int compressionOverhead); BlockCompressorStream(OutputStream out, Compressor compressor);@param;OutputStream out;OutputStream out;stream;true
org.apache.hadoop.io.compress.BlockCompressorStream; BlockCompressorStream(OutputStream out, Compressor compressor, int bufferSize, int compressionOverhead); BlockCompressorStream(OutputStream out, Compressor compressor);@param;Compressor compressor;Compressor compressor;compressor to be used;true
org.apache.hadoop.io.compress.Lz4Codec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@return;;;a stream the user can write uncompressed data to have it compressed;true
org.apache.hadoop.io.compress.Lz4Codec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@param;OutputStream out;OutputStream out;the location for the final output stream;true
org.apache.hadoop.io.compress.Lz4Codec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@return;;;a stream to read uncompressed bytes from;true
org.apache.hadoop.io.compress.Lz4Codec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@param;InputStream in;InputStream in;the stream to read compressed bytes from;true
org.apache.hadoop.io.compress.BZip2Codec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@return;;;a stream the user can write uncompressed data to, to have it compressed;true
org.apache.hadoop.io.compress.BZip2Codec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@param;OutputStream out;OutputStream out;the location for the final output stream;true
org.apache.hadoop.io.compress.BZip2Codec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@return;;;a stream to read uncompressed bytes from;true
org.apache.hadoop.io.compress.BZip2Codec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@param;InputStream in;InputStream in;the stream to read compressed bytes from;true
org.apache.hadoop.io.compress.SnappyCodec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@return;;;a stream the user can write uncompressed data to have it compressed;true
org.apache.hadoop.io.compress.SnappyCodec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@param;OutputStream out;OutputStream out;the location for the final output stream;true
org.apache.hadoop.io.compress.SnappyCodec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@return;;;a stream to read uncompressed bytes from;true
org.apache.hadoop.io.compress.SnappyCodec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@param;InputStream in;InputStream in;the stream to read compressed bytes from;true
org.apache.hadoop.io.compress.CompressionCodec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@return;;;a stream the user can write uncompressed data to have it compressed;true
org.apache.hadoop.io.compress.CompressionCodec;CompressionOutputStream createOutputStream(OutputStream out);CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor);@param;OutputStream out;OutputStream out;the location for the final output stream;true
org.apache.hadoop.io.compress.CompressionCodec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@return;;;a stream to read uncompressed bytes from;true
org.apache.hadoop.io.compress.CompressionCodec;CompressionInputStream createInputStream(InputStream in);CompressionInputStream createInputStream(InputStream in, Decompressor decompressor);@param;InputStream in;InputStream in;the stream to read compressed bytes from;true
org.apache.hadoop.io.compress.Compressor;void setInput(byte[] b, int off, int len);void setDictionary(byte[] b, int off, int len);@param;int off;int off;Start offset;true
org.apache.hadoop.io.compress.Compressor;void setInput(byte[] b, int off, int len);void setDictionary(byte[] b, int off, int len);@param;int len;int len;Length;true
org.apache.hadoop.io.compress.lz4.Lz4Compressor; Lz4Compressor(int directBufferSize, boolean useLz4HC); Lz4Compressor(int directBufferSize);Free text;;;Creates a new compressor. ;true
org.apache.hadoop.io.compress.lz4.Lz4Compressor; Lz4Compressor(int directBufferSize, boolean useLz4HC); Lz4Compressor(int directBufferSize);@param;int directBufferSize;int directBufferSize;size of the direct buffer to be used.;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;boolean isNativeZlibLoaded(Configuration conf);Class getZlibCompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;boolean isNativeZlibLoaded(Configuration conf);Compressor getZlibCompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;boolean isNativeZlibLoaded(Configuration conf);Class getZlibDecompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;boolean isNativeZlibLoaded(Configuration conf);Decompressor getZlibDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;boolean isNativeZlibLoaded(Configuration conf);DirectDecompressor getZlibDirectDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Class getZlibCompressorType(Configuration conf);Compressor getZlibCompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Class getZlibCompressorType(Configuration conf);Class getZlibDecompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Class getZlibCompressorType(Configuration conf);Decompressor getZlibDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Class getZlibCompressorType(Configuration conf);DirectDecompressor getZlibDirectDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Compressor getZlibCompressor(Configuration conf);Class getZlibDecompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Compressor getZlibCompressor(Configuration conf);Decompressor getZlibDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Compressor getZlibCompressor(Configuration conf);DirectDecompressor getZlibDirectDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Class getZlibDecompressorType(Configuration conf);Decompressor getZlibDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Class getZlibDecompressorType(Configuration conf);DirectDecompressor getZlibDirectDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.zlib.ZlibFactory;Decompressor getZlibDecompressor(Configuration conf);DirectDecompressor getZlibDirectDecompressor(Configuration conf);@return;;;the appropriate implementation of the zlib decompressor.;false
org.apache.hadoop.io.compress.zlib.ZlibFactory;Decompressor getZlibDecompressor(Configuration conf);DirectDecompressor getZlibDirectDecompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.BlockDecompressorStream; BlockDecompressorStream(InputStream in, Decompressor decompressor, int bufferSize); BlockDecompressorStream(InputStream in, Decompressor decompressor);Free text;;;Create a {@link BlockDecompressorStream}. ;true
org.apache.hadoop.io.compress.BlockDecompressorStream; BlockDecompressorStream(InputStream in, Decompressor decompressor, int bufferSize); BlockDecompressorStream(InputStream in, Decompressor decompressor);@param;InputStream in;InputStream in;input stream;true
org.apache.hadoop.io.compress.BlockDecompressorStream; BlockDecompressorStream(InputStream in, Decompressor decompressor, int bufferSize); BlockDecompressorStream(InputStream in, Decompressor decompressor);@param;Decompressor decompressor;Decompressor decompressor;decompressor to use;true
org.apache.hadoop.io.compress.CompressionCodecFactory;CompressionCodec getCodec(Path file);CompressionCodec getCodecByClassName(String classname);@return;;;the codec object;false
org.apache.hadoop.io.compress.CompressionCodecFactory;CompressionCodec getCodec(Path file);CompressionCodec getCodecByName(String codecName);@return;;;the codec object;false
org.apache.hadoop.io.compress.CompressionCodecFactory;CompressionCodec getCodecByClassName(String classname);CompressionCodec getCodecByName(String codecName);@return;;;the codec object;false
org.apache.hadoop.io.compress.CompressionCodecFactory;CompressionCodec getCodecByClassName(String classname);CompressionCodec getCodecByName(String codecName);@param;String classname;String codecName;the canonical class name of the codec;false
org.apache.hadoop.io.compress.CompressionCodecFactory;CompressionCodec getCodecByClassName(String classname);Class getCodecClassByName(String codecName);@param;String classname;String codecName;the canonical class name of the codec;false
org.apache.hadoop.io.compress.CompressionCodecFactory;CompressionCodec getCodecByName(String codecName);Class getCodecClassByName(String codecName);@param;String codecName;String codecName;the canonical class name of the codec;true
org.apache.hadoop.io.compress.Decompressor;void setInput(byte[] b, int off, int len);void setDictionary(byte[] b, int off, int len);@param;int off;int off;Start offset;true
org.apache.hadoop.io.compress.Decompressor;void setInput(byte[] b, int off, int len);void setDictionary(byte[] b, int off, int len);@param;int len;int len;Length;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;boolean isNativeBzip2Loaded(Configuration conf);Class getBzip2CompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;boolean isNativeBzip2Loaded(Configuration conf);Compressor getBzip2Compressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;boolean isNativeBzip2Loaded(Configuration conf);Class getBzip2DecompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;boolean isNativeBzip2Loaded(Configuration conf);Decompressor getBzip2Decompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;Class getBzip2CompressorType(Configuration conf);Compressor getBzip2Compressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;Class getBzip2CompressorType(Configuration conf);Class getBzip2DecompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;Class getBzip2CompressorType(Configuration conf);Decompressor getBzip2Decompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;Compressor getBzip2Compressor(Configuration conf);Class getBzip2DecompressorType(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;Compressor getBzip2Compressor(Configuration conf);Decompressor getBzip2Decompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.Bzip2Factory;Class getBzip2DecompressorType(Configuration conf);Decompressor getBzip2Decompressor(Configuration conf);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream; CBZip2OutputStream(OutputStream out); CBZip2OutputStream(OutputStream out, int blockSize);@throws;;;if an I/O error occurs in the specified stream.;true
org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream; CBZip2OutputStream(OutputStream out); CBZip2OutputStream(OutputStream out, int blockSize);@throws;;;if out == null.;true
org.apache.hadoop.io.Text;void set(byte[] utf8, int start, int len);void append(byte[] utf8, int start, int len);@param;byte[] utf8;byte[] utf8;the data to copy from;true
org.apache.hadoop.io.retry.RetryUtils;RetryPolicy getDefaultRetryPolicy(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec, Class remoteExceptionToRetry);RetryPolicy getMultipleLinearRandomRetry(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec);@param;String retryPolicyEnabledKey;String retryPolicyEnabledKey;conf property key for enabling retry;true
org.apache.hadoop.io.retry.RetryUtils;RetryPolicy getDefaultRetryPolicy(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec, Class remoteExceptionToRetry);RetryPolicy getMultipleLinearRandomRetry(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec);@param;boolean defaultRetryPolicyEnabled;boolean defaultRetryPolicyEnabled;default retryPolicyEnabledKey conf value;true
org.apache.hadoop.io.retry.RetryUtils;RetryPolicy getDefaultRetryPolicy(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec, Class remoteExceptionToRetry);RetryPolicy getMultipleLinearRandomRetry(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec);@param;String retryPolicySpecKey;String retryPolicySpecKey;conf property key for retry policy spec;true
org.apache.hadoop.io.retry.RetryUtils;RetryPolicy getDefaultRetryPolicy(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec, Class remoteExceptionToRetry);RetryPolicy getMultipleLinearRandomRetry(Configuration conf, String retryPolicyEnabledKey, boolean defaultRetryPolicyEnabled, String retryPolicySpecKey, String defaultRetryPolicySpec);@param;String defaultRetryPolicySpec;String defaultRetryPolicySpec;default retryPolicySpecKey conf value;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, RetryPolicy retryPolicy);Object create(Class iface, FailoverProxyProvider proxyProvider, RetryPolicy retryPolicy);@return;;;the retry proxy;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, RetryPolicy retryPolicy);Object create(Class iface, FailoverProxyProvider proxyProvider, RetryPolicy retryPolicy);@param;Class<T> iface;Class<T> iface;the interface that the retry will implement;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, RetryPolicy retryPolicy);Object create(Class iface, T implementation, Map methodNameToPolicyMap);@return;;;the retry proxy;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, RetryPolicy retryPolicy);Object create(Class iface, T implementation, Map methodNameToPolicyMap);@param;Class<T> iface;Class<T> iface;the interface that the retry will implement;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, RetryPolicy retryPolicy);Object create(Class iface, T implementation, Map methodNameToPolicyMap);@param;T implementation;T implementation;the instance whose methods should be retried;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, RetryPolicy retryPolicy);Object create(Class iface, FailoverProxyProvider proxyProvider, Map methodNameToPolicyMap, RetryPolicy defaultPolicy);@return;;;the retry proxy;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, RetryPolicy retryPolicy);Object create(Class iface, FailoverProxyProvider proxyProvider, Map methodNameToPolicyMap, RetryPolicy defaultPolicy);@param;Class<T> iface;Class<T> iface;the interface that the retry will implement;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, FailoverProxyProvider proxyProvider, RetryPolicy retryPolicy);Object create(Class iface, T implementation, Map methodNameToPolicyMap);@return;;;the retry proxy;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, FailoverProxyProvider proxyProvider, RetryPolicy retryPolicy);Object create(Class iface, T implementation, Map methodNameToPolicyMap);@param;Class<T> iface;Class<T> iface;the interface that the retry will implement;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, FailoverProxyProvider proxyProvider, RetryPolicy retryPolicy);Object create(Class iface, FailoverProxyProvider proxyProvider, Map methodNameToPolicyMap, RetryPolicy defaultPolicy);@return;;;the retry proxy;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, FailoverProxyProvider proxyProvider, RetryPolicy retryPolicy);Object create(Class iface, FailoverProxyProvider proxyProvider, Map methodNameToPolicyMap, RetryPolicy defaultPolicy);@param;Class<T> iface;Class<T> iface;the interface that the retry will implement;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, FailoverProxyProvider proxyProvider, RetryPolicy retryPolicy);Object create(Class iface, FailoverProxyProvider proxyProvider, Map methodNameToPolicyMap, RetryPolicy defaultPolicy);@param;FailoverProxyProvider<T> proxyProvider;FailoverProxyProvider<T> proxyProvider;provides implementation instances whose methods should be retried;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, Map methodNameToPolicyMap);Object create(Class iface, FailoverProxyProvider proxyProvider, Map methodNameToPolicyMap, RetryPolicy defaultPolicy);@return;;;the retry proxy;true
org.apache.hadoop.io.retry.RetryProxy;Object create(Class iface, T implementation, Map methodNameToPolicyMap);Object create(Class iface, FailoverProxyProvider proxyProvider, Map methodNameToPolicyMap, RetryPolicy defaultPolicy);@param;Class<T> iface;Class<T> iface;the interface that the retry will implement;true
org.apache.hadoop.io.file.tfile.Utils;void writeVInt(DataOutput out, int n);void writeVLong(DataOutput out, long n);@param;DataOutput out;DataOutput out;output stream;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int upperBound(List list, T key, Comparator cmp);@return;;;The index to the desired element if it exists, or list.size() otherwise.;false
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int upperBound(List list, T key, Comparator cmp);@param;List<? extends T> list;List<? extends T> list;The list;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int upperBound(List list, T key, Comparator cmp);@param;T key;T key;The input key.;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int upperBound(List list, T key, Comparator cmp);@param;Comparator<? super T> cmp;Comparator<? super T> cmp;Comparator for the key.;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int lowerBound(List list, T key);Free text;;;Lower bound binary search. Find the index to the first element in the list that compares greater than or equal to key. ;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int lowerBound(List list, T key);@return;;;The index to the desired element if it exists, or list.size() otherwise.;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int lowerBound(List list, T key);@param;List<? extends T> list;List<? extends Comparable<? super T>> list;The list;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int lowerBound(List list, T key);@param;T key;T key;The input key.;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int upperBound(List list, T key);@return;;;The index to the desired element if it exists, or list.size() otherwise.;false
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int upperBound(List list, T key);@param;List<? extends T> list;List<? extends Comparable<? super T>> list;The list;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key, Comparator cmp);int upperBound(List list, T key);@param;T key;T key;The input key.;true
org.apache.hadoop.io.file.tfile.Utils;int upperBound(List list, T key, Comparator cmp);int lowerBound(List list, T key);@return;;;The index to the desired element if it exists, or list.size() otherwise.;false
org.apache.hadoop.io.file.tfile.Utils;int upperBound(List list, T key, Comparator cmp);int lowerBound(List list, T key);@param;List<? extends T> list;List<? extends Comparable<? super T>> list;The list;true
org.apache.hadoop.io.file.tfile.Utils;int upperBound(List list, T key, Comparator cmp);int lowerBound(List list, T key);@param;T key;T key;The input key.;true
org.apache.hadoop.io.file.tfile.Utils;int upperBound(List list, T key, Comparator cmp);int upperBound(List list, T key);Free text;;;Upper bound binary search. Find the index to the first element in the list that compares greater than the input key. ;true
org.apache.hadoop.io.file.tfile.Utils;int upperBound(List list, T key, Comparator cmp);int upperBound(List list, T key);@return;;;The index to the desired element if it exists, or list.size() otherwise.;true
org.apache.hadoop.io.file.tfile.Utils;int upperBound(List list, T key, Comparator cmp);int upperBound(List list, T key);@param;List<? extends T> list;List<? extends Comparable<? super T>> list;The list;true
org.apache.hadoop.io.file.tfile.Utils;int upperBound(List list, T key, Comparator cmp);int upperBound(List list, T key);@param;T key;T key;The input key.;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key);int upperBound(List list, T key);@return;;;The index to the desired element if it exists, or list.size() otherwise.;false
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key);int upperBound(List list, T key);@param;List<? extends Comparable<? super T>> list;List<? extends Comparable<? super T>> list;The list;true
org.apache.hadoop.io.file.tfile.Utils;int lowerBound(List list, T key);int upperBound(List list, T key);@param;T key;T key;The input key.;true
org.apache.hadoop.io.file.tfile.ByteArray; ByteArray(byte[] buffer); ByteArray(byte[] buffer, int offset, int len);@param;byte[] buffer;byte[] buffer;the byte array buffer.;true
org.apache.hadoop.io.nativeio.NativeIO;void renameTo(File src, File dst);void copyFileUnbuffered(File src, File dst);@param;File src;File src;The source path;true
org.apache.hadoop.io.nativeio.NativeIO;void renameTo(File src, File dst);void copyFileUnbuffered(File src, File dst);@param;File dst;File dst;The destination path;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Progressable progress;Progressable progress;The Progressable object to track progress.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Progressable progress;Progressable progress;The Progressable object to track progress.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Progressable progress;Progressable progress;The Progressable object to track progress.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Progressable progress;Progressable progress;The Progressable object to track progress.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Progressable progress;Progressable progress;The Progressable object to track progress.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;int bufferSize;int bufferSize;buffer size for the underlaying outputstream.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;short replication;short replication;replication factor for the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;long blockSize;long blockSize;block size for the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Progressable progress;Progressable progress;The Progressable object to track progress.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, CompressionType compressionType, CompressionCodec codec, Progressable progress, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;FileSystem fs;FileSystem fs;The configured filesystem.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, int bufferSize, short replication, long blockSize, boolean createParent, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Free text;;;Construct the preferred type of SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Path name;Path name;The name of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Metadata metadata;Metadata metadata;The metadata of the file.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileContext fc, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata, EnumSet createFlag, CreateOpts opts);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(FileSystem fs, Configuration conf, Path name, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Progressable progress);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);Free text;;;Construct the preferred type of 'raw' SequenceFile Writer. ;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@return;;;Returns the handle to the constructed SequenceFile Writer.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Configuration conf;Configuration conf;The configuration.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;FSDataOutputStream out;FSDataOutputStream out;The stream on top which the writer is to be constructed.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class keyClass;Class keyClass;The 'key' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;Class valClass;Class valClass;The 'value' type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionType compressionType;CompressionType compressionType;The compression type.;true
org.apache.hadoop.io.SequenceFile;Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec, Metadata metadata);Writer createWriter(Configuration conf, FSDataOutputStream out, Class keyClass, Class valClass, CompressionType compressionType, CompressionCodec codec);@param;CompressionCodec codec;CompressionCodec codec;The compression codec.;true
org.apache.hadoop.io.SecureIOUtils;RandomAccessFile openForRandomRead(File f, String mode, String expectedOwner, String expectedGroup);FSDataInputStream openFSDataInputStream(File file, String expectedOwner, String expectedGroup);@param;String expectedOwner;String expectedOwner;the expected user owner for the file;true
org.apache.hadoop.io.SecureIOUtils;RandomAccessFile openForRandomRead(File f, String mode, String expectedOwner, String expectedGroup);FSDataInputStream openFSDataInputStream(File file, String expectedOwner, String expectedGroup);@param;String expectedGroup;String expectedGroup;the expected group owner for the file;true
org.apache.hadoop.io.SecureIOUtils;RandomAccessFile openForRandomRead(File f, String mode, String expectedOwner, String expectedGroup);FileInputStream openForRead(File f, String expectedOwner, String expectedGroup);@param;String expectedOwner;String expectedOwner;the expected user owner for the file;true
org.apache.hadoop.io.SecureIOUtils;RandomAccessFile openForRandomRead(File f, String mode, String expectedOwner, String expectedGroup);FileInputStream openForRead(File f, String expectedOwner, String expectedGroup);@param;String expectedGroup;String expectedGroup;the expected group owner for the file;true
org.apache.hadoop.io.SecureIOUtils;FSDataInputStream openFSDataInputStream(File file, String expectedOwner, String expectedGroup);FileInputStream openForRead(File f, String expectedOwner, String expectedGroup);@param;String expectedOwner;String expectedOwner;the expected user owner for the file;true
org.apache.hadoop.io.SecureIOUtils;FSDataInputStream openFSDataInputStream(File file, String expectedOwner, String expectedGroup);FileInputStream openForRead(File f, String expectedOwner, String expectedGroup);@param;String expectedGroup;String expectedGroup;the expected group owner for the file;true
org.apache.hadoop.io.BytesWritable; BytesWritable(byte[] bytes); BytesWritable(byte[] bytes, int length);@param;byte[] bytes;byte[] bytes;This array becomes the backing storage for the object.;true
org.apache.hadoop.io.BoundedByteArrayOutputStream; BoundedByteArrayOutputStream(int capacity); BoundedByteArrayOutputStream(int capacity, int limit);@param;int capacity;int capacity;The capacity of the underlying byte array;true
org.apache.hadoop.io.ObjectWritable;void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf);Object readObject(DataInput in, Configuration conf);Whole;;;the preceding.    ;false
org.apache.hadoop.io.ObjectWritable;void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf);Object readObject(DataInput in, ObjectWritable objectWritable, Configuration conf);Whole;;;the preceding.    ;false
org.apache.hadoop.io.ObjectWritable;Object readObject(DataInput in, Configuration conf);Object readObject(DataInput in, ObjectWritable objectWritable, Configuration conf);Whole;;;the preceding.    ;false
org.apache.hadoop.io.WritableUtils;void writeVInt(DataOutput stream, int i);void writeVLong(DataOutput stream, long i);@param;DataOutput stream;DataOutput stream;Binary output stream;true
org.apache.hadoop.io.WritableUtils;long readVLong(DataInput stream);int readVInt(DataInput stream);@param;DataInput stream;DataInput stream;Binary input stream;true
org.apache.hadoop.io.WritableUtils;long readVLong(DataInput stream);int readVIntInRange(DataInput stream, int lower, int upper);@param;DataInput stream;DataInput stream;Binary input stream;true
org.apache.hadoop.io.WritableUtils;int readVInt(DataInput stream);int readVIntInRange(DataInput stream, int lower, int upper);@param;DataInput stream;DataInput stream;Binary input stream;true
org.apache.hadoop.http.FilterContainer;void addFilter(String name, String classname, Map parameters);void addGlobalFilter(String name, String classname, Map parameters);@param;Map<String,String> parameters;Map<String,String> parameters;a map from parameter names to initial values;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf);@param;String name;String name;The name of the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf);@param;int port;int port;The port to use on the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf);@param;boolean findPort;boolean findPort;whether the server should start at the given port and increment by 1 until it finds a free port.;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;String name;String name;The name of the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;String bindAddress;String bindAddress;The address for this server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;int port;int port;The port to use on the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;boolean findPort;boolean findPort;whether the server should start at the given port and increment by 1 until it finds a free port.;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;String name;String name;The name of the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;String bindAddress;String bindAddress;The address for this server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;int port;int port;The port to use on the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;boolean findPort;boolean findPort;whether the server should start at the given port and increment by 1 until it finds a free port.;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, String[] pathSpecs); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;String[] pathSpecs;String[] pathSpecs;Path specifications that this httpserver will be serving. These will be added to any filters.;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);Free text;;;Create a status server on the given port. The jsp scripts are taken from src/webapps/<name>. ;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;String name;String name;The name of the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;int port;int port;The port to use on the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;boolean findPort;boolean findPort;whether the server should start at the given port and increment by 1 until it finds a free port.;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);Free text;;;Create a status server on the given port. The jsp scripts are taken from src/webapps/<name>. ;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;String name;String name;The name of the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;int port;int port;The port to use on the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;boolean findPort;boolean findPort;whether the server should start at the given port and increment by 1 until it finds a free port.;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);Free text;;;Create a status server on the given port. The jsp scripts are taken from src/webapps/<name>. ;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;String name;String name;The name of the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;String bindAddress;String bindAddress;The address for this server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;int port;int port;The port to use on the server;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;boolean findPort;boolean findPort;whether the server should start at the given port and increment by 1 until it finds a free port.;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.http.HttpServer; HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector); HttpServer(String name, String bindAddress, int port, boolean findPort, Configuration conf, AccessControlList adminsAcl, Connector connector, String[] pathSpecs);@param;AccessControlList adminsAcl;AccessControlList adminsAcl;AccessControlList of the admins;true
org.apache.hadoop.http.HttpServer;void setAttribute(String name, Object value);Object getAttribute(String name);@param;String name;String name;The name of the attribute;true
org.apache.hadoop.http.HttpServer;void addJerseyResourcePackage(String packageName, String pathSpec);void addServlet(String name, String pathSpec, Class clazz);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer;void addJerseyResourcePackage(String packageName, String pathSpec);void addInternalServlet(String name, String pathSpec, Class clazz);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer;void addJerseyResourcePackage(String packageName, String pathSpec);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz);@param;String name;String name;The name of the servlet (can be passed as null);true
org.apache.hadoop.http.HttpServer;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz);@param;Class<? extends HttpServlet> clazz;Class<? extends HttpServlet> clazz;The servlet class;true
org.apache.hadoop.http.HttpServer;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String name;String name;The name of the servlet (can be passed as null);true
org.apache.hadoop.http.HttpServer;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;Class<? extends HttpServlet> clazz;Class<? extends HttpServlet> clazz;The servlet class;true
org.apache.hadoop.http.HttpServer;void addInternalServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String name;String name;The name of the servlet (can be passed as null);true
org.apache.hadoop.http.HttpServer;void addInternalServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer;void addInternalServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;Class<? extends HttpServlet> clazz;Class<? extends HttpServlet> clazz;The servlet class;true
org.apache.hadoop.http.HttpServer;void addSslListener(InetSocketAddress addr, String keystore, String storPass, String keyPass);void addSslListener(InetSocketAddress addr, Configuration sslConf, boolean needCertsAuth);Free text;;;Configure an ssl listener on the server. ;true
org.apache.hadoop.http.HttpServer;void addSslListener(InetSocketAddress addr, String keystore, String storPass, String keyPass);void addSslListener(InetSocketAddress addr, Configuration sslConf, boolean needCertsAuth);@param;InetSocketAddress addr;InetSocketAddress addr;address to listen on;true
org.apache.hadoop.http.HttpServer2;void setAttribute(String name, Object value);Object getAttribute(String name);@param;String name;String name;The name of the attribute;true
org.apache.hadoop.http.HttpServer2;void addJerseyResourcePackage(String packageName, String pathSpec);void addServlet(String name, String pathSpec, Class clazz);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer2;void addJerseyResourcePackage(String packageName, String pathSpec);void addInternalServlet(String name, String pathSpec, Class clazz);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer2;void addJerseyResourcePackage(String packageName, String pathSpec);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer2;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz);@param;String name;String name;The name of the servlet (can be passed as null);true
org.apache.hadoop.http.HttpServer2;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer2;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz);@param;Class<? extends HttpServlet> clazz;Class<? extends HttpServlet> clazz;The servlet class;true
org.apache.hadoop.http.HttpServer2;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String name;String name;The name of the servlet (can be passed as null);true
org.apache.hadoop.http.HttpServer2;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer2;void addServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;Class<? extends HttpServlet> clazz;Class<? extends HttpServlet> clazz;The servlet class;true
org.apache.hadoop.http.HttpServer2;void addInternalServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String name;String name;The name of the servlet (can be passed as null);true
org.apache.hadoop.http.HttpServer2;void addInternalServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;String pathSpec;String pathSpec;The path spec for the servlet;true
org.apache.hadoop.http.HttpServer2;void addInternalServlet(String name, String pathSpec, Class clazz);void addInternalServlet(String name, String pathSpec, Class clazz, boolean requireAuth);@param;Class<? extends HttpServlet> clazz;Class<? extends HttpServlet> clazz;The servlet class;true
org.apache.hadoop.http.HtmlQuoting;boolean needsQuoting(byte[] data, int off, int len);boolean needsQuoting(String str);Free text;;;Does the given string need to be quoted? ;true
org.apache.hadoop.http.HtmlQuoting;boolean needsQuoting(byte[] data, int off, int len);boolean needsQuoting(String str);@return;;;does the string contain any of the active html characters?;true
org.apache.hadoop.http.HtmlQuoting;boolean needsQuoting(byte[] data, int off, int len);boolean needsQuoting(String str);@param;byte[] data;String str;the string to check;true
org.apache.hadoop.ha.ZKFCProtocol;void cedeActive(int millisToCede);void gracefulFailover();@throws;;;if the operation is disallowed;true
org.apache.hadoop.ha.HAServiceProtocol;void monitorHealth();void transitionToActive(StateChangeRequestInfo reqInfo);@throws;;;if access is denied.;true
org.apache.hadoop.ha.HAServiceProtocol;void monitorHealth();void transitionToActive(StateChangeRequestInfo reqInfo);@throws;;;if other errors happen;true
org.apache.hadoop.ha.HAServiceProtocol;void monitorHealth();void transitionToStandby(StateChangeRequestInfo reqInfo);@throws;;;if access is denied.;true
org.apache.hadoop.ha.HAServiceProtocol;void monitorHealth();void transitionToStandby(StateChangeRequestInfo reqInfo);@throws;;;if other errors happen;true
org.apache.hadoop.ha.HAServiceProtocol;void monitorHealth();HAServiceStatus getServiceStatus();@throws;;;if access is denied.;true
org.apache.hadoop.ha.HAServiceProtocol;void monitorHealth();HAServiceStatus getServiceStatus();@throws;;;if other errors happen;true
org.apache.hadoop.ha.HAServiceProtocol;void transitionToActive(StateChangeRequestInfo reqInfo);void transitionToStandby(StateChangeRequestInfo reqInfo);@throws;;;if access is denied.;true
org.apache.hadoop.ha.HAServiceProtocol;void transitionToActive(StateChangeRequestInfo reqInfo);void transitionToStandby(StateChangeRequestInfo reqInfo);@throws;;;if other errors happen;true
org.apache.hadoop.ha.HAServiceProtocol;void transitionToActive(StateChangeRequestInfo reqInfo);HAServiceStatus getServiceStatus();@throws;;;if access is denied.;true
org.apache.hadoop.ha.HAServiceProtocol;void transitionToActive(StateChangeRequestInfo reqInfo);HAServiceStatus getServiceStatus();@throws;;;if other errors happen;true
org.apache.hadoop.ha.HAServiceProtocol;void transitionToStandby(StateChangeRequestInfo reqInfo);HAServiceStatus getServiceStatus();@throws;;;if access is denied.;true
org.apache.hadoop.ha.HAServiceProtocol;void transitionToStandby(StateChangeRequestInfo reqInfo);HAServiceStatus getServiceStatus();@throws;;;if other errors happen;true
org.apache.hadoop.log.Log4Json;String toJson(LoggingEvent event);Writer toJson(Writer writer, LoggingEvent event);Free text;;;Convert an event to JSON ;true
org.apache.hadoop.log.Log4Json;String toJson(LoggingEvent event);Writer toJson(Writer writer, LoggingEvent event);@param;LoggingEvent event;LoggingEvent event;the event -must not be null;true
org.apache.hadoop.log.Log4Json;String toJson(LoggingEvent event);Writer toJson(Writer writer, LoggingEvent event);@throws;;;on problems generating the JSON;true
org.apache.hadoop.log.Log4Json;Writer toJson(Writer writer, LoggingEvent event);Writer toJson(Writer writer, String loggerName, long timeStamp, String level, String threadName, String message, ThrowableInformation ti);@return;;;the writer;true
org.apache.hadoop.ipc.Client;void setPingInterval(Configuration conf, int pingInterval);int getPingInterval(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.Client;void setPingInterval(Configuration conf, int pingInterval);int getTimeout(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.Client;void setPingInterval(Configuration conf, int pingInterval);void setConnectTimeout(Configuration conf, int timeout);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.Client;int getPingInterval(Configuration conf);int getTimeout(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.Client;int getPingInterval(Configuration conf);void setConnectTimeout(Configuration conf, int timeout);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.Client;int getTimeout(Configuration conf);void setConnectTimeout(Configuration conf, int timeout);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);Free text;;;Make a call, passing <code>rpcRequest</code>, to the IPC server defined by <code>remoteId</code>, returning the rpc respond. ;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);@param;Writable rpcRequest;Writable rpcRequest;- contains serialized method and method parameters;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);@param;ConnectionId remoteId;ConnectionId remoteId;- the target rpc server;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);@param;Writable rpcRequest;Writable rpcRequest;- contains serialized method and method parameters;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);@param;ConnectionId remoteId;ConnectionId remoteId;- the target rpc server;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;Writable rpcRequest;Writable rpcRequest;- contains serialized method and method parameters;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;ConnectionId remoteId;ConnectionId remoteId;- the target rpc server;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);@param;Writable rpcRequest;Writable rpcRequest;- contains serialized method and method parameters;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);@param;ConnectionId remoteId;ConnectionId remoteId;- the target rpc server;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;Writable rpcRequest;Writable rpcRequest;- contains serialized method and method parameters;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;ConnectionId remoteId;ConnectionId remoteId;- the target rpc server;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;AtomicBoolean fallbackToSimpleAuth;AtomicBoolean fallbackToSimpleAuth;- set to true or false during this method to indicate if a secure client falls back to simple auth;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);Free text;;;Make a call, passing <code>rpcRequest</code>, to the IPC server defined by <code>remoteId</code>, returning the rpc response. ;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;Writable rpcRequest;Writable rpcRequest;- contains serialized method and method parameters;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;ConnectionId remoteId;ConnectionId remoteId;- the target rpc server;true
org.apache.hadoop.ipc.Client;Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass);Writable call(RPC.RpcKind rpcKind, Writable rpcRequest, ConnectionId remoteId, int serviceClass, AtomicBoolean fallbackToSimpleAuth);@param;int serviceClass;int serviceClass;- service class for RPC;true
org.apache.hadoop.ipc.ClientCache;Client getClient(Configuration conf, SocketFactory factory, Class valueClass);Client getClient(Configuration conf);@return;;;an IPC client;true
org.apache.hadoop.ipc.ClientCache;Client getClient(Configuration conf, SocketFactory factory, Class valueClass);Client getClient(Configuration conf);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.ClientCache;Client getClient(Configuration conf, SocketFactory factory, Class valueClass);Client getClient(Configuration conf, SocketFactory factory);@return;;;an IPC client;true
org.apache.hadoop.ipc.ClientCache;Client getClient(Configuration conf, SocketFactory factory, Class valueClass);Client getClient(Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.ClientCache;Client getClient(Configuration conf, SocketFactory factory, Class valueClass);Client getClient(Configuration conf, SocketFactory factory);@param;SocketFactory factory;SocketFactory factory;SocketFactory for client socket;true
org.apache.hadoop.ipc.ClientCache;Client getClient(Configuration conf);Client getClient(Configuration conf, SocketFactory factory);@return;;;an IPC client;true
org.apache.hadoop.ipc.ClientCache;Client getClient(Configuration conf);Client getClient(Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;Configuration;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;void setProtocolEngine(Configuration conf, Class protocol, Class engine);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);Free text;;;Get a proxy connection to a remote server ;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@return;;;the proxy;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);Free text;;;Get a proxy connection to a remote server ;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@return;;;the proxy;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@return;;;the protocol proxy;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@return;;;the protocol proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@return;;;the protocol proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@param;long connTimeout;long connTimeout;time in milliseconds before giving up;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);Free text;;;Get a proxy connection to a remote server ;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@return;;;the proxy;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;long connTimeout;long timeout;time in milliseconds before giving up;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long connTimeout;long timeout;time in milliseconds before giving up;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@param;long connTimeout;long timeout;time in milliseconds before giving up;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long connTimeout;long timeout;time in milliseconds before giving up;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@return;;;the protocol proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@return;;;the protocol proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, long connTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;int rpcTimeout;int rpcTimeout;timeout for each RPC;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@param;long timeout;long timeout;time in milliseconds before giving up;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T waitForProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;ProtocolProxy waitForProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, int rpcTimeout, RetryPolicy connectionRetryPolicy, long timeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;false
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);Whole;;;talking to a server at the named address.    ;false
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@return;;;the protocol proxy;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Class<T> protocol;Class<T> protocol;protocol class;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;long clientVersion;long clientVersion;client version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;InetSocketAddress addr;InetSocketAddress addr;remote address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;Configuration conf;Configuration conf;configuration to use;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);@throws;;;if the far end through a RemoteException;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;Class<T> protocol;Class<T> protocol;protocol;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;long clientVersion;long clientVersion;client's version;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;InetSocketAddress addr;InetSocketAddress addr;server address;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;UserGroupInformation ticket;UserGroupInformation ticket;security ticket;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@param;int rpcTimeout;int rpcTimeout;max time for each rpc, 0 means no timeout;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);@throws;;;if any error occurs;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@return;;;the proxy;false
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> protocol;Class<T> protocol;protocol;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;long clientVersion;long clientVersion;client's version;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;InetSocketAddress addr;InetSocketAddress addr;server address;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;UserGroupInformation ticket;UserGroupInformation ticket;security ticket;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;int rpcTimeout;int rpcTimeout;max time for each rpc, 0 means no timeout;true
org.apache.hadoop.ipc.RPC;T getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@throws;;;if any error occurs;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@return;;;the proxy;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;Class<T> protocol;Class<T> protocol;protocol;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;long clientVersion;long clientVersion;client's version;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;InetSocketAddress addr;InetSocketAddress addr;server address;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;UserGroupInformation ticket;UserGroupInformation ticket;security ticket;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;Configuration conf;Configuration conf;configuration;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;SocketFactory factory;SocketFactory factory;socket factory;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;int rpcTimeout;int rpcTimeout;max time for each rpc, 0 means no timeout;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@param;RetryPolicy connectionRetryPolicy;RetryPolicy connectionRetryPolicy;retry policy;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);@throws;;;if any error occurs;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.RPC;ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);ProtocolProxy getProtocolProxy(Class protocol, long clientVersion, InetSocketAddress addr, Configuration conf);Free text;;;Get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server ;true
org.apache.hadoop.ipc.ProtocolSignature;int getFingerprint(Method[] methods);int getFingerprint(int[] hashcodes);@return;;;the hash code;true
org.apache.hadoop.ipc.ProtocolSignature;int getFingerprint(Method[] methods);int getFingerprint(int[] hashcodes);@param;Method[] methods; nullParamName;an array of methods;true
org.apache.hadoop.ipc.ProtocolSignature;ProtocolSignature getProtocolSignature(int clientMethodsHashCode, long serverVersion, Class protocol);ProtocolSignature getProtocolSignature(VersionedProtocol server, String protocol, long clientVersion, int clientMethodsHash);Free text;;;Get a server protocol's signature ;true
org.apache.hadoop.ipc.VersionedProtocol;long getProtocolVersion(String protocol, long clientVersion);ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash);Free text;;;Return protocol version corresponding to protocol interface. ;false
org.apache.hadoop.ipc.VersionedProtocol;long getProtocolVersion(String protocol, long clientVersion);ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash);@param;String protocol;String protocol;The classname of the protocol interface;true
org.apache.hadoop.ipc.VersionedProtocol;long getProtocolVersion(String protocol, long clientVersion);ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash);@param;long clientVersion;long clientVersion;The version of the protocol that the client speaks;true
org.apache.hadoop.ipc.WritableRpcEngine;ProtocolProxy getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy);ProtocolProxy getProxy(Class protocol, long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, Configuration conf, SocketFactory factory, int rpcTimeout, RetryPolicy connectionRetryPolicy, AtomicBoolean fallbackToSimpleAuth);Whole;;;talking to a server at the named address.    ;false
org.apache.hadoop.service.ServiceStateException;RuntimeException convert(Throwable fault);RuntimeException convert(String text, Throwable fault);Free text;;;Convert any exception into a {@link RuntimeException}. If the caught exception is already of that type, it is typecast to a {@link RuntimeException} and returned. All other exception types are wrapped in a new instance of ServiceStateException ;true
org.apache.hadoop.service.ServiceStateException;RuntimeException convert(Throwable fault);RuntimeException convert(String text, Throwable fault);@return;;;a ServiceStateException to rethrow;true
org.apache.hadoop.service.ServiceStateException;RuntimeException convert(Throwable fault);RuntimeException convert(String text, Throwable fault);@param;Throwable fault;Throwable fault;exception or throwable;true
org.apache.hadoop.service.ServiceOperations;Exception stopQuietly(Service service);Exception stopQuietly(Log log, Service service);Free text;;;Stop a service, if it is null do nothing. Exceptions are caught and logged at warn level. (but not Throwables). This operation is intended to be used in cleanup operations ;true
org.apache.hadoop.service.ServiceOperations;Exception stopQuietly(Service service);Exception stopQuietly(Log log, Service service);@return;;;any exception that was caught, null if none was.;true
org.apache.hadoop.service.ServiceOperations;Exception stopQuietly(Service service);Exception stopQuietly(Log log, Service service);@param;Service service;Service service;a service, may be null;true
org.apache.hadoop.service.Service;void stop();void close();@throws;;;on any failure during the stop operation;true
org.apache.hadoop.service.ServiceStateModel;Service.STATE getState();boolean isInState(Service.STATE proposed);@return;;;the state;false
org.apache.hadoop.service.ServiceStateModel;boolean isInState(Service.STATE proposed);Service.STATE enterState(Service.STATE proposed);@param;Service.STATE proposed;Service.STATE proposed;proposed new state;true
org.apache.hadoop.service.ServiceStateModel;boolean isInState(Service.STATE proposed);void checkStateTransition(String name, Service.STATE state, Service.STATE proposed);@param;Service.STATE proposed;Service.STATE proposed;proposed new state;true
org.apache.hadoop.service.ServiceStateModel;boolean isInState(Service.STATE proposed);boolean isValidStateTransition(Service.STATE current, Service.STATE proposed);@param;Service.STATE proposed;Service.STATE proposed;proposed new state;true
org.apache.hadoop.service.ServiceStateModel;Service.STATE enterState(Service.STATE proposed);void checkStateTransition(String name, Service.STATE state, Service.STATE proposed);@param;Service.STATE proposed;Service.STATE proposed;proposed new state;true
org.apache.hadoop.service.ServiceStateModel;Service.STATE enterState(Service.STATE proposed);boolean isValidStateTransition(Service.STATE current, Service.STATE proposed);@param;Service.STATE proposed;Service.STATE proposed;proposed new state;true
org.apache.hadoop.service.ServiceStateModel;void checkStateTransition(String name, Service.STATE state, Service.STATE proposed);boolean isValidStateTransition(Service.STATE current, Service.STATE proposed);@param;Service.STATE state;Service.STATE current;current state;false
org.apache.hadoop.service.ServiceStateModel;void checkStateTransition(String name, Service.STATE state, Service.STATE proposed);boolean isValidStateTransition(Service.STATE current, Service.STATE proposed);@param;Service.STATE proposed;Service.STATE proposed;proposed new state;true
org.apache.hadoop.fs.ContentSummary;String getHeader(boolean qOption);String toString(boolean qOption);Free text;;;if qOption is false, output directory count, file count, and content size, if qOption is true, output quota and remaining quota as well. ;false
org.apache.hadoop.fs.ContentSummary;String getHeader(boolean qOption);String toString(boolean qOption);@param;boolean qOption;boolean qOption;a flag indicating if quota needs to be printed or not;true
org.apache.hadoop.fs.ContentSummary;String getHeader(boolean qOption);String toString(boolean qOption, boolean hOption);@param;boolean qOption;boolean qOption;a flag indicating if quota needs to be printed or not;true
org.apache.hadoop.fs.ContentSummary;String toString(boolean qOption);String toString(boolean qOption, boolean hOption);@return;;;the string representation of the object;true
org.apache.hadoop.fs.ContentSummary;String toString(boolean qOption);String toString(boolean qOption, boolean hOption);@param;boolean qOption;boolean qOption;a flag indicating if quota needs to be printed or not;true
org.apache.hadoop.fs.FSInputChecker; FSInputChecker(Path file, int numOfRetries); FSInputChecker(Path file, int numOfRetries, boolean verifyChecksum, Checksum sum, int chunkSize, int checksumSize);@param;Path file;Path file;The name of the file to be read;true
org.apache.hadoop.fs.FSInputChecker; FSInputChecker(Path file, int numOfRetries); FSInputChecker(Path file, int numOfRetries, boolean verifyChecksum, Checksum sum, int chunkSize, int checksumSize);@param;int numOfRetries;int numOfRetries;Number of read retries when ChecksumError occurs;true
org.apache.hadoop.fs.FSInputChecker; FSInputChecker(Path file, int numOfRetries, boolean verifyChecksum, Checksum sum, int chunkSize, int checksumSize);void set(boolean verifyChecksum, Checksum sum, int maxChunkSize, int checksumSize);@param;int chunkSize;int maxChunkSize;maximun chunk size;false
org.apache.hadoop.fs.FileStatus;void setOwner(String owner);void setGroup(String group);@param;String owner;String group;if it is null, default value is set;false
org.apache.hadoop.fs.FileStatus;int compareTo(Object o);boolean equals(Object o);@param;Object o;Object o;the object to be compared.;true
org.apache.hadoop.fs.PathIOException; PathIOException(String path); PathIOException(String path, Throwable cause);@param;String path;String path;for the exception;true
org.apache.hadoop.fs.PathIOException; PathIOException(String path); PathIOException(String path, String error);@param;String path;String path;for the exception;true
org.apache.hadoop.fs.PathIOException; PathIOException(String path, Throwable cause); PathIOException(String path, String error);@param;String path;String path;for the exception;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, long interval, long initialUsed);Free text;;;Keeps track of disk usage. ;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, long interval, long initialUsed);@param;File path;File path;the path to check disk usage in;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, long interval, long initialUsed);@param;long interval;long interval;refresh the disk usage at this interval;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, long interval, long initialUsed);@throws;;;if we fail to refresh the disk usage;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, Configuration conf);Free text;;;Keeps track of disk usage. ;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, Configuration conf);@param;File path;File path;the path to check disk usage in;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, Configuration conf);@throws;;;if we fail to refresh the disk usage;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, Configuration conf, long initialUsed);Free text;;;Keeps track of disk usage. ;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, Configuration conf, long initialUsed);@param;File path;File path;the path to check disk usage in;true
org.apache.hadoop.fs.DU; DU(File path, long interval); DU(File path, Configuration conf, long initialUsed);@throws;;;if we fail to refresh the disk usage;true
org.apache.hadoop.fs.DU; DU(File path, long interval, long initialUsed); DU(File path, Configuration conf);Free text;;;Keeps track of disk usage. ;true
org.apache.hadoop.fs.DU; DU(File path, long interval, long initialUsed); DU(File path, Configuration conf);@param;File path;File path;the path to check disk usage in;true
org.apache.hadoop.fs.DU; DU(File path, long interval, long initialUsed); DU(File path, Configuration conf);@throws;;;if we fail to refresh the disk usage;true
org.apache.hadoop.fs.DU; DU(File path, long interval, long initialUsed); DU(File path, Configuration conf, long initialUsed);Free text;;;Keeps track of disk usage. ;true
org.apache.hadoop.fs.DU; DU(File path, long interval, long initialUsed); DU(File path, Configuration conf, long initialUsed);@param;File path;File path;the path to check disk usage in;true
org.apache.hadoop.fs.DU; DU(File path, long interval, long initialUsed); DU(File path, Configuration conf, long initialUsed);@throws;;;if we fail to refresh the disk usage;true
org.apache.hadoop.fs.DU; DU(File path, Configuration conf); DU(File path, Configuration conf, long initialUsed);Free text;;;Keeps track of disk usage. ;true
org.apache.hadoop.fs.DU; DU(File path, Configuration conf); DU(File path, Configuration conf, long initialUsed);@param;File path;File path;the path to check disk usage in;true
org.apache.hadoop.fs.DU; DU(File path, Configuration conf); DU(File path, Configuration conf, long initialUsed);@param;Configuration conf;Configuration conf;configuration object;true
org.apache.hadoop.fs.DU; DU(File path, Configuration conf); DU(File path, Configuration conf, long initialUsed);@throws;;;if we fail to refresh the disk usage;true
org.apache.hadoop.fs.FileSystem;FileSystem get(URI uri, Configuration conf, String user);FileSystem get(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;FileSystem get(URI uri, Configuration conf, String user);URI getDefaultUri(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;FileSystem get(URI uri, Configuration conf, String user);FileSystem newInstance(URI uri, Configuration conf, String user);@param;URI uri;URI uri;of the filesystem;true
org.apache.hadoop.fs.FileSystem;FileSystem get(URI uri, Configuration conf, String user);FileSystem newInstance(URI uri, Configuration conf, String user);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;FileSystem get(URI uri, Configuration conf, String user);FileSystem newInstance(URI uri, Configuration conf, String user);@param;String user;String user;to perform the get as;true
org.apache.hadoop.fs.FileSystem;FileSystem get(URI uri, Configuration conf, String user);FileSystem newInstance(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;FileSystem get(Configuration conf);URI getDefaultUri(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;FileSystem get(Configuration conf);FileSystem newInstance(URI uri, Configuration conf, String user);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;FileSystem get(Configuration conf);FileSystem newInstance(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;URI getDefaultUri(Configuration conf);FileSystem newInstance(URI uri, Configuration conf, String user);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;URI getDefaultUri(Configuration conf);FileSystem newInstance(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;void setDefaultUri(Configuration conf, URI uri);void setDefaultUri(Configuration conf, String uri);Whole;;; @param the configuration to alter@param the new default filesystem uri  ;false
org.apache.hadoop.fs.FileSystem;LocalFileSystem getLocal(Configuration conf);LocalFileSystem newInstanceLocal(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to configure the file system with;true
org.apache.hadoop.fs.FileSystem;FileSystem newInstance(URI uri, Configuration conf, String user);FileSystem newInstance(Configuration conf);@param;Configuration conf;Configuration conf;the configuration to use;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(FileSystem fs, Path file, FsPermission permission);boolean mkdirs(FileSystem fs, Path dir, FsPermission permission);@param;FileSystem fs;FileSystem fs;file system handle;true
org.apache.hadoop.fs.FileSystem;BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len);BlockLocation[] getFileBlockLocations(Path p, long start, long len);Free text;;;Return an array containing hostnames, offset and size of portions of the given file.  For a nonexistent file or regions, null will be returned. This call is most helpful with DFS, where it returns hostnames of machines that contain the given file. The FileSystem will simply return an elt containing 'localhost'. ;true
org.apache.hadoop.fs.FileSystem;BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len);BlockLocation[] getFileBlockLocations(Path p, long start, long len);@param;long start;long start;offset into the given file;true
org.apache.hadoop.fs.FileSystem;BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len);BlockLocation[] getFileBlockLocations(Path p, long start, long len);@param;long len;long len;length for which to get locations for;true
org.apache.hadoop.fs.FileSystem;BlockLocation[] getFileBlockLocations(Path p, long start, long len);FsServerDefaults getServerDefaults(Path p);@param;Path p;Path p;path is used to identify an FS since an FS could have another FS that it could be delegating the call to;true
org.apache.hadoop.fs.FileSystem;FsServerDefaults getServerDefaults();FsServerDefaults getServerDefaults(Path p);Free text;;;Return a set of server default configuration values ;true
org.apache.hadoop.fs.FileSystem;FsServerDefaults getServerDefaults();FsServerDefaults getServerDefaults(Path p);@return;;;server default configuration values;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataInputStream open(Path f);Free text;;;Opens an FSDataInputStream at the indicated Path. ;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataInputStream open(Path f, int bufferSize);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f);FSDataOutputStream create(Path f, boolean overwrite);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f);FSDataOutputStream create(Path f, Progressable progress);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f);FSDataOutputStream create(Path f, short replication);Free text;;;Create an FSDataOutputStream at the indicated Path. Files are overwritten by default. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f);FSDataOutputStream create(Path f, short replication);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f);FSDataOutputStream create(Path f, short replication, Progressable progress);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite);FSDataOutputStream create(Path f, Progressable progress);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite);FSDataOutputStream create(Path f, short replication);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite);FSDataOutputStream create(Path f, short replication, Progressable progress);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);Free text;;;Create an FSDataOutputStream at the indicated Path. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);Free text;;;Create an FSDataOutputStream at the indicated Path. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, Progressable progress);FSDataOutputStream create(Path f, short replication);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, Progressable progress);FSDataOutputStream create(Path f, short replication, Progressable progress);Free text;;;Create an FSDataOutputStream at the indicated Path with write-progress reporting. Files are overwritten by default. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, Progressable progress);FSDataOutputStream create(Path f, short replication, Progressable progress);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, Progressable progress);FSDataOutputStream create(Path f, short replication, Progressable progress);@param;Progressable progress;Progressable progress;to report progress;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, short replication);FSDataOutputStream create(Path f, short replication, Progressable progress);@param;Path f;Path f;the file to create;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, short replication);FSDataOutputStream create(Path f, short replication, Progressable progress);@param;short replication;short replication;the replication factor;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);Free text;;;Create an FSDataOutputStream at the indicated Path. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Create an FSDataOutputStream at the indicated Path with write-progress reporting. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Create an FSDataOutputStream at the indicated Path with write-progress reporting. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Create an FSDataOutputStream at the indicated Path with write-progress reporting. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Create an FSDataOutputStream at the indicated Path with write-progress reporting. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Create an FSDataOutputStream at the indicated Path with write-progress reporting. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Create an FSDataOutputStream at the indicated Path with write-progress reporting. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;EnumSet<CreateFlag> flags;EnumSet<CreateFlag> flags;CreateFlags to use for this stream.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;EnumSet<CreateFlag> flags;EnumSet<CreateFlag> flags;CreateFlags to use for this stream.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;EnumSet<CreateFlag> flags;EnumSet<CreateFlag> flags;CreateFlags to use for this stream.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream create(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress, ChecksumOpt checksumOpt);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;boolean primitiveMkdir(Path f, FsPermission absolutePermission);void primitiveMkdir(Path f, FsPermission absolutePermission, boolean createParent);Whole;;;This version of the mkdirs method assumes that the permission is absolute. It has been added to support the FileContext that processes the permission with umask before calling this method. This a temporary method added to support the transition from FileSystem to FileContext for user applications.    ;false
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Opens an FSDataOutputStream at the indicated Path with write-progress reporting. Same as create(), except fails if parent directory doesn't already exist. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;boolean overwrite;boolean overwrite;if a file with this name already exists, then if true, the file will be overwritten, and if false an error will be thrown.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Opens an FSDataOutputStream at the indicated Path with write-progress reporting. Same as create(), except fails if parent directory doesn't already exist. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);Free text;;;Opens an FSDataOutputStream at the indicated Path with write-progress reporting. Same as create(), except fails if parent directory doesn't already exist. ;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);@param;short replication;short replication;required block replication for the file.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet flags, int bufferSize, short replication, long blockSize, Progressable progress);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream append(Path f);FSDataOutputStream append(Path f, int bufferSize);@param;Path f;Path f;the existing file to be appended.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream append(Path f);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;Path f;Path f;the existing file to be appended.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream append(Path f, int bufferSize);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;Path f;Path f;the existing file to be appended.;true
org.apache.hadoop.fs.FileSystem;FSDataOutputStream append(Path f, int bufferSize);FSDataOutputStream append(Path f, int bufferSize, Progressable progress);@param;int bufferSize;int bufferSize;the size of the buffer to be used.;true
org.apache.hadoop.fs.FileSystem;short getReplication(Path src);boolean setReplication(Path src, short replication);@param;Path src;Path src;file name;true
org.apache.hadoop.fs.FileSystem;boolean rename(Path src, Path dst);void rename(Path src, Path dst, Rename options);@param;Path src;Path src;path to be renamed;true
org.apache.hadoop.fs.FileSystem;boolean rename(Path src, Path dst);void rename(Path src, Path dst, Rename options);@param;Path dst;Path dst;new path after rename;true
org.apache.hadoop.fs.FileSystem;boolean rename(Path src, Path dst);void rename(Path src, Path dst, Rename options);@throws;;;on failure;true
org.apache.hadoop.fs.FileSystem;boolean delete(Path f, boolean recursive);boolean deleteOnExit(Path f);@param;Path f;Path f;the path to delete.;true
org.apache.hadoop.fs.FileSystem;boolean isDirectory(Path f);boolean isFile(Path f);Whole;;;Note: Avoid using this method. Instead reuse the FileStatus returned by getFileStatus() or listStatus() methods.  @param path to check  ;false
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f);FileStatus[] listStatus(Path f, PathFilter filter);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f);FileStatus[] listStatus(Path[] files);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f);FileStatus[] listStatus(Path[] files, PathFilter filter);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f);RemoteIterator listFiles(Path f, boolean recursive);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f);FileStatus getFileStatus(Path f);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f, PathFilter filter);FileStatus[] listStatus(Path[] files);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f, PathFilter filter);FileStatus[] listStatus(Path[] files, PathFilter filter);@param;PathFilter filter;PathFilter filter;the user-supplied path filter;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f, PathFilter filter);FileStatus[] listStatus(Path[] files, PathFilter filter);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f, PathFilter filter);RemoteIterator listFiles(Path f, boolean recursive);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path f, PathFilter filter);FileStatus getFileStatus(Path f);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path[] files);FileStatus[] listStatus(Path[] files, PathFilter filter);@param;Path[] files;Path[] files;a list of paths;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path[] files);FileStatus[] listStatus(Path[] files, PathFilter filter);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path[] files);RemoteIterator listFiles(Path f, boolean recursive);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path[] files);FileStatus getFileStatus(Path f);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path[] files, PathFilter filter);RemoteIterator listFiles(Path f, boolean recursive);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileStatus[] listStatus(Path[] files, PathFilter filter);FileStatus getFileStatus(Path f);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;RemoteIterator listLocatedStatus(Path f);RemoteIterator listLocatedStatus(Path f, PathFilter filter);@return;;;an iterator that traverses statuses of the files/directories in the given path;true
org.apache.hadoop.fs.FileSystem;RemoteIterator listLocatedStatus(Path f);RemoteIterator listFiles(Path f, boolean recursive);@param;Path f;Path f;is the path;true
org.apache.hadoop.fs.FileSystem;RemoteIterator listFiles(Path f, boolean recursive);FileStatus getFileStatus(Path f);@throws;;;when the path does not exist, IOException see specific implementation;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path[] srcs, Path dst);@param;Path src;Path[] srcs;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path[] srcs, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path[] srcs, Path dst);@param;Path dst;Path[] srcs;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path[] srcs, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path[] srcs;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path[] srcs;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveFromLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path[] srcs;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path[] srcs;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path[] srcs;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path[] srcs;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path[] srcs;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path[] srcs;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path[] srcs;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path[] srcs;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path[] srcs;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path[] srcs;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path[] srcs;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path[] srcs;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path[] srcs;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveFromLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);Free text;;;The src file is on the local disk.  Add it to FS at the given dst name. delSrc indicates if the source should be removed ;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;boolean overwrite;boolean overwrite;whether to overwrite an existing file;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void moveToLocalFile(Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path dst;path;false
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;false
org.apache.hadoop.fs.FileSystem;void moveToLocalFile(Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;boolean delSrc;boolean delSrc;whether to delete the src;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path src;Path dst;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path src;path;true
org.apache.hadoop.fs.FileSystem;void copyToLocalFile(boolean delSrc, Path src, Path dst);void copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem);@param;Path dst;Path dst;path;true
org.apache.hadoop.fs.FileSystem;Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile);void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile);@param;Path fsOutputFile;Path fsOutputFile;path of output file;true
org.apache.hadoop.fs.FileSystem;void access(Path path, FsAction mode);void checkAccessPermissions(FileStatus stat, FsAction mode);@param;FsAction mode;FsAction mode;type of access to check;true
org.apache.hadoop.fs.FileSystem;void access(Path path, FsAction mode);FsStatus getStatus();@throws;;;see specific implementation;true
org.apache.hadoop.fs.FileSystem;void access(Path path, FsAction mode);FsStatus getStatus(Path p);@throws;;;see specific implementation;true
org.apache.hadoop.fs.FileSystem;FileChecksum getFileChecksum(Path f);FileChecksum getFileChecksum(Path f, long length);@param;Path f;Path f;The file path;true
org.apache.hadoop.fs.FileSystem;FsStatus getStatus();FsStatus getStatus(Path p);@return;;;a FsStatus object;true
org.apache.hadoop.fs.FileSystem;FsStatus getStatus();FsStatus getStatus(Path p);@throws;;;see specific implementation;true
org.apache.hadoop.fs.FileSystem;void setOwner(Path p, String username, String groupname);void setTimes(Path p, long mtime, long atime);@param;Path p;Path p;The path;true
org.apache.hadoop.fs.FileSystem;Path createSnapshot(Path path);Path createSnapshot(Path path, String snapshotName);@return;;;the snapshot path.;true
org.apache.hadoop.fs.FileSystem;Path createSnapshot(Path path);Path createSnapshot(Path path, String snapshotName);@param;Path path;Path path;The directory where snapshots will be taken.;true
org.apache.hadoop.fs.FileSystem;Path createSnapshot(Path path, String snapshotName);void deleteSnapshot(Path path, String snapshotName);@param;String snapshotName;String snapshotName;The name of the snapshot;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void removeAclEntries(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void removeAclEntries(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void modifyAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileSystem;void removeAclEntries(Path path, List aclSpec);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileSystem;void removeAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeDefaultAcl(Path path);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeDefaultAcl(Path path);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeDefaultAcl(Path path);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileSystem;void removeDefaultAcl(Path path);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeDefaultAcl(Path path);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAcl(Path path);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAcl(Path path);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void removeAcl(Path path);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void setAcl(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void setAcl(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);Free text;;;Set an xattr of a file or directory. The name must be prefixed with the namespace followed by ".". For example, "user.attr". <p/> Refer to the HDFS extended attributes user documentation for details. ;true
org.apache.hadoop.fs.FileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.FileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;byte[] value;byte[] value;xattr value.;true
org.apache.hadoop.fs.FileSystem;void setXAttr(Path path, String name, byte[] value);byte[] getXAttr(Path path, String name);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.FileSystem;void setXAttr(Path path, String name, byte[] value, EnumSet flag);byte[] getXAttr(Path path, String name);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.FileSystem;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);@return;;;Map<String, byte[]> describing the XAttrs of the file or directory;true
org.apache.hadoop.fs.FileSystem;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.FileSystem;Map getXAttrs(Path path);List listXAttrs(Path path);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.FileSystem;Map getXAttrs(Path path, List names);List listXAttrs(Path path);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.FilterFileSystem;void copyFromLocalFile(boolean delSrc, Path src, Path dst);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);Whole;;;The src file is on the local disk.  Add it to FS at the given dst name. delSrc indicates if the source should be removed    ;false
org.apache.hadoop.fs.FSDataInputStream;int read(long position, byte[] buffer, int offset, int length);void readFully(long position, byte[] buffer, int offset, int length);@param;long position;long position;position in the input stream to seek;true
org.apache.hadoop.fs.FSDataInputStream;int read(long position, byte[] buffer, int offset, int length);void readFully(long position, byte[] buffer, int offset, int length);@param;byte[] buffer;byte[] buffer;buffer into which data is read;true
org.apache.hadoop.fs.FSDataInputStream;int read(long position, byte[] buffer, int offset, int length);void readFully(long position, byte[] buffer, int offset, int length);@param;int offset;int offset;offset into the buffer in which data is written;true
org.apache.hadoop.fs.TrashPolicy;void initialize(Configuration conf, FileSystem fs, Path home);TrashPolicy getInstance(Configuration conf, FileSystem fs, Path home);@param;Configuration conf;Configuration conf;the configuration to be used;true
org.apache.hadoop.fs.TrashPolicy;void initialize(Configuration conf, FileSystem fs, Path home);TrashPolicy getInstance(Configuration conf, FileSystem fs, Path home);@param;Path home;Path home;the home directory;true
org.apache.hadoop.fs.InvalidPathException; InvalidPathException(String path); InvalidPathException(String path, String reason);Free text;;;Constructs exception with the specified detail message. ;true
org.apache.hadoop.fs.InvalidPathException; InvalidPathException(String path); InvalidPathException(String path, String reason);@param;String path;String path;invalid path.;true
org.apache.hadoop.fs.LocatedFileStatus; LocatedFileStatus(FileStatus stat, BlockLocation[] locations); LocatedFileStatus(long length, boolean isdir, int block_replication, long blocksize, long modification_time, long access_time, FsPermission permission, String owner, String group, Path symlink, Path path, BlockLocation[] locations);Free text;;;Constructor ;true
org.apache.hadoop.fs.LocatedFileStatus; LocatedFileStatus(FileStatus stat, BlockLocation[] locations); LocatedFileStatus(long length, boolean isdir, int block_replication, long blocksize, long modification_time, long access_time, FsPermission permission, String owner, String group, Path symlink, Path path, BlockLocation[] locations);@param;BlockLocation[] locations;BlockLocation[] locations;a file's block locations;true
org.apache.hadoop.fs.LocatedFileStatus;int compareTo(Object o);boolean equals(Object o);@param;Object o;Object o;the object to be compared.;true
org.apache.hadoop.fs.FileSystemLinkResolver;T next(FileSystem fs, Path p);T resolve(FileSystem filesys, Path path);@return;;;Generic type determined by implementation;false
org.apache.hadoop.fs.shell.Command;LinkedList expandArguments(LinkedList args);List expandArgument(String arg);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;LinkedList expandArguments(LinkedList args);void processArguments(LinkedList args);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;LinkedList expandArguments(LinkedList args);void processArgument(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;LinkedList expandArguments(LinkedList args);void processPathArgument(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;LinkedList expandArguments(LinkedList args);void processPaths(PathData parent, PathData items);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;LinkedList expandArguments(LinkedList args);void postProcessPath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;LinkedList expandArguments(LinkedList args);void recursePath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;List expandArgument(String arg);void processArguments(LinkedList args);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;List expandArgument(String arg);void processArgument(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;List expandArgument(String arg);void processPathArgument(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;List expandArgument(String arg);void processPaths(PathData parent, PathData items);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;List expandArgument(String arg);void postProcessPath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;List expandArgument(String arg);void recursePath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArguments(LinkedList args);void processArgument(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArguments(LinkedList args);void processPathArgument(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArguments(LinkedList args);void processPaths(PathData parent, PathData items);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArguments(LinkedList args);void postProcessPath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArguments(LinkedList args);void recursePath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArgument(PathData item);void processPathArgument(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArgument(PathData item);void processPaths(PathData parent, PathData items);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArgument(PathData item);void postProcessPath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processArgument(PathData item);void recursePath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processPathArgument(PathData item);void processPaths(PathData parent, PathData items);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processPathArgument(PathData item);void postProcessPath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processPathArgument(PathData item);void recursePath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processPaths(PathData parent, PathData items);void postProcessPath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processPaths(PathData parent, PathData items);void recursePath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.Command;void processPath(PathData item);void postProcessPath(PathData item);@param;PathData item;PathData item;a PathData object;true
org.apache.hadoop.fs.shell.Command;void postProcessPath(PathData item);void recursePath(PathData item);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.PathData; PathData(String pathString, Configuration conf); PathData(URI localPath, Configuration conf);Free text;;;Creates an object to wrap the given parameters as fields.  The string used to create the path will be recorded since the Path object does not return exactly the same string used to initialize it ;true
org.apache.hadoop.fs.shell.PathData; PathData(String pathString, Configuration conf); PathData(URI localPath, Configuration conf);@param;Configuration conf;Configuration conf;the configuration file;true
org.apache.hadoop.fs.shell.PathData; PathData(String pathString, Configuration conf); PathData(URI localPath, Configuration conf);@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.PathData; PathData(String pathString, Configuration conf);FileStatus refreshStatus();@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.PathData; PathData(URI localPath, Configuration conf);FileStatus refreshStatus();@throws;;;if anything goes wrong...;true
org.apache.hadoop.fs.shell.CommandWithDestination;void getLocalDestination(LinkedList args);void getRemoteDestination(LinkedList args);@param;LinkedList<String> args;LinkedList<String> args;is the list of arguments;true
org.apache.hadoop.fs.shell.CommandWithDestination;void copyFileToTarget(PathData src, PathData target);void copyStreamToTarget(InputStream in, PathData target);@throws;;;if copy fails;true
org.apache.hadoop.fs.shell.CommandFactory; CommandFactory(Configuration conf);Command getInstance(String cmdName, Configuration conf);@param;Configuration conf;Configuration conf;the hadoop configuration;true
org.apache.hadoop.fs.shell.CommandFactory;void addClass(Class cmdClass, String names);void addObject(Command cmdObject, String names);@param;String names;String names;one or more command names that will invoke this class;true
org.apache.hadoop.fs.Trash; Trash(Configuration conf); Trash(FileSystem fs, Configuration conf);@param;Configuration conf;Configuration conf;a Configuration;true
org.apache.hadoop.fs.Trash;boolean moveToAppropriateTrash(FileSystem fs, Path p, Configuration conf);boolean moveToTrash(Path path);@return;;;false if the item is already in the trash or trash is disabled;false
org.apache.hadoop.fs.GlobPattern; GlobPattern(String globPattern);void set(String glob);@param;String globPattern;String glob;the glob pattern string;false
org.apache.hadoop.fs.FileUtil;Path[] stat2Paths(FileStatus[] stats);Path[] stat2Paths(FileStatus[] stats, Path path);@return;;;an array of paths corresponding to the input;true
org.apache.hadoop.fs.FileUtil;Path[] stat2Paths(FileStatus[] stats);Path[] stat2Paths(FileStatus[] stats, Path path);@param;FileStatus[] stats;FileStatus[] stats;an array of FileStatus objects;true
org.apache.hadoop.fs.FileUtil;boolean fullyDelete(File dir);boolean fullyDelete(File dir, boolean tryGrantPermissions);Free text;;;Delete a directory and all its contents.  If we return false, the directory may be partially-deleted. (1) If dir is symlink to a file, the symlink is deleted. The file pointed     to by the symlink is not deleted. (2) If dir is symlink to a directory, symlink is deleted. The directory     pointed to by symlink is not deleted. (3) If dir is a normal file, it is deleted. (4) If dir is a normal directory, then dir and all its contents recursively     are deleted. ;true
org.apache.hadoop.fs.FileUtil;boolean fullyDeleteContents(File dir);boolean fullyDeleteContents(File dir, boolean tryGrantPermissions);Free text;;;Delete the contents of a directory, not the directory itself.  If we return false, the directory may be partially-deleted. If dir is a symlink to a directory, all the contents of the actual directory pointed to by dir will be deleted. ;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(String filename);String makeShellPath(File file);Whole;;;Convert a os-native filename to a path that works for the shell.  @param The filename to convert @return The unix pathname @throws on windows, there can be problems with the subprocess;false
org.apache.hadoop.fs.FileUtil;String makeShellPath(String filename);String makeShellPath(File file, boolean makeCanonicalPath);Free text;;;Convert a os-native filename to a path that works for the shell. ;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(String filename);String makeShellPath(File file, boolean makeCanonicalPath);@return;;;The unix pathname;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(String filename);String makeShellPath(File file, boolean makeCanonicalPath);@param;String filename;File file;The filename to convert;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(String filename);String makeShellPath(File file, boolean makeCanonicalPath);@throws;;;on windows, there can be problems with the subprocess;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(File file);String makeShellPath(File file, boolean makeCanonicalPath);Free text;;;Convert a os-native filename to a path that works for the shell. ;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(File file);String makeShellPath(File file, boolean makeCanonicalPath);@return;;;The unix pathname;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(File file);String makeShellPath(File file, boolean makeCanonicalPath);@param;File file;File file;The filename to convert;true
org.apache.hadoop.fs.FileUtil;String makeShellPath(File file);String makeShellPath(File file, boolean makeCanonicalPath);@throws;;;on windows, there can be problems with the subprocess;true
org.apache.hadoop.fs.FileUtil;void setOwner(File file, String username, String groupname);void setPermission(File f, FsPermission permission);@param;File file;File f;the file to change;false
org.apache.hadoop.fs.FileUtil;boolean setReadable(File f, boolean readable);boolean setWritable(File f, boolean writable);@return;;;true on success, false otherwise;false
org.apache.hadoop.fs.FileUtil;boolean setReadable(File f, boolean readable);boolean setWritable(File f, boolean writable);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setReadable(File f, boolean readable);boolean setExecutable(File f, boolean executable);@return;;;true on success, false otherwise;false
org.apache.hadoop.fs.FileUtil;boolean setReadable(File f, boolean readable);boolean setExecutable(File f, boolean executable);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setReadable(File f, boolean readable);boolean canRead(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setReadable(File f, boolean readable);boolean canWrite(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setReadable(File f, boolean readable);boolean canExecute(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setWritable(File f, boolean writable);boolean setExecutable(File f, boolean executable);@return;;;true on success, false otherwise;false
org.apache.hadoop.fs.FileUtil;boolean setWritable(File f, boolean writable);boolean setExecutable(File f, boolean executable);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setWritable(File f, boolean writable);boolean canRead(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setWritable(File f, boolean writable);boolean canWrite(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setWritable(File f, boolean writable);boolean canExecute(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setExecutable(File f, boolean executable);boolean canRead(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setExecutable(File f, boolean executable);boolean canWrite(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean setExecutable(File f, boolean executable);boolean canExecute(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean canRead(File f);boolean canWrite(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean canRead(File f);boolean canExecute(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;boolean canWrite(File f);boolean canExecute(File f);@param;File f;File f;input file;true
org.apache.hadoop.fs.FileUtil;File[] listFiles(File dir);String[] list(File dir);@param;File dir;File dir;directory for which listing should be performed;true
org.apache.hadoop.fs.FileUtil;File[] listFiles(File dir);String[] list(File dir);@throws;;;for invalid directory or for a bad disk.;true
org.apache.hadoop.fs.ChecksumFileSystem;FileStatus[] listStatus(Path f);RemoteIterator listLocatedStatus(Path f);Free text;;;List the statuses of the files/directories in the given path if the path is a directory. ;false
org.apache.hadoop.fs.ChecksumFileSystem;FileStatus[] listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@param;Path f;Path f;given path;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf);@return;;;the complete path to the file on a local disk;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf);@param;String pathStr;String pathStr;the requested path (this will be created on the first available disk);true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);@return;;;the complete path to the file on a local disk;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);@param;String pathStr;String pathStr;the requested path (this will be created on the first available disk);true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathToRead(String pathStr, Configuration conf);@return;;;the complete path to the file on a local disk;false
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);Path getLocalPathToRead(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);File createTmpFileForWrite(String pathStr, long size, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, Configuration conf);boolean ifExists(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);@return;;;the complete path to the file on a local disk;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);@param;String pathStr;String pathStr;the requested path (this will be created on the first available disk);true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);@param;long size;long size;the size of the file that is going to be written;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);Path getLocalPathToRead(String pathStr, Configuration conf);@return;;;the complete path to the file on a local disk;false
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);Path getLocalPathToRead(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);File createTmpFileForWrite(String pathStr, long size, Configuration conf);@param;long size;long size;the size of the file that is going to be written;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);File createTmpFileForWrite(String pathStr, long size, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf);boolean ifExists(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);Path getLocalPathToRead(String pathStr, Configuration conf);@return;;;the complete path to the file on a local disk;false
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);Path getLocalPathToRead(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);File createTmpFileForWrite(String pathStr, long size, Configuration conf);@param;long size;long size;the size of the file that is going to be written;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);File createTmpFileForWrite(String pathStr, long size, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathForWrite(String pathStr, long size, Configuration conf, boolean checkWrite);boolean ifExists(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathToRead(String pathStr, Configuration conf);File createTmpFileForWrite(String pathStr, long size, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathToRead(String pathStr, Configuration conf);boolean ifExists(String pathStr, Configuration conf);@param;String pathStr;String pathStr;the requested file (this will be searched);true
org.apache.hadoop.fs.LocalDirAllocator;Path getLocalPathToRead(String pathStr, Configuration conf);boolean ifExists(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.LocalDirAllocator;File createTmpFileForWrite(String pathStr, long size, Configuration conf);boolean ifExists(String pathStr, Configuration conf);@param;Configuration conf;Configuration conf;the Configuration object;true
org.apache.hadoop.fs.RemoteIterator;boolean hasNext();E next();@throws;;;if any IO error occurs;true
org.apache.hadoop.fs.viewfs.ViewFileSystem; ViewFileSystem(URI theUri, Configuration conf); ViewFileSystem(Configuration conf);Free text;;;Convenience Constructor for apps to call directly ;true
org.apache.hadoop.fs.viewfs.ConfigUtil;void addLink(Configuration conf, String mountTableName, String src, URI target);void addLink(Configuration conf, String src, URI target);@param;Configuration conf;Configuration conf;- add the link to this conf;true
org.apache.hadoop.fs.viewfs.ConfigUtil;void addLink(Configuration conf, String mountTableName, String src, URI target);void addLink(Configuration conf, String src, URI target);@param;String src;String src;- the src path name;true
org.apache.hadoop.fs.viewfs.ConfigUtil;void addLink(Configuration conf, String mountTableName, String src, URI target);void addLink(Configuration conf, String src, URI target);@param;URI target;URI target;- the target URI link;true
org.apache.hadoop.fs.viewfs.ConfigUtil;void setHomeDirConf(Configuration conf, String homedir);void setHomeDirConf(Configuration conf, String mountTableName, String homedir);@param;Configuration conf;Configuration conf;- add to this conf;true
org.apache.hadoop.fs.viewfs.ConfigUtil;void setHomeDirConf(Configuration conf, String homedir);void setHomeDirConf(Configuration conf, String mountTableName, String homedir);@param;String homedir;String homedir;- the home dir path starting with slash;true
org.apache.hadoop.fs.viewfs.ConfigUtil;String getHomeDirValue(Configuration conf);String getHomeDirValue(Configuration conf, String mountTableName);@return;;;home dir value, null if variable is not in conf;true
org.apache.hadoop.fs.viewfs.ConfigUtil;String getHomeDirValue(Configuration conf);String getHomeDirValue(Configuration conf, String mountTableName);@param;Configuration conf;Configuration conf;- from this conf;true
org.apache.hadoop.fs.AbstractFileSystem;void rename(Path src, Path dst, Options.Rename options);void renameInternal(Path src, Path dst, boolean overwrite);Whole;;;The specification of this method matches that of {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path f must be for this file system.    ;false
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void removeAclEntries(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void removeAclEntries(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void modifyAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAclEntries(Path path, List aclSpec);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeDefaultAcl(Path path);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeDefaultAcl(Path path);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeDefaultAcl(Path path);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.AbstractFileSystem;void removeDefaultAcl(Path path);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeDefaultAcl(Path path);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAcl(Path path);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAcl(Path path);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void removeAcl(Path path);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void setAcl(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void setAcl(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);Free text;;;Set an xattr of a file or directory. The name must be prefixed with the namespace followed by ".". For example, "user.attr". <p/> Refer to the HDFS extended attributes user documentation for details. ;true
org.apache.hadoop.fs.AbstractFileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.AbstractFileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.AbstractFileSystem;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;byte[] value;byte[] value;xattr value.;true
org.apache.hadoop.fs.AbstractFileSystem;void setXAttr(Path path, String name, byte[] value);byte[] getXAttr(Path path, String name);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.AbstractFileSystem;void setXAttr(Path path, String name, byte[] value, EnumSet flag);byte[] getXAttr(Path path, String name);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.AbstractFileSystem;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);Free text;;;Get all of the xattrs for a file or directory. Only those xattrs for which the logged-in user has permissions to view are returned. <p/> Refer to the HDFS extended attributes user documentation for details. ;true
org.apache.hadoop.fs.AbstractFileSystem;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);@return;;;Map<String, byte[]> describing the XAttrs of the file or directory;true
org.apache.hadoop.fs.AbstractFileSystem;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.AbstractFileSystem;Map getXAttrs(Path path);List listXAttrs(Path path);@return;;;Map<String, byte[]> describing the XAttrs of the file or directory;false
org.apache.hadoop.fs.AbstractFileSystem;Map getXAttrs(Path path);List listXAttrs(Path path);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.AbstractFileSystem;Map getXAttrs(Path path, List names);List listXAttrs(Path path);@return;;;Map<String, byte[]> describing the XAttrs of the file or directory;false
org.apache.hadoop.fs.AbstractFileSystem;Map getXAttrs(Path path, List names);List listXAttrs(Path path);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);@throws;;;If path f is not valid;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void mkdir(Path dir, FsPermission permission, boolean createParent);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void mkdir(Path dir, FsPermission permission, boolean createParent);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean delete(Path f, boolean recursive);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean delete(Path f, boolean recursive);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean delete(Path f, boolean recursive);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean delete(Path f, boolean recursive);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataInputStream open(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataInputStream open(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FileStatus getFileStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;Path resolvePath(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void mkdir(Path dir, FsPermission permission, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void mkdir(Path dir, FsPermission permission, boolean createParent);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void mkdir(Path dir, FsPermission permission, boolean createParent);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void mkdir(Path dir, FsPermission permission, boolean createParent);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean delete(Path f, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean delete(Path f, boolean recursive);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean delete(Path f, boolean recursive);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean delete(Path f, boolean recursive);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean delete(Path f, boolean recursive);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean delete(Path f, boolean recursive);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f, int bufferSize);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f, int bufferSize);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f, int bufferSize);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean setReplication(Path f, short replication);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean setReplication(Path f, short replication);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void rename(Path src, Path dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setPermission(Path f, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setPermission(Path f, FsPermission permission);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setPermission(Path f, FsPermission permission);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setOwner(Path f, String username, String groupname);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setOwner(Path f, String username, String groupname);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setTimes(Path f, long mtime, long atime);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataOutputStream create(Path f, EnumSet createFlag, Options.CreateOpts opts);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean delete(Path f, boolean recursive);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean delete(Path f, boolean recursive);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean delete(Path f, boolean recursive);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean delete(Path f, boolean recursive);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FSDataInputStream open(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FSDataInputStream open(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FSDataInputStream open(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FSDataInputStream open(Path f, int bufferSize);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean setReplication(Path f, short replication);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean setReplication(Path f, short replication);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void rename(Path src, Path dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setPermission(Path f, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setPermission(Path f, FsPermission permission);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setOwner(Path f, String username, String groupname);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void mkdir(Path dir, FsPermission permission, boolean createParent);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f, int bufferSize);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f, int bufferSize);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean setReplication(Path f, short replication);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean setReplication(Path f, short replication);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void rename(Path src, Path dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setPermission(Path f, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setPermission(Path f, FsPermission permission);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setPermission(Path f, FsPermission permission);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setPermission(Path f, FsPermission permission);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setOwner(Path f, String username, String groupname);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setOwner(Path f, String username, String groupname);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setOwner(Path f, String username, String groupname);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setTimes(Path f, long mtime, long atime);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setTimes(Path f, long mtime, long atime);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileChecksum getFileChecksum(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If path f is invalid;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean delete(Path f, boolean recursive);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@param;Path f;Path f;the file name to open;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If file f does not exist;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FSDataInputStream open(Path f, int bufferSize);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean setReplication(Path f, short replication);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean setReplication(Path f, short replication);@throws;;;If file f does not exist;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean setReplication(Path f, short replication);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean setReplication(Path f, short replication);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void rename(Path src, Path dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void rename(Path src, Path dst, Options.Rename options);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setPermission(Path f, FsPermission permission);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setTimes(Path f, long mtime, long atime);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileChecksum getFileChecksum(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean setReplication(Path f, short replication);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean setReplication(Path f, short replication);@throws;;;If file f does not exist;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean setReplication(Path f, short replication);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean setReplication(Path f, short replication);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean setReplication(Path f, short replication);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void rename(Path src, Path dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void rename(Path src, Path dst, Options.Rename options);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setPermission(Path f, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setPermission(Path f, FsPermission permission);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setPermission(Path f, FsPermission permission);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setPermission(Path f, FsPermission permission);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setOwner(Path f, String username, String groupname);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setTimes(Path f, long mtime, long atime);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setTimes(Path f, long mtime, long atime);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileChecksum getFileChecksum(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FSDataInputStream open(Path f, int bufferSize);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void rename(Path src, Path dst, Options.Rename options);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void rename(Path src, Path dst, Options.Rename options);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void rename(Path src, Path dst, Options.Rename options);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setPermission(Path f, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setPermission(Path f, FsPermission permission);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setPermission(Path f, FsPermission permission);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setTimes(Path f, long mtime, long atime);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileChecksum getFileChecksum(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;boolean setReplication(Path f, short replication);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setPermission(Path f, FsPermission permission);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setPermission(Path f, FsPermission permission);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setPermission(Path f, FsPermission permission);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setPermission(Path f, FsPermission permission);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setTimes(Path f, long mtime, long atime);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileChecksum getFileChecksum(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void rename(Path src, Path dst, Options.Rename options);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setOwner(Path f, String username, String groupname);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setOwner(Path f, String username, String groupname);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setOwner(Path f, String username, String groupname);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setOwner(Path f, String username, String groupname);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setOwner(Path f, String username, String groupname);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setTimes(Path f, long mtime, long atime);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setTimes(Path f, long mtime, long atime);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setTimes(Path f, long mtime, long atime);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileChecksum getFileChecksum(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileChecksum getFileChecksum(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setPermission(Path f, FsPermission permission);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setTimes(Path f, long mtime, long atime);@param;Path f;Path f;The path;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setTimes(Path f, long mtime, long atime);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setTimes(Path f, long mtime, long atime);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setTimes(Path f, long mtime, long atime);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setTimes(Path f, long mtime, long atime);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setTimes(Path f, long mtime, long atime);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileChecksum getFileChecksum(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If server implementation throws undeclared exception to RPC server RuntimeExceptions:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setOwner(Path f, String username, String groupname);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileChecksum getFileChecksum(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileChecksum getFileChecksum(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileChecksum getFileChecksum(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileChecksum getFileChecksum(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileChecksum getFileChecksum(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setTimes(Path f, long mtime, long atime);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void setVerifyChecksum(boolean verifyChecksum, Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileChecksum getFileChecksum(Path f);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void setVerifyChecksum(boolean verifyChecksum, Path f);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);void access(Path path, FsAction mode);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);void access(Path path, FsAction mode);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FileStatus getFileLinkStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);FsStatus getFsStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;void access(Path path, FsAction mode);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);Path getLinkTarget(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);Path getLinkTarget(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If an I/O error occurred;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FileStatus getFileLinkStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);BlockLocation[] getFileBlockLocations(Path f, long start, long len);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;Path getLinkTarget(Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);FsStatus getFsStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);FsStatus getFsStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);FsStatus getFsStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);FsStatus getFsStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);FsStatus getFsStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;BlockLocation[] getFileBlockLocations(Path f, long start, long len);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);void createSymlink(Path target, Path link, boolean createParent);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;FsStatus getFsStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void createSymlink(Path target, Path link, boolean createParent);RemoteIterator listStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void createSymlink(Path target, Path link, boolean createParent);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;void createSymlink(Path target, Path link, boolean createParent);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@param;Path f;Path f;is the path;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If f does not exist;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);RemoteIterator listLocatedStatus(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;RemoteIterator listStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;RemoteIterator listLocatedStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If access is denied;true
org.apache.hadoop.fs.FileContext;RemoteIterator listLocatedStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If file system for f is not supported;true
org.apache.hadoop.fs.FileContext;RemoteIterator listLocatedStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an I/O error occurred Exceptions applicable to file systems accessed over RPC:;true
org.apache.hadoop.fs.FileContext;RemoteIterator listLocatedStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC client;true
org.apache.hadoop.fs.FileContext;RemoteIterator listLocatedStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If an exception occurred in the RPC server;true
org.apache.hadoop.fs.FileContext;RemoteIterator listLocatedStatus(Path f);boolean deleteOnExit(Path f);@throws;;;If server implementation throws undeclared exception to RPC server;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void removeAclEntries(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void removeAclEntries(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void modifyAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAclEntries(Path path, List aclSpec);void removeDefaultAcl(Path path);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileContext;void removeAclEntries(Path path, List aclSpec);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAclEntries(Path path, List aclSpec);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileContext;void removeAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAclEntries(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeDefaultAcl(Path path);void removeAcl(Path path);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeDefaultAcl(Path path);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeDefaultAcl(Path path);void setAcl(Path path, List aclSpec);@throws;;;if an ACL could not be modified;true
org.apache.hadoop.fs.FileContext;void removeDefaultAcl(Path path);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeDefaultAcl(Path path);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAcl(Path path);void setAcl(Path path, List aclSpec);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAcl(Path path);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void removeAcl(Path path);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void setAcl(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void setAcl(Path path, List aclSpec);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);Free text;;;Set an xattr of a file or directory. The name must be prefixed with the namespace followed by ".". For example, "user.attr". <p/> Refer to the HDFS extended attributes user documentation for details. ;true
org.apache.hadoop.fs.FileContext;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;Path path;Path path;Path to modify;true
org.apache.hadoop.fs.FileContext;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.FileContext;void setXAttr(Path path, String name, byte[] value);void setXAttr(Path path, String name, byte[] value, EnumSet flag);@param;byte[] value;byte[] value;xattr value.;true
org.apache.hadoop.fs.FileContext;void setXAttr(Path path, String name, byte[] value);byte[] getXAttr(Path path, String name);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.FileContext;void setXAttr(Path path, String name, byte[] value, EnumSet flag);byte[] getXAttr(Path path, String name);@param;String name;String name;xattr name.;true
org.apache.hadoop.fs.FileContext;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);Free text;;;Get all of the xattrs for a file or directory. Only those xattrs for which the logged-in user has permissions to view are returned. <p/> Refer to the HDFS extended attributes user documentation for details. ;true
org.apache.hadoop.fs.FileContext;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);@return;;;Map<String, byte[]> describing the XAttrs of the file or directory;true
org.apache.hadoop.fs.FileContext;Map getXAttrs(Path path);Map getXAttrs(Path path, List names);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.FileContext;Map getXAttrs(Path path);List listXAttrs(Path path);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.FileContext;Map getXAttrs(Path path, List names);List listXAttrs(Path path);@param;Path path;Path path;Path to get extended attributes;true
org.apache.hadoop.fs.HarFileSystem;URI getCanonicalUri();FileSystem[] getChildFileSystems();Whole;;;Used for delegation token related functionality. Must delegate to underlying file system.    ;false
org.apache.hadoop.fs.HarFileSystem;BlockLocation[] fixBlockLocations(BlockLocation[] locations, long start, long len, long fileOffsetInHar);BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len);@param;long start;long start;the start of the desired range in the contained file;true
org.apache.hadoop.fs.HarFileSystem;BlockLocation[] fixBlockLocations(BlockLocation[] locations, long start, long len, long fileOffsetInHar);BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len);@param;long len;long len;the length of the desired range;true
org.apache.hadoop.fs.HarFileSystem;boolean setReplication(Path src, short replication);boolean delete(Path f, boolean recursive);Whole;;;Not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;boolean setReplication(Path src, short replication);void setPermission(Path p, FsPermission permission);Whole;;;Not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;boolean delete(Path f, boolean recursive);void setPermission(Path p, FsPermission permission);Whole;;;Not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;boolean mkdirs(Path f, FsPermission permission);void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;boolean mkdirs(Path f, FsPermission permission);Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;boolean mkdirs(Path f, FsPermission permission);void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;boolean mkdirs(Path f, FsPermission permission);void setOwner(Path p, String username, String groupname);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst);void setOwner(Path p, String username, String groupname);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile);void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile);void setOwner(Path p, String username, String groupname);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.HarFileSystem;void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile);void setOwner(Path p, String username, String groupname);Whole;;;not implemented.    ;false
org.apache.hadoop.fs.FsShell;Path getCurrentTrashDir();void close();@throws;;;upon error;true
org.apache.hadoop.fs.FsShell;Path getCurrentTrashDir();void main(String[] argv);@throws;;;upon error;false
org.apache.hadoop.fs.FsShell;void close();void main(String[] argv);@throws;;;upon error;false
org.apache.hadoop.fs.GlobFilter; GlobFilter(String filePattern); GlobFilter(String filePattern, PathFilter filter);@param;String filePattern;String filePattern;the file pattern.;true
org.apache.hadoop.fs.GlobFilter; GlobFilter(String filePattern); GlobFilter(String filePattern, PathFilter filter);@throws;;;thrown if the file pattern is incorrect.;true
org.apache.hadoop.conf.Configuration;void addDeprecation(String key, String[] newKeys);void addDeprecation(String key, String newKey);@param;String key;String key;Key that is to be deprecated;true
org.apache.hadoop.conf.Configuration;void addResource(String name);void addResource(URL url);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(String name);void addResource(Path file);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(String name);void addResource(InputStream in, String name);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(String name);void addResource(Configuration conf);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(URL url);void addResource(Path file);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(URL url);void addResource(InputStream in, String name);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(URL url);void addResource(Configuration conf);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(Path file);void addResource(InputStream in, String name);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(Path file);void addResource(Configuration conf);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;void addResource(InputStream in, String name);void addResource(Configuration conf);Free text;;;Add a configuration resource. The properties of this resource will override properties of previously added resources, unless they were marked <a href="#Final">final</a>. ;true
org.apache.hadoop.conf.Configuration;String get(String name);String getTrimmed(String name);@return;;;the value of the name or its replacing property, or null if no such property exists.;false
org.apache.hadoop.conf.Configuration;String getTrimmed(String name);String getTrimmed(String name, String defaultValue);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getTrimmed(String name);String getRaw(String name);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getTrimmed(String name);Class getClasses(String name, Class defaultValue);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getTrimmed(String name);List getInstances(String name, Class xface);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getTrimmed(String name, String defaultValue);String getRaw(String name);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getTrimmed(String name, String defaultValue);Class getClasses(String name, Class defaultValue);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getTrimmed(String name, String defaultValue);List getInstances(String name, Class xface);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getRaw(String name);Class getClasses(String name, Class defaultValue);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;String getRaw(String name);List getInstances(String name, Class xface);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void set(String name, String value, String source);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void set(String name, String value, String source);@param;String value;String value;property value.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);int getInt(String name, int defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setInt(String name, int value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);long getLong(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);long getLongBytes(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setLong(String name, long value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);float getFloat(String name, float defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setFloat(String name, float value);@param;String value;float value;property value.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setDouble(String name, double value);@param;String value;double value;property value.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value);void setClass(String name, Class theClass, Class xface);@param;String value;Class<?> theClass;property value.;false
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);int getInt(String name, int defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setInt(String name, int value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);long getLong(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);long getLongBytes(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setLong(String name, long value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);float getFloat(String name, float defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setFloat(String name, float value);@param;String value;float value;property value.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setDouble(String name, double value);@param;String value;double value;property value.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void set(String name, String value, String source);void setClass(String name, Class theClass, Class xface);@param;String value;Class<?> theClass;property value.;false
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);int getInt(String name, int defaultValue);@param;String defaultValue;int defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);long getLong(String name, long defaultValue);@param;String defaultValue;long defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);long getLongBytes(String name, long defaultValue);@param;String defaultValue;long defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);float getFloat(String name, float defaultValue);@param;String defaultValue;float defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);double getDouble(String name, double defaultValue);@param;String defaultValue;double defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;String defaultValue;boolean defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);Class getClasses(String name, Class defaultValue);@param;String defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);Class getClass(String name, Class defaultValue);@param;String defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;String get(String name, String defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;String defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);void setInt(String name, int value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);long getLong(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);long getLong(String name, long defaultValue);@param;int defaultValue;long defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);long getLong(String name, long defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);long getLongBytes(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);long getLongBytes(String name, long defaultValue);@param;int defaultValue;long defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);long getLongBytes(String name, long defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);void setLong(String name, long value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);float getFloat(String name, float defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);float getFloat(String name, float defaultValue);@param;int defaultValue;float defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);float getFloat(String name, float defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);double getDouble(String name, double defaultValue);@param;int defaultValue;double defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);double getDouble(String name, double defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;int defaultValue;boolean defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);Class getClasses(String name, Class defaultValue);@param;int defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);Class getClass(String name, Class defaultValue);@param;int defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;int defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;int getInt(String name, int defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;int[] getInts(String name);void setBooleanIfUnset(String name, boolean value);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;int[] getInts(String name);void setEnum(String name, T value);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;int[] getInts(String name);Pattern getPattern(String name, Pattern defaultValue);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;int[] getInts(String name);void setPattern(String name, Pattern pattern);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;int[] getInts(String name);char[] getPassword(String name);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);long getLong(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);long getLongBytes(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);void setLong(String name, long value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);float getFloat(String name, float defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setInt(String name, int value);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);long getLongBytes(String name, long defaultValue);@return;;;property value as a long, or defaultValue.;false
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);long getLongBytes(String name, long defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);long getLongBytes(String name, long defaultValue);@param;long defaultValue;long defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);long getLongBytes(String name, long defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);void setLong(String name, long value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);float getFloat(String name, float defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);float getFloat(String name, float defaultValue);@param;long defaultValue;float defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);float getFloat(String name, float defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);double getDouble(String name, double defaultValue);@param;long defaultValue;double defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);double getDouble(String name, double defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;long defaultValue;boolean defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);Class getClasses(String name, Class defaultValue);@param;long defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);Class getClass(String name, Class defaultValue);@param;long defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;long defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLong(String name, long defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);void setLong(String name, long value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);float getFloat(String name, float defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);float getFloat(String name, float defaultValue);@param;long defaultValue;float defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);float getFloat(String name, float defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);double getDouble(String name, double defaultValue);@param;long defaultValue;double defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);double getDouble(String name, double defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;long defaultValue;boolean defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);Class getClasses(String name, Class defaultValue);@param;long defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);Class getClass(String name, Class defaultValue);@param;long defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;long defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;long getLongBytes(String name, long defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);float getFloat(String name, float defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setLong(String name, long value);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);void setFloat(String name, float value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);double getDouble(String name, double defaultValue);@param;float defaultValue;double defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);double getDouble(String name, double defaultValue);@throws;;;when the value is invalid;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;float defaultValue;boolean defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);Class getClasses(String name, Class defaultValue);@param;float defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);Class getClass(String name, Class defaultValue);@param;float defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;float defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;float getFloat(String name, float defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);double getDouble(String name, double defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);void setDouble(String name, double value);@param;float value;double value;property value.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setFloat(String name, float value);void setClass(String name, Class theClass, Class xface);@param;float value;Class<?> theClass;property value.;false
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);void setDouble(String name, double value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);boolean getBoolean(String name, boolean defaultValue);@param;double defaultValue;boolean defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);Class getClasses(String name, Class defaultValue);@param;double defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);Class getClass(String name, Class defaultValue);@param;double defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;double defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;double getDouble(String name, double defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);boolean getBoolean(String name, boolean defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setDouble(String name, double value);void setClass(String name, Class theClass, Class xface);@param;double value;Class<?> theClass;property value.;false
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);void setBoolean(String name, boolean value);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);Class getClasses(String name, Class defaultValue);@param;boolean defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);Class getClass(String name, Class defaultValue);@param;boolean defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;boolean defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;boolean getBoolean(String name, boolean defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);Collection getStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBoolean(String name, boolean value);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setBooleanIfUnset(String name, boolean value);void setEnum(String name, T value);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setBooleanIfUnset(String name, boolean value);void setEnum(String name, T value);@param;boolean value;T value;new value;true
org.apache.hadoop.conf.Configuration;void setBooleanIfUnset(String name, boolean value);Pattern getPattern(String name, Pattern defaultValue);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setBooleanIfUnset(String name, boolean value);void setPattern(String name, Pattern pattern);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setBooleanIfUnset(String name, boolean value);void setPattern(String name, Pattern pattern);@param;boolean value;Pattern pattern;new value;false
org.apache.hadoop.conf.Configuration;void setBooleanIfUnset(String name, boolean value);char[] getPassword(String name);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setEnum(String name, T value);Pattern getPattern(String name, Pattern defaultValue);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setEnum(String name, T value);void setPattern(String name, Pattern pattern);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setEnum(String name, T value);void setPattern(String name, Pattern pattern);@param;T value;Pattern pattern;new value;false
org.apache.hadoop.conf.Configuration;void setEnum(String name, T value);char[] getPassword(String name);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;T getEnum(String name, T defaultValue);void setTimeDuration(String name, long value, TimeUnit unit);@param;String name;String name;Property name;true
org.apache.hadoop.conf.Configuration;T getEnum(String name, T defaultValue);long getTimeDuration(String name, long defaultValue, TimeUnit unit);@param;String name;String name;Property name;true
org.apache.hadoop.conf.Configuration;void setTimeDuration(String name, long value, TimeUnit unit);long getTimeDuration(String name, long defaultValue, TimeUnit unit);@param;String name;String name;Property name;true
org.apache.hadoop.conf.Configuration;Pattern getPattern(String name, Pattern defaultValue);void setPattern(String name, Pattern pattern);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;Pattern getPattern(String name, Pattern defaultValue);char[] getPassword(String name);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;void setPattern(String name, Pattern pattern);char[] getPassword(String name);@param;String name;String name;property name;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);String[] getStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getStringCollection(String name);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);String[] getStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);Collection getTrimmedStringCollection(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);String[] getTrimmedStrings(String name, String defaultValue);@param;String defaultValue;String defaultValue;The default value;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getStrings(String name, String defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getTrimmedStringCollection(String name);String[] getTrimmedStrings(String name);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getTrimmedStringCollection(String name);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getTrimmedStringCollection(String name);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getTrimmedStringCollection(String name);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getTrimmedStringCollection(String name);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Collection getTrimmedStringCollection(String name);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name);String[] getTrimmedStrings(String name, String defaultValue);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name, String defaultValue);void setStrings(String name, String values);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name, String defaultValue);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name, String defaultValue);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;String[] getTrimmedStrings(String name, String defaultValue);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setStrings(String name, String values);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setStrings(String name, String values);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;void setStrings(String name, String values);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;InetSocketAddress getSocketAddr(String hostProperty, String addressProperty, String defaultAddressValue, int defaultPort);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@return;;;InetSocketAddress;true
org.apache.hadoop.conf.Configuration;InetSocketAddress getSocketAddr(String hostProperty, String addressProperty, String defaultAddressValue, int defaultPort);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;String defaultAddressValue;String defaultAddress;the default value;true
org.apache.hadoop.conf.Configuration;InetSocketAddress getSocketAddr(String hostProperty, String addressProperty, String defaultAddressValue, int defaultPort);InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);@param;int defaultPort;int defaultPort;the default port;true
org.apache.hadoop.conf.Configuration;InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;InetSocketAddress getSocketAddr(String name, String defaultAddress, int defaultPort);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;InetSocketAddress updateConnectAddr(String hostProperty, String addressProperty, String defaultAddressValue, InetSocketAddress addr);InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);@return;;;InetSocketAddress for clients to connect;true
org.apache.hadoop.conf.Configuration;InetSocketAddress updateConnectAddr(String name, InetSocketAddress addr);void setClass(String name, Class theClass, Class xface);@param;String name;String name;property name.;true
org.apache.hadoop.conf.Configuration;Class getClassByName(String name);Class getClass(String name, Class defaultValue);@param;String name;String name;the class name.;true
org.apache.hadoop.conf.Configuration;Class getClassByName(String name);Class getClass(String name, Class defaultValue, Class xface);@param;String name;String name;the class name.;true
org.apache.hadoop.conf.Configuration;Class getClasses(String name, Class defaultValue);Class getClass(String name, Class defaultValue);@param;Class<?> defaultValue;Class<?> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;Class getClasses(String name, Class defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;Class<?> defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;Class getClasses(String name, Class defaultValue);List getInstances(String name, Class xface);@param;String name;String name;the property name.;true
org.apache.hadoop.conf.Configuration;Class getClass(String name, Class defaultValue);Class getClass(String name, Class defaultValue, Class xface);@return;;;property value as a Class, or defaultValue.;true
org.apache.hadoop.conf.Configuration;Class getClass(String name, Class defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;String name;String name;the class name.;true
org.apache.hadoop.conf.Configuration;Class getClass(String name, Class defaultValue);Class getClass(String name, Class defaultValue, Class xface);@param;Class<?> defaultValue;Class<? extends U> defaultValue;default value.;true
org.apache.hadoop.conf.Configuration;Class getClass(String name, Class defaultValue, Class xface);void setClass(String name, Class theClass, Class xface);@param;Class<U> xface;Class<?> xface;the interface implemented by the named class.;true
org.apache.hadoop.conf.Configuration;Path getLocalPath(String dirsProp, String path);File getFile(String dirsProp, String path);@return;;;local file under the directory with the given path.;false
org.apache.hadoop.conf.Configuration;Path getLocalPath(String dirsProp, String path);File getFile(String dirsProp, String path);@param;String dirsProp;String dirsProp;directory in which to locate the file.;true
org.apache.hadoop.conf.Configuration;Path getLocalPath(String dirsProp, String path);File getFile(String dirsProp, String path);@param;String path;String path;file-path.;true
org.apache.hadoop.conf.Configuration;InputStream getConfResourceAsInputStream(String name);Reader getConfResourceAsReader(String name);@param;String name;String name;configuration resource name.;true
org.apache.hadoop.conf.ReconfigurationException; ReconfigurationException(); ReconfigurationException(String property, String newVal, String oldVal, Throwable cause);Whole;;;Create a new instance of {@link ReconfigurationException}.    ;false
org.apache.hadoop.conf.ReconfigurationException; ReconfigurationException(); ReconfigurationException(String property, String newVal, String oldVal);Whole;;;Create a new instance of {@link ReconfigurationException}.    ;false
org.apache.hadoop.conf.ReconfigurationException; ReconfigurationException(String property, String newVal, String oldVal, Throwable cause); ReconfigurationException(String property, String newVal, String oldVal);Whole;;;Create a new instance of {@link ReconfigurationException}.    ;false
